{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fundemantal Concepts toward Understanding Gaussian Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to introduce **essential concepts** for the sake of understanding **Gaussian Process**, a powerful machine learning model to predict the shape of a function while giving corresponding **uncertainties** for beginners. Some Concepts will come with **python code examples**.\n",
    "The notebook is formated into three main parts.\n",
    "1. **Mathematic Concepts**\n",
    "- Vectors and Matrices\n",
    "- Eigenvalues / Eigenvectors and Matrix Properties\n",
    "2. **Statistic Concepts**\n",
    "- Random Variables\n",
    "- Mean, Variance, and Standard Deviation\n",
    "- Covariance and Correlation\n",
    "- Gaussian Distribution\n",
    "- Multidimension Gaussian Distribution\n",
    "- The Central Limit Theorem\n",
    "- Probability Theory Basics\n",
    "- Probability and Likelihood\n",
    "- Baye's Theorem\n",
    "3. **Machine Learning Concepts**\n",
    "- Machine Learning Essentials\n",
    "- Machine Learning Categories\n",
    "- Regression\n",
    "- Maximum A Posteriori (MAP)\n",
    "- True Bayesian Prediction\n",
    "- Kernel Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Mathematic Concepts**\n",
    "#### 1. Vectors and Matrices\n",
    "##### Vectors\n",
    "A vector is an ordered list of numbers, represented as a row: $x=[x_1,x_2,x_3]$. In python, it is usually represented as a list `x=[x1,x2,x3]`. A more common way to manipulate vectors in python is through np.array, which can be defined by converting a list or with numbers. `x_np=np.array([1,2,3])`.Multidimensional arrays can also be defined by`np.ndarray` method. The length of the vector can be extracted by `len(a)`, or `a.size()` if a is a np array.\n",
    "##### Vector Operations\n",
    "Some important operations of a vector are:\n",
    "- Addtion: for two vectors with equal length, the sum of them is just a vector with the same length where each component of the resulting vector is the sum of both components in the original vector (elementwise).\n",
    "- Scalar multiplication: $c\\cdot [x_1,x_2,\\cdots,x_n] = [cx_1,cx_2,\\cdots, cx_n]$\n",
    "- Dot Product: $x\\cdot y = \\sum_{i=1}^n a_i b_i$. In python, this can be easily done by `np.dot(a,b)`.\n",
    "- Norm: The norm of a vector, or the macnitude of the vector, is $||x||=\\sqrt{x_1^2,x_2^2,\\cdots,x_n^2}$. In python, the norm of a vector can be calculated by `np.linalg.norm(X)`.\n",
    "##### Matrix\n",
    "A matrix is a rectangular array of numbers. A matrix that is n by m means that it has n rows and m columns. For example, the following matrix A is a 3*2 matrix:\n",
    "$$A=\\begin{bmatrix}\n",
    "0.896 & 0.887 \\\\\n",
    "0.007 &  0.639 \\\\\n",
    "0.135 & 0.244\n",
    "\\end{bmatrix}$$\n",
    "A vector can be seen as a special type of matrix where the **size is n by 1 or 1 by n**. In python, a matrix can be defined as `np.array` or `np.ndarray`. To extract the shape of the matrix $A=np.ndarray()$, one can use `A.shape()`. A matrix whose number of columns is equal to the number of rows is called a **square matrix**.\n",
    "<br> A special matrix is the identity matrix, a square matrix with ones on the main diagonal and zeros elsewhere. It can be generated with `np.eye()` or `np.identity()`.\n",
    "$$ I = \\begin{pmatrix}\n",
    "1 & 0 & \\cdots & 0 \\\\\n",
    "0 & 1 & \\cdots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & \\cdots & 1 \\end{pmatrix}$$\n",
    "##### Matrix Operations\n",
    "- Addition: the addition between two matrices with the same size is element-wise.\n",
    "- Multiplication: In order to perform multiplication between two matrices, the number of columns in the first matrix must be equal to the number of rows in the second matrix. That is, in order to perform $AB$, $A$ and $B$ must have **shape i by n, and n by j**, respectively. The result is an **i by j** matrix.To perform matrix multiplication in python, one can use `np.dot(A,B)`.\n",
    "- Transpose: an operator that flips a matrix over its diagonal by switching the row and column indices of matrix B and producing another matrix. Transpose of a matrix in python is`A.T` suppose A is any matrix.\n",
    "$$A = \\begin{bmatrix}\n",
    "A_{11} & A_{12} \\\\\n",
    "A_{21} &  A_{22} \\\\\n",
    "A_{31} & A_{32}\n",
    "\\end{bmatrix}, \n",
    "A^T=\\begin{bmatrix}\n",
    "A_{11} & A_{21} && A_{31} \\\\\n",
    "A_{12} & A_{22} && A_{32}\n",
    "\\end{bmatrix}$$\n",
    "- Determinant: the determinant is a scalar-valued function of the entries of a **square matrix**. The determinant of a square matrix can be represented as either $det(A)$ or $|A|$. It is calculated as follows. For square matrices that is larger than 3 by 3, the determinant could be tedious to calculate, but is can be eaasily be done with `np.linalg.det(A)`.\n",
    "$$\\begin{vmatrix}\n",
    "a & b \\\\\n",
    "c & d\\end{vmatrix} = ad-bc, \\,\n",
    "\\begin{vmatrix}\n",
    "a & b & c\\\\\n",
    "d & e & f \\\\\n",
    "g & h & i\\end{vmatrix} = aei+bfg+cdh-ceg-bdi-afh$$\n",
    "- Inversion: The inverse of a matrix is defined as the matrix where the multiplication between the inverse and the original matrix results in an idendity matrix. In order to perform inversion, the matrix must have a non-zero determinant.\n",
    "$$ A^{-1}A=I $$\n",
    "$$ A^{-1}=\\frac{1}{|A|}, \\, |A| \\neq 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Eigenvalues / Eigenvectors and Matrix Properties\n",
    "###### Eigenvalues and Eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Statistics**\n",
    "#### 1. Random Variables\n",
    "A random variable is a numerical value assigned to the outcome of a random experiment. Mathematically, a random variable ***X*** is a **function** that maps outcomes from a sample space to real numbers.\n",
    "$$ X:\\Omega \\to \\mathbb{R} $$\n",
    "##### Discrete Variables\n",
    "- The value of the variable is contrained to a finite or countably finite set of values.\n",
    "- **Example**: The number one gets from tossing a dice. Possible values are limited to {1,2,3,4,5,6,}.\n",
    "##### Continuous Random Variables\n",
    "- The value of the variable is contrained to a **range**, while **uncountably** many.\n",
    "- **Example**: The height of a randomly chosen person.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Mean, Variance, and Standard Deviation\n",
    "To descibe the distribution of a random raviable, these three measurements are key.\n",
    "##### Mean(Expected Value)\n",
    "Mean(represented as $\\mu$), or sometimes called the Expected value(represented as $\\mathbb{E}[X]$), represents the average outcome if the experiment were repeated **infinitely**. Note that in real cases, obtaining the real mean or real expected value could be extremely challenging because it requires one to have full grasp of the distribution (for example, it is extremely expensive to survey everyone on earth's dinner item to probe the distribution). Instead, we sample over the population and use the sample measurements to probe the overall situation. Therefore, the goal becomes that the **sample mean** approaches the true mean.\n",
    "- For a discrete random variable, where $P(X = x_i)$ is the probability of $x_i$: $$\\mathbb{E}[X]=\\sum_{i}x_i P(X = x_i)$$\n",
    "- For a continuous random variable, where $f(x)$ is the probability density function, which will be explained in the next section: $$\\mathbb{E}[X] = \\int_{-\\infty}^{\\infty} x f(x) \\,dx$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Variance\n",
    "The variance is a measure of variability of the random variable, or the spread of the values around the **mean**. It is denoted as $Var(X)$ or $\\sigma^2$.\n",
    "$$ Var(X)=\\mathbb{E}[(X - \\mathbb{E}[X])^2]= \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$$\n",
    "- For a continuous variable, where $f(x)$ is the probability density function:\n",
    "$$ Var(X) = \\int_{-\\infty}^{\\infty} (x - \\mu)^2 f(x) \\,dx $$\n",
    "##### Standard Deviation\n",
    "The standard deviation is the square root of the variance:\n",
    "$$\\sigma = \\sqrt{\\text{Var}(X)}$$\n",
    "\n",
    "It has the same unit as $X$, making it easier to interpret than variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Covariance and Correlation\n",
    "##### Covariance\n",
    "When we are dealing with two or more random variables, it is important that we develop another measurement to describe the relationship between two different random variables $X$ and $Y$. The covariance is defined as:\n",
    "$$ Cov(X,Y)=\\mathbb{E}[(X-\\mu_X)(Y-\\mu_Y)] $$\n",
    "The covariance describes the tendency of a random variable to move towards the same direction of the movement of another random variable:\n",
    "- if  $Cov(X,Y)>0$, $X$ and $Y$ tend to increase together.\n",
    "- if  $Cov(X,Y)=0$, $X$ and $Y$ have no correlation.\n",
    "- if  $Cov(X,Y)<0$, $X$ and $Y$ tend to move in opposite directions.\n",
    "\n",
    "<br> When dealing with **more than two random variables**, it is common to construct a **covariance matrix**. For example we have random variables $X_1 , X_2 , \\cdots , X_n$, the covariance matrix is:\n",
    "$$K_{ij}=Cov(X_i,X_j)$$\n",
    "\n",
    "##### Correlation\n",
    "If we are going to compare the covariance between $A$ and $B$ with that between $C$ and $D$, it is usually challenging due to the different scales the two covariances inherently have. Therefore, standardizing the covariance, which generates correlation, makes direct comparison more reasonable. The correlation between two variables is obtained by deviding their covariance with the product of their standard deviation:\n",
    "$$ r(X,Y) = \\frac{Cov(X,Y)}{\\sigma_X \\sigma_Y}$$\n",
    "Because the correlation is standardized, its possible values are within $[-1,1]$.\n",
    "- if  $0<r(X,Y) \\le 1$, $X$ and $Y$ tend to increase together,and have perfect positive correlation at $r=1$\n",
    "- if  $r(X,Y)=0$, $X$ and $Y$ have no correlation.\n",
    "- if  $-1 \\le r(X,Y)<0$, $X$ and $Y$ tend to move in opposite directions,and have perfect negative correlation at $r=-1$\n",
    "\n",
    "<br>Note that a correlation of 0 **does not imply independence**, but independent random variables have correlation of 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Gaussian Distribution\n",
    "The gaussian distribution, namely one of the most popular distribution patterns, is a **continuous probability** distribution that models many natural phenomena and can be seen very often. Gaussian Process is based on multivariate gaussian distribution. \n",
    "##### One-Dimentional Gaussian Distribution\n",
    "A gaussian distribution is a distribution that described by two parameters, the mean $\\mu$, and the standard deviation $\\sigma$. To state that a random variable $X$ follows a one-dimensional gaussian distribution described by $\\mu$ and $\\sigma$, one can use the following math expression: $$ X \\sim \\mathcal{N}(\\mu,\\sigma) $$  \n",
    "By definition, in one dimentional case, it is said that a random variable $X$ follows gaussian distribution if its **probability function** (PDF) is given by the following expression:\n",
    "$$f(x)=\\frac{1}{\\sqrt{2\\pi \\sigma^2}}exp(-\\frac{(x-\\mu)^2}{2\\sigma^2})$$\n",
    "A probability distribution function (PDF) describes the likelihood of a continouous random variable taking on a particular value. PDF does not give the probability of a single value, rather it defines a probability density over a range. It has the following properties, where $f(x)$ is the PDF:\n",
    "- The probability that a random variable $X$ falls within an interval $[a,b]$ is givenby $P(a\\le X \\le b)=\\int_a^b f(x) \\, dx$\n",
    "- Non-negativity: $f(x)\\ge 0$\n",
    "- Normalization: $\\int_{-\\infty}^{\\infty}f(x) \\, dx=1$\n",
    "- the probability of $X$ being exactly any single value is **zero**. Instead, it can only be used to compute probabilities over an **interval**.\n",
    "##### Properties of Gaussian Distribution\n",
    "- The shape of gaussian distribution is a **Bell-shaped curve** that is **symmetric around the mean** $\\mu$.\n",
    "- Mean = Median = Mode: the highest peak is exactly at $x=\\mu$.\n",
    "- A gaussian distribution is fully described by only **two parameters**: the mean $\\mu$ which determines the center, and the standard deviation $\\sigma$ which determines the spread.\n",
    "- Rule of thumb: \n",
    "    - $68%$ of the data falls within $\\mu \\pm \\sigma$\n",
    "    - $95%$ of the data falls within $\\mu \\pm 2\\sigma$\n",
    "    - $99.7%$ of the data falls within $\\mu \\pm 3\\sigma$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Multivariate Gaussian Distribution\n",
    "A gaussian distribution can be generalized to multivariate random variables. In this case, we are dealing with random vectors rather than univariate random variables, where each component of the random vector is gaussian distributed. For a multivariate random vector X that is n-dimensional:\n",
    "$$X = \\begin{bmatrix} X_1 \\\\ X_2 \\\\ \\cdots \\\\ X_n \\end{bmatrix}$$\n",
    "The corresponding mean vector $\\mu$ and covariance matrix $\\Sigma$ are represented as follow. The mean vector represents the expected value of each component of $X$, and the covariance matrix encodes the variances and the covariances between two random variables.\n",
    "$$\\mu = \\begin{bmatrix} \\mu_1 \\\\ \\mu_2 \\\\ \\cdots \\\\ \\mu_n \\end{bmatrix}, \n",
    "  \\Sigma = \\begin{bmatrix} Cov(X_1,X_1) && Cov(X_1,X_2) && \\cdots && Cov(X_1,X_n)\\\\ \n",
    "                Cov(X_2,X_1) && Cov(X_2,X_2) && \\cdots && Cov(X_2,X_n) \\\\\n",
    "                \\vdots && \\vdots && \\ddots && \\vdots\\\\ \n",
    "                Cov(X_n,X_1) && Cov(X_n,X_2) && \\cdots && Cov(X_n,X_n)\\end{bmatrix}$$\n",
    "Key properties of the covariance matrix $\\Sigma$:\n",
    "- The matrix is **symmetric**: $\\Sigma_{ij} = \\Sigma{ji}$\n",
    "- The diagonal of the covariance matrix is just the variance of each components of the random vector $X$ :$Cov(X_i,X_i)=Var(X_i)$.  \n",
    "- The matrix need to be **Positive Semi-Definite** to ensure that the variances are always non-negative.\n",
    "- The covariance matrix controls the direction in which the data is more spread, and the correlation between variables. (**More content not finished**)\n",
    "<br> A multivariate gaussian can still be fully described by its mean vector and covariance matrix. The probability density function of a multivariate gaussian distributed random vector $X$ with length $n$, mean vector $\\mu$, and covariance matrix $\\Sigma$ is:\n",
    "$$f(X)=\\frac{1}{(2\\pi)^{n/2}|\\Sigma|^{1/2}} exp(-\\frac{1}{2}(X-\\mu)^T\\Sigma^{-1}(X-\\mu))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Central Limit Theorem (CLT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Probability Theory Basics\n",
    "##### Joint Probability\n",
    "Joint probability is the probability of two events happening together. Suppose we have event $X$ and event $Y, the joint probablity of $X$ and $Y$ is expressed as $P(X,Y)$, or $P(X\\cap Y)$. If $X$ and $Y$ are **independent events**, the joint probability of $X$ and $Y$ is just the product of the probability of $X$ and probability of $Y$: $P(X,Y)=P(X)P(Y)$.\n",
    "##### Marginal Probability\n",
    "Marginal Probability is the probability of a single event **regardless of other variables**. It is obtained by summing (or integrating) out the other variable from the joint probability.\n",
    "- Discrete Variable: $P(X=x)= \\displaystyle\\sum_y P(X=x,Y=y)$\n",
    "- Continuous Variable: $P(X=x)= \\int P(X=x,Y=y)dy$\n",
    "##### Conditional Probability\n",
    "The conditional probability is the probability of an event given that another event has already occurred. It is expressed as $P(X|Y)$, which represents the probability of event $X$ given that event $Y$ already happened. \n",
    "- If $X$ and $Y$ are **not independent**, the contioanl probability is calculated as:\n",
    "$$P(X|Y)=\\frac{P(X,Y)}{P(Y)}$$\n",
    "- If $X$ and $Y$ are **Independent**, the conditional probability $P(X|Y)$ is just the probability of $X$.\n",
    "$$P(X|Y)=P(X)$$\n",
    "#### Example Calculations\n",
    "Suppose we have the following normalized distribution of a pile of products produced in two difference locations, California and New York. The two random variables are **Location(L)** and Type of **Products(T)**.\n",
    "<div style=\"text-align: center\"> <img src=\"image.png\" alt=\"Drawing\" width=\"500\"/> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The numbers shaded in **blue** represent the **joint probability**. For example, $P(T=Shoes,L=California)=0.48$. If the joint probability is unknwon, we can calculate as $P(T=Shoes,L=California)=P(T=Shoes)P(L=California)=0.48*0.39=0.17$, given that we know they are independent events.\n",
    "- The **marginal probabilities** are shaded in **green**, as they represents the probability of an event regardless of the other variable. In another word, if we do not know the marginal probability of California, we can add up all the products produced in California to obtain the marginal probability: $P(L=California)=\\displaystyle\\sum_i P(L=California,T=i)=0.22+0.09+0.17=0.48$.\n",
    "- Contional Probabilities are not shown in the table, but can be easilly calculated. For instance, if we want to calculate $P(Shoes|New York)$, which is the probability of shoes given that they are produced in New York, we calculate as follow:\n",
    "$$P(Shoes|New York)=\\frac{P(Shoes,New York)}{P(New York)}=\\frac{0.22}{0.52}=0.42$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Probability and Likelihood\n",
    "Probability and Likelihood are two terminologies that have subtle yet cirtical difference in statistics. Understanding the key difference between them helps the understanding of Bayes' Theorem and gaussian process a lot.\n",
    "##### Probability\n",
    "Probability is generally understood as \"how likely an event is to occur\", but this is not enough to understand it statistically. In statistics, probability $P(D|\\theta)$ is the possibility of the occurance of an event based on a pre-selected model with **specific parameters**, or our understanding of the model. \n",
    "<br>For example, the probability of getting a head with a fair coin is $P=\\frac{1}{2}$. The pre-selected model here is a fair coin, which we assumed that the chances of getting a head and getting a tail are the same. The estimation of probability is done **before any events occur**. The nature of probability is **interpreting a model**.\n",
    "##### Likelihood\n",
    "Although one can still understand likelihood as \"chance of an event occurance\", it is defined differently in statistics. In statistics, likelihood is the **plausibility of a model** (usually equal to a set of parameters or a single parameter), given an observation. Likelihood is represented as $L(\\theta|D)$, where $D$ is the observation (data) and $\\theta$ is the parameter set whose plausibility is being evaluated.It is interpreted as \"the possibility of the observed data **given some parameters**\". As you may notice, one key difference between Probability and Likelihood is **whether or not we make any observation**.\n",
    "<br>Continuing on the die example, say we tossed the coin 10 times and get 7 heads. After we make the observation, we come up with three models to describe the coin, where $\\theta_H$ is the possibility of head:\n",
    "1.  The coin is **fair**. ($\\theta_H=0.5$)\n",
    "2. The coin is biased and the chance of getting a **head** is 0.7.($\\theta_H=0.7$)\n",
    "3. The coin is biased and the chance of getting a **tail** is 0.7.($\\theta_H=1-0.7=0.3$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the likelihood of each model, a.k.a their plausibility, we use these models to calculate the chance of getting our observation:\n",
    "1. $L(\\theta_H=0.5|D)=\\frac{10!}{7!(10-7)!}0.5^7 (1-0.5)^{10-7}=11.72\\%$\n",
    "2. $L(\\theta_H=0.3|D)=\\frac{10!}{7!(10-7)!}0.7^7 (1-0.7)^{10-7}=26.68\\%$\n",
    "3. $L(\\theta_H=0.7|D)=\\frac{10!}{7!(10-7)!}0.5^{10-7} (1-0.5)^{7}=0.90\\%$\n",
    "As the result indicates, the model with the highest likelihood is the model where we assume the coin is biased and has 70% chance to get a head in each toss. This means this model is the most plausible one among three. As you may noticed, the \"most plausible\" model based on our likelihood calculation does not align with our understanding about a fair coin, whose chance to get a head is 50%. This is because the likelihood **only considers the data, not the truth**. Another factor in this example is that the number of datapoints involved is too **small**, which indicates high fluctuation.\n",
    "\n",
    "A way to assess the credibility of likelihood is to add **confidence interval** into the interpretation. This is an important reason why in Bayesian Thinking models like gaussian process, incorporating conficence interval is necessary.\n",
    "##### Example - Gaussian Distritbuion\n",
    "It is also beneficial to visualize the difference between probability and likelihood with a **continuous distribution**, since the later gaussian process involves multivariate gaussian distribution, whith is continuous.\n",
    "Let's say we have a gaussian distributinon $X\\sim \\mathcal{N}(0,1)$. \n",
    "<br>Now we want to know the **probability** to get $-0.3$. The probability of getting this number with the given model is:\n",
    "$$P(-0.3|\\mu=0,\\sigma=1)=\\frac{1}{\\sqrt{2\\pi \\cdot 1^2}}exp(-\\frac{(-0.3-0)^2}{2\\cdot 1^2})=0.381$$\n",
    "<br>If instead we don't know the model parameters, but we get -0.3 after an inquiry, we can come up with the most plausible model to describe this event via likelihood calculation. Let's say we have two candidate models as following, and their likelihood of them are:\n",
    "1. $X\\sim \\mathcal{N}(-1,1)$:$L(\\mu=-0.1|X=-0.3)=\\frac{1}{\\sqrt{2\\pi \\cdot 1^2}}exp(-\\frac{(-0.3-(-0.1))^2}{2\\cdot 1^2})=0.312$\n",
    "2. $X\\sim \\mathcal{N}(-0.3,1)$:$L(\\mu=-0.3|X=-0.3)=\\frac{1}{\\sqrt{2\\pi \\cdot 1^2}}exp(-\\frac{(-0.3-(-0.3))^2}{2\\cdot 1^2})=0.399$\n",
    "\n",
    "Based on the calculation, the model $X\\sim \\mathcal{N}(-0.3,1)$ has the highest likelihood, thus it is by far the best model to describe the event.\n",
    "\n",
    "A visualization of this analysis is shown below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a289a46702d440ab98e1fd3e26cf0e4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=-0.3, description='x:', max=3.0, min=-3.0), FloatSlider(value=-0.8, de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.prob_like(obs, like_mu)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from ipywidgets import interact, FloatSlider\n",
    "\n",
    "def prob_like(obs, like_mu):\n",
    "    # Given parameters\n",
    "    sigma = 1  # Known variance\n",
    "    mu = 0  # Known mean for probability plot\n",
    "    x_obs = -0.3  # Target Value\n",
    "    x_values = np.linspace(-4, 3, 100)\n",
    "    prob_obs = norm.pdf(obs, loc = mu, scale = sigma)\n",
    "    # Plot Setup\n",
    "    probability_values = norm.pdf(x_values, loc=mu, scale=sigma)\n",
    "    mu_values = [like_mu, -0.3]  # Different means to compare\n",
    "    colors = [\"orange\", \"green\"]\n",
    "    marker_colors = [\"red\", \"blue\"]\n",
    "\n",
    "\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5), dpi=120)\n",
    "\n",
    "    # --- Plot Probability ---\n",
    "    axes[0].plot(x_values, probability_values, label=\"f(x)\", color=\"blue\")\n",
    "    axes[0].fill_between(x_values, probability_values, alpha=0.2, color=\"blue\")\n",
    "    axes[0].axvline(x_obs, linestyle=\"dashed\", color=\"red\", label=\"x=-0.3\")\n",
    "    axes[0].scatter(obs, prob_obs, s=50, color = \"red\", label = f\"P({obs:.1f})={prob_obs:.2f}\")\n",
    "    axes[0].axhline(norm.pdf(x_obs, scale=1), linestyle=\"dashed\", color=\"g\", label=\"P(-0.3)\")\n",
    "    axes[0].set_title(\"Probability: P(x | μ=0, σ²=1)\")\n",
    "    axes[0].set_xlim([-3,3])\n",
    "    axes[0].set_xlabel(\"x\")\n",
    "    axes[0].set_ylabel(\"Density\")\n",
    "    axes[0].legend()\n",
    "\n",
    "    # --- Plot Likelihood with Three Distributions ---\n",
    "    for mu, color, marker_color in zip(mu_values, colors, marker_colors):\n",
    "        y_values = norm.pdf(x_values, loc=mu, scale=1)\n",
    "        axes[1].plot(x_values, y_values, label=f\"μ={mu:.2f}, σ=1\", color=color)\n",
    "        axes[1].fill_between(x_values, y_values, alpha=0.2, color=color)\n",
    "        likelihood = norm.pdf(x_obs, loc=mu, scale=1)\n",
    "        axes[1].scatter(x_obs, likelihood, s = 60, color=marker_color, label=f\"L(μ={mu:.2f})\")\n",
    "\n",
    "    axes[1].axvline(x_obs, linestyle=\"dashed\", color=\"red\", label=\"Observed x=-0.3\")\n",
    "    axes[1].set_title(\"Likelihood: L(μ | x=-0.3, σ²=1)\")\n",
    "    axes[1].set_xlim([-4,3])\n",
    "    axes[1].set_xlabel(\"x\")\n",
    "    axes[1].set_ylabel(\"Likelihood\")\n",
    "    axes[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "# interactivity with a slider\n",
    "Prob_slider = FloatSlider(min=-3, max=3, step=0.1, value=-0.3, description='x:')\n",
    "like_slider = FloatSlider(min=-3, max=3, step=0.1, value=-0.8, description='μ:')\n",
    "interact(prob_like, obs=Prob_slider, like_mu = like_slider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one can observe from the visualization, the probability is the density of a certain or a range of value based on a single model, while the likelihood is the calculation of plausibility of a model based on an observation.\n",
    "##### Summary of Probability and Likelihood\n",
    "| **Aspect**         | **Probability**                                              | **Likelihood**                                              |\n",
    "|---------------------|--------------------------------------------------------------|-------------------------------------------------------------|\n",
    "| **Definition**     | Measures the chance of an event occurring given a set of fixed parameters $\\theta$ | Measures how plausible a model (represented by a set of parameter $\\theta $ )is given observed data. |\n",
    "| **Mathematical Form** | $P(D\\|\\theta)$ | $L(\\theta \\| D)$,sometimes $P(D \\| \\theta)$, **could be confusing** |\n",
    "| **Focus**         | Data as a random variable, parameters are fixed               | Parameters as variables, data is fixed                      |\n",
    "| **Interpretation** | Answers \"What is the probability of this event occurring?\"     | Answers \"How well does the parameters explain the observed data?\" |\n",
    "| **Application**    | Used for predictive modeling, calculating expected outcomes   | Used for parameter estimation (e.g., MLE, MAP) |\n",
    "| **Example**        | Probability of rolling a head with a fair coin: $P(head) = \\frac{1}{2} $ | Given that I rolled three heads in a roll, how likely is it that the coin is fair? |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Bayes' Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Definition\n",
    "Bayes's theorem is a rule to update the probability when new information is available. New information could be our assumptions, observations, and new data. Suppose $A$ and $B$ are two events, baye's theorem is expressed as the following:\n",
    "$$P(A|B)=\\frac{P(B|A)P(A)}{P(B)}$$\n",
    "where:\n",
    "- $P(A|B)$ is called the **Posterior**, the conditional probability of $A$ given $B$.\n",
    "- $P(A)$ is called the **Prior**, which is our **original** belief of $A$.\n",
    "- $P(B|A)$ is the likelihood, where is explains how well $A$ explains $B$.\n",
    "- $P(B)$ is a normalizing factor, usually called normalizing constant or the Marginal probability. It is **neglected** in some usecases including gaussian process, which will be explained later.  \n",
    "\n",
    "In machine leaning, $A$ in the equation above usually represents the **data** that we want to probe, and $B$ is usually either an **evidence**(some observation), or an **assumption** we made. Therefore, the equation is answering the quesion: given new information $B$, which could be evidence or assumption, how should we update our understanding of $A$?\n",
    "Bayes' Theorem is the most important concept toward Gaussian Process, as the training of gaussian process is highly dependent on the theorem.\n",
    "\n",
    "##### Difference between Conditional Probability\n",
    "In a previous example where we have the data of apperal products from different locations, the concept of conditional probability $P(A|B)$ was illustrated. Since the Bayes' Theorem is also calculating the conditional probability, but with a much more complicated formula, why don't we just use the simpler one?\n",
    "\n",
    "The key is that the previous example, where we know **everything** about the data, is extremely rare and nearly impposible in real life, because usually the dataset is way **bigger** and therefore way **harder** to acquire every piece of distribution information.Take the following problem as an example:\n",
    "\n",
    "**Q:** The overall rate of having a rare disease in an area is $2\\%$. The disease could be tested, but the test is only $95\\%$ accurate. Given a person is tested positive, how likely is that the person truly has the disease?\n",
    "\n",
    "**Analysis:** First of all, we want to know if we can calculate the result directly. Suppose we **Know** everyone who has the disease and that we are able to **make everyone do a test** and obtain the result, the calculation should be pretty simple, as we only need to calculate the conditional probability:\n",
    "$$P(Disease|Positive)=\\frac{P(Disease,Positive)}{P(Positive)}$$\n",
    "However, this is very unrealistic, majorly because the joint probability $P(Disease,Positive)$ requires unfathomably large amount of resource and time. Here is how Baye's Theorem can help us:\n",
    "\n",
    "**Bayes' Theorem Approach** With Bayes' Theorem, we first need to identify the terms.\n",
    "- Posterior $P(Disease|Positive)$:The probability of getting disease given that the test is positive.\n",
    "- Prior $P(Disease)$: The probability of getting a disease is $2\\%$, something we know.\n",
    "- Likelihood $P(Positive|Disease)$: How accurate the test results are. You can understand this term with the definition of likelihood: Disease is the data, and testing is the parameter. The term says how plausible the parameter \"testing\" is, which is the same as \"how well\" testing explains the data  \"disease\". Keep in mind that $L(Disease|Positive)=P(Positive|Disease)$\n",
    "- Marginal Probability $P(Positive)$: The probability of getting a positive. This is calculatable based on existing information (distribution of no disease can be inferred):\n",
    "$$P(Positive)=P(Positive|Disease)\\cdot P(Disease)+P(Positive|No Disease)\\cdot P(No Disease) \\\\ \n",
    "= 0.95\\cdot 0.02 + (1-0.95) \\cdot (1-0.02) = 0.068$$\n",
    "\n",
    "Then we can plug in these numbers:\n",
    "$$\n",
    "P(Disease|Positive)=\\frac{P(Positive|Disease)\\cdot P(Disease)}{P(Positive)}\n",
    "\\\\ =\\frac{0.95\\cdot0.02}{0.068}=27.9\\%\n",
    "$$\n",
    "\n",
    "Thus, the chance of actually getting the disease given that the testing is positive is only $27.9\\%$.\n",
    "\n",
    "Bayes' Theorem is very powerful when the understanding of the data is limited. When all the data is known (nearly impossible in many usecases), it is not that big of a deal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning(ML)\n",
    "#### 1.Maching Learning Essentials\n",
    "##### Machine Learning Setup\n",
    "The goal of machine learning is to **make predictions** through training. The setup of ML includes the following:\n",
    "- Feature Space $\\mathcal{X}$: the **input** variables (called features) to describe the datapoint. Each dimension of the feature vector represents a feature: $\\mathcal{X}\\in \\mathcal{R}^d, X_i=[X_i^1,X_i^2,\\cdots,X_i^d]$\n",
    "- Label Space $\\mathcal{C}$: The label space is the possible **outputs** (called labels) that the model aims to predict. The range of label is determined by the nature of the model:\n",
    "    - Classification: $\\mathcal{C}={0,1}$ or $\\mathcal{C}={Y_1,Y_2,\\cdots,Y_n}$\n",
    "    - Regression: $\\mathcal{C}\\in \\mathcal{R}$\n",
    "- Training Data $\\mathcal{D}$: the set of datapoints used during training. It is the combination of the features and labels: $\\mathcal{D}=\\mathcal{X}\\times \\mathcal{C}\\subseteq \\mathcal{R}^d\\times\\mathcal{C}$\n",
    "\n",
    "The rough idea of machine learning is that the datapoints $(X,Y)$ are drawn from some unknown distribution, and we would like to learn (train) a function $h(x)=y$, or $h(x)\\approx y$ that is true for any new pairs of $(X,Y)$.\n",
    "\n",
    "##### Lost Functions\n",
    "Previously we mentioned training a function $h(x)=y$ to make predictions for new pairs of $(X,Y)$. It is normally done by defining and minimizing lost functions. A loss function $\\mathcal{L}(h)$ is a mathematical function that quantifies how **well** a machine learning model’s predictions **match the actual target values**. Depends on the type of models, lost function can be the difference between the predicted output and the true label or other form. Purpose of a loss function includes:\n",
    "- Error Measurement: quantification of model's performance\n",
    "- Optimization Guide: During training, the parameters of the model are updated such that the lost function is minimized: $h(x)=argmin(\\mathcal{L})$\n",
    "- Comparing Models\n",
    "\n",
    "##### No Free Lunch Theorem\n",
    "The no free lunch theorem is a concept i Machine Learning where it states that **no single algorithm is universally better than all others**. This implies that it is impossible to find a single model that work absolutely best on all problems. For different tasks, we need to **make assumptions** on the data/system we are dealing with. For instance, in gaussian process, we assume that any finite set of value from the function that we try to model follows a multivariate gaussian distribution.\n",
    "\n",
    "##### Bias/Variance Trade-Off\n",
    "\n",
    "##### Training/Testing Split\n",
    "The idea of training/testing split is to divide a dataset into two separate subsets:\n",
    "1. Training Set: Used to train the model\n",
    "2. Testing Set: Used to evaluate the model's performance on unseen data. It is not used to train the model, but rather kept hidden in the training phase and fed to the model after training to validate the model.\n",
    "Training/Testing Split allows us to train a model while keeping necessary know data to evalute the performance. It helps prevent overfitting and measure generalization of the model. This could be done with the sklearn library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set: [[5, 7], [3, 5], [1, 3], [4, 6]] [50, 30, 10, 40]\n",
      "Testing Set: [[2, 4]] [20]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# dataset\n",
    "X = [[1,3], [2,4], [3,5], [4,6], [5,7]]\n",
    "y = [10, 20, 30, 40, 50] \n",
    "\n",
    "# Split into 80% training and 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # random_state could be blank\n",
    "\n",
    "print(\"Training Set:\", X_train, y_train)\n",
    "print(\"Testing Set:\", X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.Machine Learning Categories\n",
    "There are three big categories of machine learning.\n",
    "##### Supervised Learning\n",
    "The model learns from **labeled** data, meaning each training sample has both an input $X$  and a corresponding correct output(label) $y$. The model is evaluated based on how **well it predicts the correct outputs** on unseen data and therefore supervised learning could further be splitted into **Regression(continuous prediction)** and **Classification(discrete prediction)**. Gaussian Process is a supervised learning model of regression. There is also a category called the \"Semi-Supervised\" learning.\n",
    "##### Unsupervised Learning\n",
    "In unsupervised learning, the model learns from **unlabeled data**, meaning it does **not** have predefined outputs. The training set of unsupervised learning **does not** include a label for each feature input. The goal is to discover **patterns, structures, or relationships** in the data. Clustering is a type of unsupervised learning.\n",
    "##### Reinforced Learning\n",
    "In reinforcement learning, an **agent** interacts with an environment and learns by **trial and error**, receiving rewards or penalties based on its actions.\n",
    "\n",
    "<div style=\"text-align: center\"> <img src=\"https://analystprep.com/study-notes/wp-content/uploads/2021/03/Img_12.jpg\" alt=\"Drawing\" width=\"500\"/> </div>\n",
    "<div style=\"text-align: center\"> Supervised vs. Unsupervised Learning. In supervised learning, every datapoint has an accompanying label, as the star, circle, and cross. The model is trained to predict the label given an input feature vector. In unsupervised learning, datapoints are not labeled. The model is trained to discover underlying relationships among datapoints.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.Regression\n",
    "As mentioned above, supervised learning can be divided into regression and classification. Since Gaussian Process is a regression model, this notebook will neglect classification.\n",
    "\n",
    "Regression is a supervised learning technique used to model the relationship between input variables (features $X$) and a **continuous** output variable (Label $Y$).The goal of a regression model is to find a function that best predict $Y$, where $\\epsilon$ is the noise term, representing random errors:\n",
    "$$Y=f(X)+\\epsilon$$\n",
    "\n",
    "##### Linear Regression\n",
    "Linear regression is the simplest form of regression, where we assumea linear relationship between $X$ and $Y$, as well as a gaussian distribution of noise: $\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)$. A general form of linear regression is the following:\n",
    "$$Y_i=\\theta^T X_i+\\epsilon_i$$\n",
    "In the case of **1D linear regression(simple linear regression)**, the shape and size of $X$, $Y$, and $\\theta$ is:\n",
    "$$X=\\begin{bmatrix} 1 \\\\ X_1 \\end{bmatrix},Y= \\begin{bmatrix} Y_1 \\end{bmatrix}, \\theta=\\begin{bmatrix} \\theta_0 \\\\ \\theta_1 \\end{bmatrix}$$\n",
    "One **confusion** in linear regression notation is that the **intercept** is usually incorporated in the parameter vector $\\theta$. As shown above, even in a simple linear regression the model is actually $y=ax+b$, the general form is usually expressed as $y=\\theta_0+\\theta_1x$, and $\\theta_0,\\,\\theta_1$ is combined into one vector $\\theta$. In order to make the model work with this notation, an entry $X_0=1$ is added at the begining of the feature input, making its size **1 bigger than its original**. Therefore, in the case of **1D Linear Regression**, the feature and parameter **length** is **2**.\n",
    "\n",
    "To find the optimal $\\theta$, we need to utilize lost function mentioned above to evaluate the peformance of the model. The most popular loss function in linear regression is the **Mean Squared Error**:\n",
    "$$MSE=\\frac{1}{n}\\displaystyle\\sum_{i=1}^n (Y_i-\\hat{Y}_i)^2$$\n",
    "In the above equation, $Y$ is the label value from **training data**, and $\\hat{Y}$ is the predicted label value **according to the model**.\n",
    "\n",
    "Our goal is to find the veector $\\theta$ which **minimizes** the MSE function. This could be done by gradient descent, newton's method, or simplest in its closed form solution:\n",
    "$$argmin_{\\theta}(\\frac{1}{n}\\displaystyle\\sum_{i=1}^n (Y_i-\\hat{Y}_i)^2)=(X^TX)^{-1}XY^T$$\n",
    "\n",
    "To train a linear regression model, *sklearn* is a popular library. \n",
    "\n",
    "An **interactive example** of how MSE is related to the model peformance is shown below. For simplicity, the example below assumes a **1D linear model with intercept of 0**. Thus, slope is the only independent variable to calculate MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8467134d4594f98ac3c317b01b11544",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='Slope:', max=8.0, min=-5.0), Output()), _dom_classes…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_regression(slope)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, FloatSlider\n",
    "\n",
    "# Generate data\n",
    "np.random.seed(116)\n",
    "x = np.linspace(0, 10, 100)\n",
    "y = 2 * x + np.random.normal(0, 2, 100)\n",
    "\n",
    "# Define MSE function\n",
    "def f_mse(slope, x, y):\n",
    "    y_pred = slope * x\n",
    "    mse = np.mean((y - y_pred) ** 2)\n",
    "    return mse\n",
    "\n",
    "# plotting function\n",
    "def plot_regression(slope):\n",
    "    # Create a figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4),dpi=150)\n",
    "    \n",
    "    # Data and model line\n",
    "    ax1.scatter(x, y, alpha = 0.7, label='Data', color='blue')\n",
    "    y_pred = slope * x\n",
    "    ax1.plot(x, y_pred, label=f'Fitted Line (slope={slope:.2f})', color='red')\n",
    "    ax1.set_xlabel('x')\n",
    "    ax1.set_ylabel('y')\n",
    "    ax1.set_ylim([-10,30])\n",
    "    ax1.set_title('Data & Model Line')\n",
    "    ax1.grid()\n",
    "    ax1.legend()\n",
    "    \n",
    "    # MSE vs Slope\n",
    "    slopes = np.linspace(-5, 8, 100)\n",
    "    mse_values = [f_mse(s, x, y) for s in slopes]\n",
    "    mse_value = f_mse(slope,x,y)\n",
    "    ax2.plot(slopes, mse_values, label='MSE', color='green')\n",
    "    ax2.scatter(x=slope, y = mse_value, color='red', label=f'Current MSE ({mse_value:.2f})')\n",
    "    ax2.set_xlabel('Slope')\n",
    "    ax2.set_ylabel('MSE')\n",
    "    ax2.set_title('MSE vs Slope')\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# interactivity with a slider\n",
    "slope_slider = FloatSlider(min=-5, max=8, step=0.1, value=1, description='Slope:')\n",
    "interact(plot_regression, slope=slope_slider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.Maximum Likelihood Estimation(MLE)\n",
    "Maximum Likehood Estimation(MLE) is an approach to estimate model parameters. It treats the model parameters as **unknown values** and finds the parameters that maximize the **likelihood function**. \n",
    "\n",
    "Take the 1D linear regression problem as an example, we now estimate the best parameter by maximizing the likelihood. Remeber that we assume a **gaussian noise** $\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)$ on the model $\\hat{Y}=\\theta^T\\hat{X}+\\epsilon$(another example of the **no free lunch theorem**), which implies that the likelihood of the parameter given an observation $Y_i$ is:\n",
    "$$\n",
    "\\begin{align}\n",
    "L(\\theta|X_i,Y_i)=P(Y_i|X_i,\\theta) &=\\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp(-\\frac{(Y_i-\\hat{Y}_i)^2}{2\\sigma^2}) \\notag\n",
    "\\\\ &= \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp(-\\frac{(Y_i-\\theta^T X_i)^2}{2\\sigma^2}) \\notag\n",
    "\\end{align}\n",
    "$$\n",
    "As a result, the likelihood of observing a dataset is (also called the **Likelihood Function**):\n",
    "$$\n",
    "\\begin{align}\n",
    "L(\\theta|X,Y) &=P(Y|X,\\theta)=\\displaystyle \\prod_{i=1}^n\\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp(-\\frac{(Y_i-\\hat{Y}_i)^2}{2\\sigma^2}) \\notag\n",
    "\\\\ &=\\displaystyle \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp(-\\frac{(Y_i-\\theta^T X_i)^2}{2\\sigma^2}) \\notag\n",
    "\\end{align}\n",
    "$$\n",
    "This is **not easy to maximize**, so we take the log of our likelihood function and maximize the log instead:\n",
    "$$\n",
    "\\log L(\\theta|X,Y)=-\\frac{n}{2}\\log{(2\\pi\\sigma^2)}-\\frac{1}{2\\sigma^2}\\displaystyle\\sum_{i=1}^n (Y_i-\\theta^TX_i)^2\n",
    "$$\n",
    "Since $-\\frac{n}{2}\\log{(2\\pi\\sigma^2)}$ and $\\frac{1}{2\\sigma^2}$ are constants, maximizing the log likelihood is equivalent to miminizing:\n",
    "$$\n",
    "\\displaystyle\\sum_{i=1}^n (Y_i-\\theta^TX_i)^2\n",
    "$$\n",
    "This is exactly the Mean Squared Error. Therefore, MLE in linear regression is technically the same as MSE, because MSE **implicitly assumes a gaussian likelihood** function of Y. The resulting model from MLE is:\n",
    "$$\\hat{Y}=\\hat{\\theta}^T\\hat{X},\\text{where} \\,  \\hat{\\theta}=\\argmin_{\\theta}\\displaystyle\\sum_{i=1}^n (Y_i-\\theta^TX_i)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.Maximum A Posteriori (MAP)\n",
    "Maximum A Posterirori (MAP) is another approach to estimate paramters. Similart to MLE mentioned above, MAP also uses the idea of likelihood function, but it is maximizing the posterior, calculated by using **Bayes' theorem** and assuming a **prior** over the data under investigation.\n",
    "\n",
    "Take the 1D linear regression example again, remember that we are still using the likelihood function $P(Y|X,\\theta)$. But this time, we are using the Bayes' theorem to incorporate the likelihood function to calculate the **posterior**:\n",
    "$$P(\\theta|X,Y)=\\frac{P(Y|X,\\theta)P(\\theta)}{P(X,Y)}$$\n",
    "- $P(\\theta|X,Y)$: **Posterior**. The posterior can be interpret as \"how likely is that the parameter takes value $\\theta$ given observation $(X,Y)$\". Maximizing it returns the most possible value of $\\theta$ given observation $(X,Y)$. \n",
    "- $P(Y|X,\\theta)$: **Likelihood Function**, as in MLE.\n",
    "- $P(\\theta)$: **Prior** over parameter $\\theta$, is an assumption/understanding of the parameter $\\theta$.\n",
    "In real application, the term $P(X,Y)$ is usually dropped, as it is a constant regarding the posterior and we are only maximizing the posterior, not trying to get its value:\n",
    "$$P(\\theta|X,Y)\\propto{P(Y|X,\\theta)P(\\theta)}$$\n",
    "For the prior $P(\\theta)$, it is common to assume that $\\theta$ follows a **Gaussian** Prior:\n",
    "$$P(\\theta)=\\frac{1}{\\sqrt{2\\pi \\tau^2}}\\exp(-\\frac{||\\theta||^2}{2\\tau^2})$$\n",
    "Then the posterior becomes:\n",
    "$$\n",
    "\\begin{align}\n",
    " &P(\\theta|X,Y) =P(Y_i|X_i,\\theta)P(\\theta) \\notag\n",
    "\\\\ =\\displaystyle \\prod_{i=1}^n[\\frac{1}{\\sqrt{2\\pi \\sigma^2}}&\\exp(-\\frac{(Y_i-Y_i-\\theta^T X_i)^2}{2\\sigma^2})]\\cdot[ \\frac{1}{\\sqrt{2\\pi \\tau^2}}\\exp(-\\frac{||\\theta||^2}{2\\tau^2})]\\notag\n",
    "\\end{align}\n",
    "$$\n",
    "Taking the $log$ of the posterior:\n",
    "$$\n",
    "\\begin{align}\n",
    " &\\log P(\\theta|X,Y)\\notag\n",
    "\\\\ &=-\\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\displaystyle \\sum_{i=1}^n(Y_i-\\theta^T X_i)^2-\\frac{1}{2\\tau^2}||\\theta||^2\\notag\n",
    "\\end{align}\n",
    "$$\n",
    "Maximizing the $\\log$ of the Posterior is equal to minimizing:\n",
    "$$\\displaystyle \\sum_{i=1}^n(Y_i-\\theta^T X_i)^2+\\frac{\\sigma^2}{\\tau^2}||\\theta||^2\\notag$$\n",
    "where $\\frac{\\sigma^2}{\\tau^2}$ is usually simplified as $\\lambda$, the **regularization parameter**. The difference between MLE and MAP is the regulatization parameter, which is equivalent to **Ridge Regression (L2 Regularization)**. Regularization is a good way to avoid **overfitting**.\n",
    "\n",
    "The resulting model from MLE is:\n",
    "$$\\hat{Y}=\\hat{\\theta}^T\\hat{X},\\text{where} \\, \\hat{\\theta}=\\argmin_{\\theta}\\displaystyle \\sum_{i=1}^n(Y_i-\\theta^T X_i)^2+\\frac{\\sigma^2}{\\tau^2}||\\theta||^2$$\n",
    "##### Visualization of difference between MLE and MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d40a2cf8b1f49fbb28d7b0bd476ad5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=3.3, description='Slope:', max=8.0, min=-5.0), FloatSlider(value=0.0, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_regression(slope, lam)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, FloatSlider\n",
    "\n",
    "# Generate Linear data with noise\n",
    "np.random.seed(116)\n",
    "x = np.linspace(0, 10, 10)\n",
    "y = 2 * x + np.random.normal(0, 1, 10)\n",
    "\n",
    "# Add an outlier\n",
    "x = np.append(x,[4,5,6])\n",
    "y = np.append(y,[45,47,49])\n",
    "\n",
    "# Define MSE function\n",
    "def f_mle(slope, x, y):\n",
    "    y_pred = slope * x\n",
    "    mse = np.mean((y - y_pred) ** 2)\n",
    "    return mse\n",
    "\n",
    "def f_map(slope, x, y, lam = 1):\n",
    "    y_pred = slope * x\n",
    "    map = np.mean((y - y_pred) ** 2) + lam * slope**2\n",
    "    return map\n",
    "\n",
    "# plotting function\n",
    "def plot_regression(slope, lam):\n",
    "    # Create a figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4),dpi=150)\n",
    "    \n",
    "    # Data and model line\n",
    "    ax1.scatter(x, y, alpha = 0.7, label='Data', color='blue')\n",
    "    y_pred = slope * x\n",
    "    ax1.plot(x, y_pred, label=f'Fitted Line (slope={slope:.2f})', color='red')\n",
    "    ax1.set_xlabel('x')\n",
    "    ax1.set_ylabel('y')\n",
    "    ax1.set_ylim([-10,50])\n",
    "    ax1.set_title('Data & Model Line')\n",
    "    ax1.grid()\n",
    "    ax1.legend()\n",
    "    \n",
    "    # MSE vs Slope\n",
    "    slopes = np.linspace(-5, 8, 100)\n",
    "    mse_values = [f_mle(s, x, y) for s in slopes]\n",
    "    mse_value = f_mle(slope,x,y)\n",
    "    map_values = [f_map(s, x, y, lam = lam) for s in slopes]\n",
    "    map_value = f_map(slope,x,y, lam = lam)\n",
    "    ax2.plot(slopes, mse_values, label='MLE', color='green')\n",
    "    ax2.scatter(x=slope, y = mse_value, color='red', label=f'Current MLE ({mse_value:.2f})')\n",
    "    ax2.plot(slopes, map_values, label='MAP', color='orange')\n",
    "    ax2.scatter(x=slope, y = map_value, color='purple', label=f'Current MAP ({map_value:.2f})')\n",
    "    ax2.set_xlabel('Slope')\n",
    "    ax2.set_ylabel('MSE/MAP')\n",
    "    ax2.set_title('MSE vs MAP')\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# interactivity with a slider\n",
    "slope_slider = FloatSlider(min=-5, max=8, step=0.1, value=3.3, description='Slope:')\n",
    "lam_slider = FloatSlider(min=0, max=10, step=0.1, value=0, description='Lambda:')\n",
    "interact(plot_regression, slope=slope_slider, lam=lam_slider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In above visualization above, the dataset has a size of 13, and 3 of them are outliers. The lambda (regularization parameter) value is default set to 0 (which makes MAP equal to MLE). As shown above, when MLE is minimized, the model is overfitted and largelly affected by the outliers. As the regularization parameter increases, it gradually eliminates the impact brought to the model by the outliers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.True Bayesian Prediction\n",
    "You may notice that under MLE and MAP, we are only making **point estimation**. That means, with each input feature vector $X$, the model only give one corresponding prediction label $Y$. If you recall the description of gaussian process at the beginning of this notebook, you notice that gaussian process not only provide predictions, but also **uncertainties**. In fact, providing uncertainties along with predictions are very beneficial in decision making, and gaussian process is not the only model that incorporates the idea.\n",
    "\n",
    "In order to enable the model to provide uncertainty, a \"true bayesian prediction\" can be done treating the likelihood function $P(Y|X,\\theta)$ differently. \n",
    "\n",
    "In MAP, we maximize the posterior, which is related to the likelihood function, to find the **most probable** set of parameter, then plug the only parameter result into the model. The most important assumption we make is the **prior**, where we assume the parameter $\\theta$ is **Gaussian Distributed**.\n",
    "\n",
    "If we integrate over every possible set of parameter, no matter what distribution the parameter follows, we get a \"distribution of Label Y\" over parameter, providing both the prediction and the uncertainty:\n",
    "$$P(\\hat{Y}|X,Y,\\hat{X})=\\int_{\\theta} P(\\hat{Y}|\\theta,X,Y)P(\\theta|X,Y)d\\theta$$\n",
    "\n",
    "Some notes:\n",
    "- $X$, $Y$ are training data (input), $\\hat{X}$ is the testing input to the model(features whose label $\\hat{Y}$is to be predicted).\n",
    "- Different from MLE and MAP, whose output is a model $\\hat{Y}=\\hat{\\theta}^T\\hat{X}$, the output of true bayesian prediction is a **probability distribution of predicted label $\\hat{Y}$**.\n",
    "- The integration is **very computationally complex** and may **not have a closed form** in many circumstances.\n",
    "- If we assume a gaussian prior over parameter $\\theta$ (as we did in MAP), the integration will be calculatable with a **closed form**. A gaussian prior is usually safe, and popular to have, because of Central Limit Theorem. <span style=\"color: red;\">This is one key reason why gaussian process works.</span>\n",
    "\n",
    "##### True Bayesian Linear Regression\n",
    "Let's take linear regression as an example to illustrate how true bayesian prediction work. As a starting point, we define the likelihood function and gaussian prior over parameter $\\theta$ as we did in MAP:\n",
    "$$\\begin{align}\n",
    "P(Y|X,\\theta)&=\\displaystyle \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp(-\\frac{(Y_i-\\theta^T X_i)^2}{2\\sigma^2})\n",
    "\\\\ P(\\theta)&=\\frac{1}{\\sqrt{2\\pi \\tau^2}}\\exp(-\\frac{||\\theta||^2}{2\\tau^2})\n",
    "\\end{align}$$\n",
    "Althouth it is mentioned above that true bayesian prediction can deal with all parameter distribution, a gaussian prior is still selected in this example because the integration with a gaussian prior has a **closed form**. This makes true bayesian prediction the **same as MAP**, but with additional uncertainty information.\n",
    "\n",
    "Notice that both the likelihood function and prior are gaussian distributed:\n",
    "$$\\begin{align}\n",
    "P(Y|X,\\theta)&\\sim \\mathcal{N}(\\theta^T X,\\sigma^2I) \\\\\n",
    "P(\\theta)&\\sim \\mathcal{N}(0,\\tau^2I)\n",
    "\\end{align}$$\n",
    "\n",
    "because the posterior $P(\\theta|X,Y)$ is the **product of two gaussians**, it is also gaussian. We did not bother to state this in MAP because we only care about maximizing it, not its distribution.\n",
    "$$P(\\theta|X,Y)\\propto{P(Y|X,\\theta)P(\\theta)}\\sim \\mathcal{N}(\\mu_{\\theta},\\Sigma_{\\theta})$$\n",
    "To obtain the **posterior distribution** mean and variance, we calculate:\n",
    "$$\\begin{align}\n",
    "P(\\theta|X,Y)&\\propto{P(Y|X,\\theta)P(\\theta)} \\notag \\\\ \n",
    "\\log{P(\\theta|X,Y)}&=\\log{P(Y|X,\\theta)}+\\log{P(\\theta)} \\notag \\\\\n",
    "&=-\\frac{1}{2\\sigma^2}(Y-\\theta^TX)(Y-\\theta^TX)^T-\\frac{1}{2\\tau^2}\\theta^T\\theta + C\\notag \\\\\n",
    "&=-\\frac{1}{2\\sigma^2}YY^T+\\frac{1}{\\sigma^2}YX^T\\theta-\\frac{1}{2\\sigma^2}\\theta^TXX^T\\theta-\n",
    "    \\frac{1}{2\\tau^2}\\theta^T\\theta \\notag + C\\\\\n",
    "&=-\\frac{1}{2}[\\theta^T(\\frac{XX^T}{\\sigma^2}+\\frac{I}{\\tau^2})\\theta-2\\theta^T\\frac{XY}{\\sigma^2}] + C\n",
    "\\end{align}$$\n",
    "\n",
    "What is the expression of the mean and covariance matrix of the predictive distribution one gets from performing bayesian linear regression on a model $Y=\\theta^T X + \\epsilon$, where the noise epsilon follows a gaussan $\\mathcal{N}(0, \\sigma^2)$, and the prior distribution $P(\\theta)$ follows $\\mathcal{N}(0, \\tau^2*I)$? The predictive distribution is $P(\\hat{Y} | \\hat{X}, X,Y)$ where $X$ and $Y$ are training data, and $\\hat{X}$ and $\\hat Y$ are prediction input and output. In the last step of integration, explain how to peform it, and in everystep include the sizes of all matrices to ensure that the calculation is correct. Note that $X$ and $X_hat$ are $d*n$ and $d*m$, $Y$ and $Y_hat$ have shape $1*n$ and $1*m$.\n",
    "\n",
    "Recognizing that for a multivariate gaussian distribution\n",
    "$$P(X)=\\frac{1}{(2\\pi)^{n/2}|\\Sigma|^{1/2}} exp(-\\frac{1}{2}(X-\\mu)^T\\Sigma^{-1}(X-\\mu))$$\n",
    "Its $\\log$ density is:\n",
    "$$\\log{P(X)}=-\\frac{n}{2}\\log{2\\pi}-\\frac{1}{2}\\log{|\\Sigma|}-\\frac{1}{2}(X-\\mu)^T\\Sigma^{-1}(X-\\mu)$$\n",
    "And the **First two terms are constant $C$, unrelated to target variable $X$**.\n",
    "Therefore, comparing the general form of multivariate gaussian log density and our derivation of the log likelihood function, we recognize the covariance and mean of the likelihood function:\n",
    "$$\\begin{align}\n",
    "\\Sigma_{\\theta} &= (\\frac{XX^T}{\\sigma^2}+\\frac{I}{\\tau^2})^{-1} \\notag\n",
    "\\\\ & = \\sigma^2(XX^T+\\lambda I)^{-1} ,\\, \\text{introduce}\\lambda = \\frac{\\sigma^2}{\\tau^2}\n",
    "\\\\ \\mu_{\\theta} &= (\\frac{XX^T}{\\sigma^2}+\\frac{I}{\\tau^2})^{-1} \\frac{XY^T}{\\sigma^2} \\notag\n",
    "\\\\ & = (XX^T+\\lambda I)^{-1} XY^T\n",
    "\\\\ \\text{Therefore,  } P(\\theta|X,Y) &\\sim \\mathcal{N}[(XX^T+\\lambda I)^{-1} XY^T,\\sigma^2(XX^T+\\lambda I)^{-1},]\n",
    "\\end{align}$$\n",
    "\n",
    "From here, we can continue to compute the **Predictive Distribution** of linear regression:\n",
    "$$P(\\hat{Y}|X,Y,\\hat{X})=\\int_{\\theta} P(\\hat{Y}|\\theta,X,Y)P(\\theta|X,Y)d\\theta$$\n",
    "In previous step, we derived the distribution of the **Likelihood Function** $P(\\theta|X,Y)$. To complete the integral, we also need to define $P(\\hat{Y}|\\theta,X,Y)$, which we defined as a **Linear Model**:\n",
    "$$\\begin{align}\n",
    "\\hat{Y}=\\theta^T\\hat{X}+\\epsilon,\\,\\text{where}\\,\\,\\epsilon\\sim\\mathcal{N}(0,\\sigma^2)\n",
    "\\\\ P(\\hat{Y}|\\theta,X,Y) \\sim \\mathcal{N}(\\theta^T\\hat{X},\\sigma^2)\n",
    "\\end{align}$$\n",
    "The term under integration resulted in: where C is the product of the coefficients from the PDF of gaussian distribution.\n",
    "$$P(\\hat{Y}|\\theta,X,Y)P(\\theta|X,Y)= C\\cdot \\exp(-\\frac{1}{2}[\\theta^T(\\Sigma_{\\theta}^{-1}+\\frac{\\hat{X}\\hat{X}^T}{\\hat{\\sigma}^2})\n",
    "      \\theta-2\\theta^T(\\Sigma_{\\theta}^{-1}\\mu_{\\theta}+\\frac{\\hat{X}\\hat{Y}^T}{\\hat{\\sigma}^2})])$$\n",
    "The integration of this term is **also gaussian**, where its corresponding mean and coraviance matrix is:\n",
    "$$\\begin{align}\n",
    "\\hat{\\mu}&=YX^T(XX^T+\\lambda I)^{-1}\\hat{X}=\\mu_{\\theta}^T\\hat{X}\n",
    "\\\\ \\hat{\\Sigma}&=\\sigma^2(I+\\hat{X}^T(XX^T+\\lambda I)^{-1}\\hat{X})=\\sigma^2(I+\\hat{X}^T\\Sigma_{\\theta}\\hat{X})\n",
    "\\end{align}$$\n",
    "Here, we finished establishing a linear regression model with true bayesian prediction. **In this model, $the mean \\hat{\\mu}$ is our prediction, and the covariance matrix $\\hat{\\Sigma}$ contains the uncertainty information.** In order to extract the **standard deviation** of the prediction, one can simply take the diagonal of the covariance matrix:\n",
    "$$STD = \\sqrt{diag(\\hat{\\Sigma})}$$\n",
    "\n",
    "##### Sizes of all matrices involved in previous examples\n",
    "<div align=\"center\">\n",
    "\n",
    "| Matrix         | Description                           | Size   |\n",
    "|---------------|-------------------------------------|--------|\n",
    "| $X$             | Training input matrix               | $d × n$  |\n",
    "| $Y$             | Training output vector             | $1 × n$  |\n",
    "| $\\theta$             | Parameter vector                   | $d × 1$  |\n",
    "| $\\hat{X}$    | Prediction input matrix                  | $d × m$  |\n",
    "| $\\hat{Y}$    | Prediction output vector                 | $1 × m$  |\n",
    "| $\\Sigma_{\\theta}$          | Posterior covariance               | $d × d$  |\n",
    "| $\\mu_{\\theta}$            | Posterior mean                     | $d × 1$  |\n",
    "| $\\lambda$             | Regularization factor (scalar)     | scalar |\n",
    "| $I$             | Identity Matrix                    |Square Matrix|\n",
    "| $\\hat{\\mu}$          | Predictive mean                     | $1 × m$  |\n",
    "| $\\hat{\\Sigma}$          | Predictive covariance              | $m × m$  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A python code example of the true bayesian linear regression is shown below**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAw8AAAJKCAYAAABqPnm9AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAABcSAAAXEgFnn9JSAADEdElEQVR4nOzdd3xT1fsH8E+696aUtrTssrcIlFFaGYIgogIiKCAI7q3oFxXUnwMFwQGyFEHZuEBUsKXsvfcotGVDKW3adCa5vz+ON6NN2rRNm6b9vF+vvijJvTcn6c2997nnOedRSJIkgYiIiIiIqBQOtm4AERERERHZBwYPRERERERkEQYPRERERERkEQYPRERERERkEQYPRERERERkEQYPRERERERkEQYPRERERERkEQYPRERERERkEQYPRERERERkEQYPRERERERkEQYPRERERERkEQYPRERERERkEQYPRERERERkEQYPRERERERkEQYPVCPExMRAoVBg2rRptm5KlRg7diwUCgXGjh1r66YQlWjJkiVQKBRo0KCBrZtCJjRo0AAKhQJLliwp9pxCoYBCoUBiYmKVtmnatGlQKBSIiYmp0tetTkr6u1QHJZ2D7OF8nJycrNu/k5OTbd0cu8PgoZqSD56mfjw8PNC0aVM8+eST2LVrl62bShVg+Hcm6zH3/XF1dUVoaCj69++PRYsWobCw0NZNJTsmXyQV/fHy8kKzZs0wduxY7Nmzx9bNrDaOHDmCadOmYfbs2bZuitUZXowa/ri5uSE4OBgtW7bEyJEjMWvWLFy9erXK2zd79mxMmzYNR44cqfLXrmrTpk3DtGnTGBRUIidbN4BKV7duXd3vWq0W6enpuHDhAi5cuIClS5fi/fffr9YRflWIiIhAVFQUgoKCbN2UKlGvXj1ERUWhXr16tm5KtWf4/cnKysL169dx/fp1bNq0CfPnz8emTZvg7+9vwxbWbL6+voiKikJYWJitm1JpnJ2dERAQoPt/Wloazp8/j/Pnz+uO0e+//74NW1g+UVFRAAAPDw+rbO/IkSOYPn06IiMj8fLLL5tdLigoCFFRUYiIiLDK61Y1Hx8fuLu7AwA0Gg0yMjJw+/ZtnD59GqtWrcKbb76JESNGYM6cOWbPWY0bN4abmxt8fX2t0qbZs2cjJSUFDRo0QPv27Su8vep8Dpo+fToAEdyb6/F0dnbW7d/Ozs5V1bSaQ6Jq6f3335cASKb+RGq1WtqxY4fUqVMn3TI7d+60QSupokr6O1P5lfS5pqSkSBMnTtQ9P3r0aBu0kGqC3r17SwCk3r17Gz2en58vbdq0SWrSpIluP9uwYYNtGlmKyMhICYD0ww8/VPpr/fDDDxIAKTIystJfq6pdunRJ97c29Vleu3ZNWrdunXT//ffrlgsNDZUuXbpUJe2ryr+z/L14//33K/21TJE/3y1bttjk9WsDpi3ZIUdHR0RHR+O3337TPfb777/brkFEdiQiIgILFixAXFwcAGD16tXIzs62cauoJnFxcUHfvn3x+++/w8XFBQDwzTff2LhVZEv16tXDsGHDsHHjRqxatQrOzs64du0aBg0aBLVabevmEZUJgwc7Fh4ejsDAQAAwefFTWFiIzZs348UXX0Tnzp1Rr149uLi4IDg4GP3798eKFSsgSVKx9aZMmQKFQoFWrVqV+PpKpRJeXl5mB3Xl5eXhq6++Qu/evREUFAQXFxeEhIRg6NCh+Pvvv81uNzc3F1988QW6desGf39/ODs7o06dOmjZsiWefPJJrFu3rtg6JQ3QunXrFr7//nsMGzYMLVq0gK+vL9zd3dGkSRNMmDABJ0+eNNuWooPC1q5di5iYGAQEBMDDwwPt27fHnDlzoNVqS/ysrM3SwWqSJGHhwoW499574ePjA29vb3Tr1g0//fRTqa+RlJSEF154AS1atICXlxc8PDzQokULvPzyy0hNTTW5jlarxc6dOzFlyhR07doV4eHhcHFxQWBgIHr37o3vvvvO7DiDogPYkpKS8PTTT6Nhw4ZwdXW1+oDb/v37AwAKCgpw/vx5k8uUdx8GAJVKhffffx8tWrSAu7s7goODMXDgQMTHxwMwPyCyPJ/Db7/9hqFDhyI0NBQuLi7w9/dHr169Svy8ARE43X///ahbty6cnZ3h5+eHpk2bYsiQIfj222+Rl5dXbJ1//vkHw4YN0/1tfXx80KhRI/Tr1w9ffPEF0tPTjZa3ZMB0UlISnnnmGTRt2hTu7u7w8fFBx44d8cEHH0CpVJpcJzEx0Wi80IULFzB+/HjUr18frq6uCA8Px8SJE22SXy5r2bIlOnXqBADYv3+/7vGin8mWLVswdOhQ1KtXD46OjsW+1xXZD3Nzc/HRRx+hZcuWJvfDklgyYHrTpk0YOXIkIiMj4e7ujoCAALRt2xYvvPACdu/ebbStcePGAQBSUlKKjQ0wPHabGjBdWFiIOnXqQKFQ4Kuvviqx3YsXL4ZCoYCPjw9ycnKKPV+eY5s1DR8+HB9//DEA4NSpU/jxxx+LLVPSgOmynCPlzzIlJQUAMG7cuGKfvawsxx5LJ+0oKCjAp59+irZt28LT0xP+/v7o27cv/vrrL7PrWLLfmTrny22S9enTx+h9GrbfkgHTmZmZ+OCDD9CxY0ddKlrTpk3xzDPP4OLFixa1PysrC1OnTkXz5s3h7u6OwMBAPPDAA9i7d6/Z9e2Crbs+yDRL0lmuXLmiW2bOnDnFnt+yZYvueQCSq6ur5OXlZfTYo48+Kmk0GqP1Ll68KCkUCgmAtH37drOvP2/ePAmA5OvrK6lUKqPnzp07JzVt2lT3OgqFQvL19TV67WeeeabYNpVKpdSuXTuj9fz8/CQnJyfdY6a6vEvqJn3yySeNXtfHx8doe66urtLatWtNvkd53SeffFJ67rnnJACSg4OD5OfnZ7TNJ554wuznVJLypi0Ztqso+bOYOnWq9OCDD0oAJCcnJ8nHx8eoze+9957Z7S9YsEBydnY2+ozc3d2NPsNNmzYVW8+w697c6/bs2VPKyckpcd2ff/5Zt696eHhInp6eZUp1sORz/eyzz3TL7N+/v9jz5d2HJUmSbt68KbVs2VK3nLOzs26fUSgU0rx588ymEZTlc8jKypIeeOCBYvu3/P0FIHXr1k1KT08v1sbx48cbrefl5SV5eHgYPVY0pWL69OlGz3t4eBQ7phRNFSgtVWXVqlWSq6urbn1vb2+j/9evX186depUsfUMj28JCQm6dnh7ext9v0NDQ6UrV66YfG35u1LeNBpzaUuGHn30Ud0+IDP8TObMmaP7e/n6+krOzs5G3+uK7Id37tyROnToYPR9NNwP586dW2I6i7m/qSRJkkql0r03c3+7du3a6ZavW7eu7ljg4OAg1a1b1+jn888/1y0rf3+Lfq7yMbhz585mP29JkqSYmBgJgDR27Nhiz5X32Faa0tKWisrNzZWCgoJ0x8SizP1dynqO/Pzzz6W6detKDg4OuvdX9LM39R5KO/ZYcg56++23pZ49exbb9+Qfc2lNJe13RV/DcBsvvviiVLduXd36/v7+Ru/TcL8xfK+mUsdOnDghhYeH65Zxc3OTvL29LbpukJdZvny5LnXRzc3N6Pjq7Ows/f3332bfX3XH4KGaKm3Mw65du6R77rlHAiAFBwdLd+/eLbbcnj17pFGjRkl//vmndOPGDUmr1UqSJE4oc+bM0R3ITQUeAwYMkICSL4o7duwoAZCef/55o8fv3r0rNWjQQAIgxcbGStu2bZPy8vIkSZKkjIwMadasWbqD0uzZs43W/fDDDyUAUkBAgLRu3TrdehqNRrp69aq0dOlSaeLEicXaUlLwMG3aNGnq1KnS4cOHpezsbN32Tpw4IT3++OMSAMnT01O6evVqsXXlA6S/v7/k4uIizZo1S8rMzJQkSZLS0tKkCRMm6P5O8fHxZj8rcyozePD395d8fX2lJUuW6C7WL1++LA0ePFh3Aj937lyx9X/99VfdwW3KlClScnKypNVqJa1WK505c0Z3weDj4yOlpKQYrXv58mXpwQcflFatWiVdvXpVF5hmZWVJP/zwgxQaGioBkF555ZVir2t4MPfy8pLuvfdeo4v6s2fPWvz5WPK5xsbG6k6+aWlpRs9VZB+WJP33x93dXVq8eLFu3dTUVGnEiBGSi4uL7kRSUvBQ2ucwdOhQCYDUpEkTafny5ZJSqZQkSVyY/P7771KjRo0kANLQoUONXmP79u26feCzzz6T7ty5o3suLS1N+ueff6Qnn3zS6DuRnJysuwB59dVXjZ7LyMiQtm/fLj377LPSgQMHjF6rpODh4MGDugu56Oho6ejRo5Ikie/nH3/8IdWrV08CIDVu3FjKysoyWtcwePD395eGDBkinT59WpIkMeZg1apVupP9mDFjir22JFVN8CAfpw0v0uTPxM3NTXJ0dJTGjh0rpaamSpIkju8XLlyQJKni++FDDz2ku9D57rvvpNzcXEmSxN/yoYcekpydnc3uh5JU8kXc8OHDdfvQW2+9JV2+fFmSJEnSarXSlStXpJ9//lmaPHmy0TqWjnkwFzzs3btX1yb5b11USkqKLhhLSEgweq4ix7bSlDV4kCT9Z+ji4qL728jMBQ/lPUdaMuahLMceS85Bvr6+xfa91NRU6ZFHHtG9zu+//15s/fIGD2VZv6TgQalUSg0bNpQASGFhYdKff/6pO5cdOXJE6tq1q+57deTIEbOv7+/vL7Vs2VJKSEiQNBqNpNVqpX379klRUVG670HRm7f2gsFDNWV48WMYOdepU0dydHTUHeAef/xxKTk5uVyvsWbNGt2JuajffvtNd/FjKjA5ePCgrn3Hjh0zeu7111/XnewKCwtNvvYvv/wiAZCCgoKMlpEHk3388cdlei8VGaA1aNAgCYD04YcfFnvOsNfC3EFXHrg+YcKEMr92ZQYPpk6ekiRJeXl5uov4jz76yOi5/Px8KSwsTAIgLV682OzrDxkyRAIgvfTSS2Vq9/79+3XBWtGTpeHBPDIystjFYlmUZcD0kCFDii1TkX1YvjAHIC1btqzYehqNRurTp4/Z/crSz2HDhg0SACkkJMTsnfXLly9Lnp6eEgDp8OHDusflXpd+/fqZXM+UVatWSQCkZs2aWbyOJJV8wSgHWU2aNCnWeylJknTo0CHdHVXDO9OSZBw89OnTx+RJ+KuvvtIdx0z9HSs7eNi7d68u4HrwwQd1j8ufCQBp2LBhZrdfkf3Q8ELb1HdZrVZLPXr0KPH4Zu4i7N9//9U9N3fuXLPtL6qiwYMkSboLr7ffftvkuh9//LEEiB4r+YaZJFX+sa08wcP//d//6dY5f/680XPmLvbLe44sa/BQ2jHY0nOQqc9ao9FIvXr1kgBILVu2LPa8rYOHTz/9VBdkHj9+vNi6SqVSF9QPGjTI7OvXqVNHunnzZrHnjx07pltmx44dZttYnXHMgx24efOm7uf27dvQaDQAgJycHGRmZuLmzZvl2u6gQYMAiPzP69evGz33wAMPoH79+sjNzcWyZcuKrbtgwQIAQLdu3dCmTRvd45Ik4fvvvwcAvPbaa3ByMj0b8NChQ+Hj44O0tDQcPHhQ97ifnx8AFGtPZZI/hx07dphdpn79+njiiSdMPjdkyBAAwLFjx6zfuAqIjo5Gnz59ij3u6uqqy/cv2ua//voLV69eRd26dXX5yabIn8U///xTpjZ17twZwcHBUKlUJc43/vzzz8PLy6tM2zYnJCRE9+Pp6YnIyEgsXLgQANC8eXPMnTvXaPmK7sNr1qwBIHKWH3/88WLrOTg4YOrUqRa1vaTPYdGiRQCAMWPGmJ0GNTw8XLcPGP6t5O+Z4fGkNPI6WVlZUKlUFq1TkoyMDF2b3njjDZPTgXbo0AHDhg0DAKxYscLstt555x04OBQ/nT344IMARI64qXEtiYmJkCTJ6vPBX7t2DcuWLcODDz4IrVYLhUJhdmrSt99+2+TjFd0PV65cCUAcu0x9lx0dHfHuu++W5W3pyO1q1aoVnnnmmXJto7zGjBkDAPj5559NjtmTz1ejR482yn+v7GNbeRhO71t0rJA5VXWOtMYx2Ny+Z3gMPHXqFI4fP16h17G2VatWAQAeeeQRtG7dutjz3t7eePPNNwGI/SozM9Pkdp5++mkEBwcXe7xNmzZo2LAhgOp33WAp1nmwA0UPkHl5eThz5gy++eYbLF68GJs3b8bKlSsxdOjQYutmZWXhu+++w4YNG3D69GlkZGSYHEB59epVo/maHR0dMWHCBLz//vtYuHAhXnjhBd1zKpVKdyJ/+umnjbZz6tQp3UFw7NixJk/oMnmQd0pKCu69914AImhZsWIFvvnmG9y+fRsjRoxAjx49Kly/4ejRo5g/fz527NiB5ORkZGdnF/tcr1y5Ynb9e+65x+x7CQ0NBWD5wb+qyJ+pKebaLAdQd+/eLXH+7oKCAgDQDcIr+tz333+PX375BSdOnEB6ejry8/OLLVfS5x0dHW32ubIyF1w/8cQTmD9/Ptzc3Iwer+g+fOjQIQBAr169zBb/i46OhpOTU6mzrJT0Och/qwULFmDp0qVml5NPbIZ/q/vuuw9ubm44fPgwevbsiaeeegqxsbG6E5opXbp0QVBQEK5fv457770XkydPxn333YeoqKhyFTk8dOiQ7jt43333mV2ub9++WL16NY4dO4bCwkKTc7Kb29fl/Ryo3O/n1q1bzX4Gzs7OmDVrlslqye7u7ujYsaPJ9Sq6Hx44cACAfmCpKb169bJoPyxKLk46ePDgMq1nDWPGjMG7776L1NRUbN261ehzPXjwIE6fPg0AxW72WOPYZm2mgp/SVNY5sihrHIMt3fcOHDhgdBPSlgoKCnQX9KUdlwAxScihQ4dM3qgr7Rx86dKlanfdYCkGD3bIzc0N7du3x6JFi5Ceno5ff/0VY8eORWpqKnx8fHTLnTt3DnFxcUYXaR4eHvDz89OdiOQLK1N3EidMmIAPP/wQx48fx549e9C1a1cA4o6WUqmEn58fhg8fbrTOtWvXdL/fvn3bovdjOBvGqFGjsG/fPnz99ddYuXKl7u5ZkyZN0K9fP4wfP143e4mlvvnmG7z00ku6GZEUCgV8fX3h6uoKQNyVVCqVJd5N9fb2NvucfEewulUrLk+b5b9fQUGBRT1aubm5Rv+/desW7rvvPqM7SW5ubggKCoKjoyMAsV9otdoSP29Td2vKSz5BS5KEGzdu4I8//sCUKVOwdOlStG7dGm+88YbR8hXdh+V1DC9ci3J1dUVQUBBu3LhR4nbNfQ6FhYVIS0sDIIIDc3e+zLWxUaNGWLRoESZPnozdu3frZsWpU6cO+vTpg1GjRmHIkCFGJ34/Pz+sWLECo0aNwsmTJ3U3FHx9fdGrVy8MHz4cI0aMsLjg0q1bt3S/l1RALjw8HACgVquRnp5uVPRPZm5fN7xbX5nfT8MicQqFAu7u7ggNDUX37t0xYcIENGvWzOR6gYGBZoOCiu6H8udb0mfr5uaGwMDAMvdey/ttZGRkmdazhoiICPTu3RuJiYlYtmyZUfAg9zrcc889aN68udF6FT22VYa7d+/qfpdnTixNZZwjTbHGMbikfc/V1VW37xkeC2wtPT1d1xtryXEJgNn22+N1g6WYtmTnJk6cCEBcQGzcuNHouXHjxuHKlSto0KAB1qxZgzt37kClUuHWrVu4ceOG0RSGpu6AhIaG6lJy5DQlALqUj9GjRxdLNTBMgbhx4wYkMa6mxJ+iU73Nnj0bZ8+exccff4z7778ffn5+uHDhAubOnYvOnTuXWJm0qNOnT+Pll1+GVqvFo48+in379iEvLw93797FjRs3cOPGDcyaNcvsZ1DbyH+/AQMGWPS3K/qZvfLKKzh+/DgCAwPx/fff4/r168jNzcXt27d1n7d8UV3S5y0HGtakUChQr149TJo0Cb/++isUCgXeeustJCQkGC1X0X1Yfl+l3Y23ZH8z9zkYtnHlypUWtbHolI+PP/44UlJS8N1332HEiBGoX78+bt++jdWrV2Po0KHo3bt3sWlS77vvPly6dAlLly7Fk08+iaZNmyIzMxPr16/HmDFj0KFDh0qdGrU8PRxVoXv37rr9+/r167h48SJ27NiBGTNmmA0cgJL3c2scS4HK+czkbdrq7yGnLq1du1Z3ka9Wq3U94qZSTCt6bKsMR48eBSAupMtSgd2a50hzrHEMrq7fV0uV1H7D5+z9fZYHgwc7Z3jn59KlS7rfL1++rOtaXrFiBR555BGj/EoApd71BIDJkycDEDmASqUSx48f181PXDRlCRD55bKK5DE2adIEb7/9NjZu3Ig7d+5g9+7durSsOXPm4I8//rBoO2vXroVGo0GLFi2wcuVK3HPPPbqiTTJLPofaQv77ledvV1hYiF9++QWA6O0ZN26c0f4AiBO4fMfclmJiYjBmzBhIkoTnn3/e6EKtovuwfMfO8M5xUfn5+bhz506Zty1zc3ODr68vgIp9zwICAjBp0iSsXLkSqampuHDhgq7Oy/bt203WTfH09MSYMWOwZMkSnDt3DleuXMFnn30GNzc3ox6J0hje2SwphU1+zsnJCf7+/mV7g3bMWvthSZ9tefdDuW3WHitiqUcffRTu7u5QKpW6AqmbNm3CrVu34OzsjJEjRxZbpyLHtsqQl5enu3HRtWvXYumTpbHWObIyWbrvFe3lkAMXU3VmZJb0tpZHQECA7vUvX75sdjnD5+rUqVMpbanOGDzYOcMvp6enp+53wx27Q4cOJtf9999/S93+fffdhyZNmiAnJwc///yzrteh6EBpWevWrXWpU3J3akU5ODiga9euWLt2LSIiIgAAmzdvtmhd+XNo166d2fQASz6H2kLOc7169WqJA8hNuX37tu5gb26f27FjR4knhKr03nvvwdHREadPnzYq0lTRfVjOYd+6davZZXbu3FnhqrLy32rNmjVWK1LYuHFjfPLJJxg1ahQAy75nYWFhePPNN/Haa69ZvA4gPif5O1lSwTL5+9muXTuLU6Jqgoruh507dwYg9kNzd9G3bdtWrv2we/fuAID169eXaT35713Ru/re3t66C2U5VUn+9/777zeZ/1+RY1tl+Oabb3Q3UkortFYaS86R1vrsy6KkfW/79u26fU/eV2XyTQJzF+9ZWVm6sS2myD0B5XmvLi4uaNu2LQDLjksODg5mxy3VZAwe7Nzy5ct1vxt+AeW7koC+a9RQVlYWPvroo1K3r1AoMGnSJADA3LlzdZWJTfU6AOLu4Pjx4wEAP/74Y6kH6aKDhUwNrJU5Ojrqeg0s7VI1vDtr6kDy119/lVjFsrYZPHiwbjDhSy+9ZLI6qyHDv5+Pj4/uoG1qn1Or1fjf//5nxdZWTOPGjTFixAgAwIcffqjLPa3oPvzII48AEHdlDb+fMkmSdNVlK0L+Dp47dw6ff/55icuqVCrdQFCg5O8ZIAbyAsbfs/KsUxI/Pz/drF+ff/65yX3t6NGjumq5jz32mEXbrSkquh/K+3ZqaqrJCsZardaic4ApTz31FADg5MmTmDdvnsXrycFQRkZGuV7XkJyatGnTJpw/f17XA2FuVryKHNusbc2aNXjnnXcAiCBx9OjRFq9b3nOkNT97S5W078nHwBYtWhS7EdmuXTsAMKqUbeiLL74o8XOo6HuVe67Wrl2LEydOFHs+OzsbM2bMAAAMHDjQ6HqrtmDwYKdu3LiBqVOn6r6YXbt2Rbdu3XTPt2zZUncHYvz48UZT+O3evRsxMTFGg7VKMm7cOLi6uuLEiRO4e/cu/Pz8dCcmU9599100btwYarUaAwYMwKxZs4wG/GVmZuLvv//Gk08+iZ49exqte++99+LFF19EYmKi0YDaa9eu4YUXXsCFCxcAiC+sJQYMGABAnOSee+453QlBpVJh/vz5eOSRRyweqFbZ0tLSSvypioO+m5sb5s6dC4VCgUOHDiE6Ohr//POP0YXnpUuXMH/+fHTp0sVomlMvLy/d3b1XX30VCQkJujviJ06cwMCBA3HgwAGjHjJbe/vtt6FQKJCcnIzFixfrHq/IPtyzZ0/dTBwTJ07EkiVLdCe6K1eu4PHHH8f27dtNTk1aFg8++CAeeughAMCUKVPwzDPP4Ny5c7rnCwoKsHfvXrz11luIjIw0GtT3/PPPY/jw4Vi3bp3R49nZ2fjuu+90szcZfs8+++wz3H///Vi2bJlRj2d+fj5Wr16tC2As/W4CwP/93//B2dkZFy5cQP/+/XUpJVqtFhs3bsTAgQOhVqvRuHFj3U0Ma5Jng2nQoIHVt20NFT2WymPWnnnmGSxcuFC3H6ampmLEiBHYvXt3ufbDPn366C6wnn/+ebz99tu6fUKSJFy7dg2LFi3SBRkyedpLpVKJ1atXl/l1DfXt2xchISFQq9UYNWoUcnNz4e/vjwceeMDk8hU5tlnDjRs38Msvv2DQoEEYPnw4CgsLERYWhg0bNpidhteU8p4j5c9+7dq1Fp/7K8rX11e378k9zpcvX8Zjjz2GLVu2ABDHgKLkGwX//PMP3n//fd3Yq7S0NLzzzjv46KOPdFPWmiK/159//rnUINGUZ555Bg0bNkRhYSHuv/9+/PXXX7pz2fHjx9G/f39cunQJLi4u5Q7A7V6ZK0NQlTBXJK5u3bqSr6+v7jkAUps2bUxWR16/fr1RyXoPDw9dNVEPDw+jQj8lFVORJEkaPXq0btmiFaVNuXjxotSuXTujdvr5+emqWss/TZo0MVpPLmQDiMq/fn5+uiJX8o+p6sQlFYwZOXJksXbIhfY6deokff3112YLF5VUCEdmaeEjUwz/zqX9tGvXzqJ2WVIwr6QiTJIkST/99JNuXwEgOTk5SYGBgZKrq6tRm4oWmTtw4IDR38vV1VVX5dfJyUlaunSp2WJFJRXtKauyFN978MEHJQBSeHi4rlqrJJV/H5YkSbp+/brUvHlz3TLOzs6Sn5+fBIiKvAsWLJAiIiIkANKKFSvK/TmoVKpi+7enp6fk7++vK04m/xgWkjMsfgiIarJy++SfHj166CqyF/1MAVF4LSAgQFfNF4DUokUL6fr160ZtLO37sXLlSsnFxUW3DR8fH8nNzU33//r160unTp0qtp5hkbiSlHSMq4oK06aU5ZhRkf0wLS3NaF3D/VChUEjffvtticXDSvrsVCqVNGzYMKM2+Pj4GB0jDI9Zsri4ON3z3t7eUmRkpBQZGSl9+eWXumVKOz7JXn31VaPXnzRpUimfZvmPbaUx/N76+PgYFXY13L8BSI6OjtKYMWOMKrsXZe7vUt5z5NatW3XfVUdHR6levXq6z97Ueyjt2GPJOejtt9/WFSJ0dnaW/P39jdo5depUk9tWq9VGhTQVCoXk7+8vKRQKSaFQSJ9//nmJ57lly5YZ7fNhYWFSZGSkFB0dbfF7PX78uK6oICCqwRt+51xdXaU1a9aYbH9J35uin1F5CttWB+x5sAOGReJu3ryJnJwchISEoH///li4cCEOHDhgclrIBx54ANu2bcOgQYPg5+cHtVqNoKAgjBs3DocOHUJcXJzFbXj00Ud1v5tLWTLUsGFDHDhwAEuXLsUDDzyAevXq6VInGjZsiIceegjff/+9bopI2cqVKzF9+nTExcWhYcOGKCgoQGFhISIjIzFixAjEx8frZkey1M8//4zZs2ejbdu2cHV1hUajQZs2bfDJJ59g586dVitGVpM8/vjjuHDhAqZOnYrOnTvDy8sLGRkZummCn3/+efz777946623jNbr1KkT9u3bh+HDhyMoKAharRbe3t4YPnw4du3apZslpTqRU6muXLmC+fPn6x4v7z4MiMGZ+/fvx9SpU9GsWTM4ODjAyckJAwcOREJCAiZOnKgb8FfSHbTSeHh4YMWKFdiyZQvGjBmDRo0aQavVIjs7G8HBwYiNjcWMGTNw/vx5o9lc3n33XXz11Vd46KGH0Lx5czg5OenW6du3L77//nskJiYa9RI9/fTTWLBgAR577DG0bt0aHh4eUCqV8Pf3R8+ePTF79mwcOnSo2CD50owYMQInT57EpEmT0LhxY+Tn58PJyQnt27fH9OnTceLECbRo0aLcn5G9q8h+GBgYiF27dmH69Olo3ry5bj8cMGAANm/ejGeffbbc7fLw8MC6deuwYcMGPPTQQwgNDUVeXh68vLzQtm1bvPjii0az9MnWrl2LV155Bc2aNUNhYSFSUlKQkpJSrp7VoilK5lKWDJX32FYWSqVSd77OzMyEj48PWrRogREjRmDWrFlITU3F0qVLi01iYonyniN79eqFP//8E/fddx98fX1x8+ZN3WdfWVxcXBAfH4+PP/4YUVFRyM/Ph6+vL+Li4vDnn3/iww8/NLmeo6Mj/vzzT91+6+LiAoVCgX79+mHz5s14/fXXS3zd0aNHY9myZejRowc8PDxw/fp1pKSklDiAu6jWrVvj5MmTmDZtGtq3bw8nJyfk5+ejcePGmDx5Mk6ePKlLUa2NFJLE+SmpdC+88AK++eYbdOvWTTeLExGVz/nz53VTeKampqJ+/fo2bhEREZFl2PNApVIqlboc6GeeecbGrSGyf5988gkAMTaJgQMREdkTBg9Uovz8fLz00ktQKpWoX79+iQOliUg4c+YMJkyYgG3btiErK8vo8XHjxuGHH34AIAY6ExER2ROmLZFJs2fPxuzZs3Hr1i1dBc81a9bU6hw/IksdOXLEqNaFr68vCgsLjWb+ePHFFzFnzhxbNI+IiKjc2PNAJmVkZCAlJQWSJKF9+/ZYtWoVAwciCzVu3BhffPEFBgwYgIYNG0KtVkOj0aB+/foYPnw4/v33XwYORERkl9jzQEREREREFmHPAxERERERWYTBAxERERERWYTBAxERERERWYTBAxERERERWYTBAxERERERWcTJ1g2orkJCQqBSqRAREWHrphARERERWU1qaio8PT1x48aNMq/LngczVCoVCgsLbd0MIiIiIiKrKiwshEqlKte67HkwQ+5xOHnypI1bQkRERERkPa1atSr3uux5ICIiIiIiizB4ICIiIiIiizB4ICIiIiIiizB4ICIiIiIiizB4ICIiIiIiizB4ICIiIiIii3Cq1kogSRIkSbJ1M4iIqAQKhQIKhcLWzSAisisMHqxEq9UiMzMTd+/eRX5+vq2bQ0REFnB1dYW/vz98fX3h4MDOeCKi0jB4sAJJknDjxg1kZmbauilERFQG+fn5uHHjBvLy8hASEsKeCCKiUjB4sIKsrCxd4BAcHAwfHx84OjrauFVERFQSjUYDpVKJW7duISMjA56envDx8bF1s4iIqjUGD1agVCoBAAEBAQgMDLRxa4iIyBIODg4IDAyEWq1Geno6srKyGDwQEZWCCZ5WkJOTAwDw9va2cUuIiKis5GO3SqWycUuIiKo/Bg8VJEkSNBoNADHwjoiI7It87NZoNJwpj4ioFAweKsjwRMOBdkRE9sfw2M3ggYioZAweiIiIiIjIIgweiIiIiIjIIgweiIiIiIjIIgweyKoUCkWZfho0aGD1NsTExEChUCA5Oblabcvain6Wzs7OCAoKQps2bTB27FisW7cOarXa1s0kIiKiGoR1HsiqnnzyyWKP7dixA0lJSWjXrh3at29v9FxQUFAVtazmkj9zrVaLzMxMnDt3DkuXLsWPP/6IJk2a4Oeff0aXLl0q/DpLlizBuHHj8P7772PatGkV3h4REVGtlZsLqFRAQADgYF/38hk8kFUtWbKk2GNjx45FUlIShg4dWiUXnUuXLkVOTg7CwsKq1bYqi6nPPCkpCe+88w5Wr16NPn36YOfOncUCNyIiIqpChYVAejpw5w6QlQVoNEDjxkDdurZuWZkweLAzSiWwYwdw7hyQnw+4ugJRUUB0NMDCqEJERES13FZVaty4MVatWgVvb28sXrwY48ePx6FDh2zdLCIiotpFowEyMkTAkJkpggalUvQ6BAWJ5+2MffWT1GKFhcBPPwFvvgksXAj88gvw22/i3wULxOM//SSWsxeJiYlQKBQYO3Ysbty4gQkTJiA8PBxOTk6YPXs2AOD69euYMWMGevfujbCwMLi4uCAkJATDhg3D/v37TW7X3DgFeYyFRqPBjBkz0KxZM7i6uqJ+/fp46623kJ+fX6nbAoDDhw/j/vvvh6+vL3x9fdG/f3/s378fS5YsgUKhsHrPzMyZM+Hp6YnDhw9jx44dRs/9+eefGD9+PFq0aAEfHx94enqiXbt2+Pjjj4u1PyYmBuPGjQMATJ8+3WishdzzIUkSVqxYgZEjR6JZs2bw9PSEt7c3unTpgrlz50Kr1Vr1vREREVVLkiQChUuXgGPHgJMngbNngfPngbt3AW9v8WOn2PNgBwoLga++AnbvFvudm5vo4XJxAQoKgGvXgIsXgbQ04OZN4MUXAWdnW7facrdv38Y999wDtVqNHj16IC8vDx4eHgCA33//HW+99RaaNGmCNm3awMfHBxcuXMCvv/6KDRs2YMOGDejXr1+ZXu/xxx/Hhg0b0KVLF0RFRWH79u2YMWMGrl69ip9++qnStrVr1y7cd999yM3NRYcOHRAVFYVTp06hR48eugtza/P19cX999+PtWvXYsuWLejRo4fuuaeeegoqlQqtWrVCmzZtoFQqsW/fPvzvf/9DfHw8Nm3aBEdHRwDAgAEDoFarsXPnzmJjV5o0aQIAyM/Px6hRo+Dv74+WLVuiY8eOSEtLw+7du/Hcc89h3759JlOsiIiIagSVSqQlpacD2dkigFAqxZgGX1+gYUNx8QYAeXm2bWsFMHiwA6tWicDh7FmgRQvA39/4+Xr1RCB7+rT4f926wOjRVd/O8tq4cSMeeughLF++HG5ubkbPRUdH4+jRo2jbtq3R4//88w+GDBmCZ599FufPn7e4undKSgo8PDxw4sQJ3UxPly5dQqdOnfDzzz9j+vTpaNy4sdW3pdVqMW7cOOTm5mLGjBl44403dNv58MMP8d5771n0muXRvn17rF27FqflHeQ/3333Hfr27QtPT0/dY1lZWRg1ahQ2bNiAn3/+GU888QQAYMqUKQgJCcHOnTvNjl1xcnLCunXr8MADD8BFPjhCBIcDBw7Ejz/+iPHjx6NXr16V80aJiIiqWn5+8YAhMxNQq0U+eXg44O5u61ZaFdOWqjl5jMP586YDB5m/v3j+/HmxvFJZte2sCFdXV3z99dfFAgcAaNOmTbHAAQD69++PRx99FElJSThx4kSZXu/rr782miK2YcOGGP1ftLV9+/ZK2VZCQgLOnTuH5s2b4/XXXzfaxjvvvIOGDRuW6XXLQp7R6u7du0aPDx061ChwAABvb298+eWXAESvT1k4OTlh2LBhRoEDANSpUweffPJJubZJRERU7ajVwO3b4q7u0aPAqVPi9wsXRI9CcDDQtCkQElLjAgeAPQ/V3o4dIhXJzc184CDz9xfL3boF7NwJ3H9/1bSxojp27FjibEb5+fn4+++/sW/fPty+fRsFBQUAgOPHjwMAzp8/jzZt2lj0Ws7OzoiJiSn2eLNmzQCIMRaWKsu2du3aBQB45JFHivWSODo6YtiwYZg5c6bFr10WkiQBgMnemfPnz2Pjxo24cOECVCoVtFqtbvnz58+X6/WOHDmCTZs2ISUlBTk5OZAkCVlZWRXaJhERkU3J4xju3BEDoLOz9f96eIi0pPBw4L9035qMwUM1d+6c2E8tncWrbl0x9uHsWfsJHkqa0ej48eMYMmRIiUXa5AtTS9SrV0+Xx2/Iy8sLAMwOdK7otq5duwYAqF+/vsltVeasTmlpaQCAgIAA3WOSJOH111/Hl19+qQsWiirL5woABQUFGDt2LFasWGF2mbJuk4iIyKays8WF2N27xuMYnJxEwBASIn6vRZi2VM3l54sB00UyQcxydha9aWW4BrY5U+lKgLjAHT58OJKTkzF58mQcOXIESqVSd3f87bff1i1nKUvHRlTWtsytU5b3UFZHjhwBALRs2VL32KpVqzBr1iyEhYVh7dq1uHr1KgoKCiBJki7oKWubZs2ahRUrVqB169b466+/cPPmTd02z549W65tEhERVbm8PODqVeD4cfFz5oy4K3vlCqBQABERQKNGQGBgrQscAPY8VHuuriIg+C9Tp1SFhWI/dnWt3HZVhTNnzuDMmTPo3Lkz5s2bV+z5ixcv2qBV5VOvXj0AQGpqqsnnL1++XCmvm5mZib///hsA0KdPH93jv/76KwBg3rx5eOCBB4zWKe/nKm9TDiCssU0iIqIqIRdwS08XPQtZWaKXIT9fDHyuVw8oMk6wtmLPQzXXrJkIbG/etGz5mzdFzZGoqMptV1WQB/iGh4ebfG7z5s1V3aRy6969OwBg3bp1xe6+a7Va3YW3tb322mtQqVS455570K1bN93j8mdrKo1q9erVJrclD4RWq9Umny/PNomIiGxGoxEpSefPA0eOGA98VqmAgAAx8JmBgxEGD9Vcjx5iHENenki3K8ndu/pB/tHRVdO+ytSkSRM4ODggISHBaKBtXl4eJk+ejPT0dBu2rmxiY2PRpEkTnD59WjebkezTTz+1+p35ixcvYsSIEVi8eDE8PT2xePFio+flQd0LFiwwCma2b9+Ozz//3OQ2Q0NDAUCXglSUvM3vvvvO6PG1a9di6dKl5XsjRERE1mSqgNuZMyKASE8HvLyAJk3E4GcfH1GjgYwwbama8/ERAURamqjjYG66VrnOQ1SUWN7Hp+rbam3BwcF46qmnsHDhQrRr1w6xsbFwd3fH9u3bodFoMHbsWLspOubo6IgffvgBffv2xWuvvYaff/5ZVyTu9OnTmDhxIhYuXFhsmlNLjB07FoDowVAqlTh37hzOnDkDSZLQtGlTLF++vNhsVC+++CKWLFmCuXPnIjExEW3btsXVq1exY8cOvPbaa/jiiy+KvU7Xrl0RHByMtWvXIiYmBo0aNYKDgwPGjx+P7t27480338Tff/+NKVOmYM2aNWjWrBnOnz+PAwcO4PXXXze5TSIioiqhUpke+GyqgBuViOGUHRgxAujWTQQGZ8+KnrXr10VAcf26+P/Zs+L5bt3E8jXFvHnzMHPmTDRs2BDx8fHYvn077rvvPhw4cACRkZG2bl6Z9OjRAzt27ED//v1x/vx5bNiwAUFBQdi6davuvQQGBpZ5uz/++CN+/PFHrFixAtu3b4ejoyOeeOIJrFu3DqdOnULnzp2LrdOsWTPs378fgwcPRlpaGv744w9kZ2dj/vz5Znse3Nzc8Oeff6Jv3744cuQIlixZgsWLF+PcuXMAgF69emHHjh2IjY3FxYsXsWHDBri4uGDdunV47rnnyvy+iIiIKiQ/H7h2DThxQgx8Pn1aTGOZmip6IMLDgcaNRb43AweLKSROf2JSq1atAAAnT54scTmtVqtL44iKioJDJXVvFRaKStM7dog6DmlpYlYlJyexzwcHix6HESPEAGuyL/fffz/+/vtv7NmzB/fee6+tm0NUq1TVcZyIqoBabXrgc24u4O0tehk8PcWsSbZ044a4iGvTBvgvLbgqWXqdawrTluyEszMwejQwZIgoAHf2rAioXV1Fj0N0dM1IVarJ0tPTkZWVZdRjIkkSvvnmG/z9999o0qQJunTpYsMWEhER2SGtVhRsS08XgYIcMMgF3Pz8gPr1OX7BShg82BkfH1H8zV4KwJHeuXPn0L17d7Rt2xaNGjWCRqPBiRMncPHiRbi7u2PhwoVWrUNBRERUY0mSCBIMKz7L4xhcXcUFUy0s4FYV+IkSVZFGjRph8uTJ2LJlC+Lj45Gbm4vg4GCMGjUKU6ZMKTaomYiIiIpQqfRpSdnZIlhQKsVzvr5AgwY1o9hVNcbggaiKBAcHY+7cubZuBhERkX3JzxfBwp07InjIzBQ/arXoYQgLA9zdbd3KWoPBAxERERFVL6UNfA4Orh4Dn2shBg9EREREZHulDXz29RXTqzo62rqltRqDByIiIiKyDUkSPQvp6cUHPru4iICBA5+rFf4liIiIiKhqceCz3WLwQERERESVLy/POGCQexg48NmuMHggIiIiospRWAjcvStmSsrK4sDnGoDBAxERERFZj0ajH/gsj2NQKsW/np6s+GznGDwQERERUcVIkuhRKDpTUlYWKz7XMPwLEhEREVH5ZGWJgOHuXX0BN6VS9Cr4+gING4pZk6jGYPBARERERJbLzS0+U1JmpqjT4OMjUpLc3GzdSqokTDajSqFQKIr9uLi4oH79+nj88cdx/PhxWzex0igUCjRo0MDWzShm7Nixxf4m7u7uaN68OV555RXcuHHDpu1LTEyEQqHA2LFjjR6fNm0aFAoFlixZUmmvnZycDIVCgZiYmEp7DSIiu1ZQANy4AZw6BRw7Jv49dw64dEkMig4JAZo0AerWZeBQw7HngSrVk08+qfs9MzMTBw8exPLly7F27Vr8/fff6NOnjw1bVztFR0ejSZMmAIBbt25hz549mD17NlauXIndu3dXy8CnopYsWYJx48bh/fffx7Rp02zdHCIi+6BWi3Sk9HR9HQalEsjJAby8gIAA8S8HPtcqDB6oUhW9W1xYWIinnnoKy5Ytw0svvYRjx47ZpmGV6PTp03B2drZ1M8yaMGGC0d3927dvY+DAgThw4ABef/11rF271naNM+H555/HyJEjUa9evUp7jbCwMJw+fRoeHh6V9hpERHZBq9XPlJSZKdKS5BmT3N3FOIawMMDR0dYtJRth8EBVytnZGdOmTcOyZctw/PhxZGRkwM/Pz9bNsqrmzZvbugllUqdOHcycORO9e/fGn3/+icLCwmoV/AQFBSEoKKhSX8PZ2dnu/m5ERFYjSaJHoejUqkol4OzMmZLICPuZqMrVrVtX97tarTZ67siRI3jzzTfRqVMn1KlTB66urmjUqBGeffZZXLt2zWjZ/fv3Q6FQIDo62uxrTZ8+HQqFAh999JHR4wUFBZgzZw7uueceeHt7w9PTE126dMHixYshSVKx7Vy+fBnPPfccoqKi4OHhgYCAALRq1QqTJk3C2bNnjZY1NeZBkiSsWLECI0eORLNmzeDp6Qlvb2906dIFc+fOhVarLfaahrn+x48fx5AhQ+Dv7w9PT0/07t0bu3btMvu+y6pDhw4AgLy8PKSlpQHQj5FITEzEP//8gz59+sDPzw8KhQIZGRm6ddevX4/+/fsjMDAQbm5uaNasGd59911kZ2ebfK3k5GQ89thjCAwMhJeXF7p3744///zTbNtKGvNQWFiIuXPnIjo6Gn5+fvDw8ECzZs0wceJEnDhxAgAQExODcePGAdDvD/KPvM3SxjwsW7YMPXr0gI+PDzw8PNC2bVt88sknyMvLK7as4ee2bds2xMbGwtvbGz4+Phg0aBBOnTpl9r0SEVWp7GwgNVWMYThxAjhzRoxjuHZN9CxERorZkgIDGTiQDvcEqnIHDx4EYPqO8qeffoq1a9eidevWiI6OhkKhwJEjRzBv3jz89ttvOHDgAEJDQwEA99xzDzp16oRdu3bh5MmTaNWqldG2tFotfvjhBzg6OuouHgFApVLh/vvvx/bt2xEUFIQePXrAwcEBu3fvxoQJE7B//3589913uuWvXLmCjh07Ii0tDW3btsXgwYORl5eHlJQULFy4EN26dUNUVFSJ7zk/Px+jRo2Cv78/WrZsqdve7t278dxzz2Hfvn1mBwQfOHAAzz33HMLDwxEXF4cLFy5g27ZtiIuLw/79+9G6dWuLP3tzsrKydL+7uroaPbd8+XIsWrQInTt3xv3334+kpCQo/qsE+tprr2HWrFlwc3NDly5dEBQUhIMHD+Kjjz7CX3/9ha1bt8LT01O3raSkJHTv3h23bt1Cs2bN0LFjR1y6dAmDBw/G5MmTy9Rmw7+jl5cXevbsCW9vb1y6dAlLlixBWFgYWrdujQEDBkCtVmPnzp1o164d2rdvr9uGPPajJJMmTcKCBQvg5uaG2NhYeHh4IDExEe+88w7Wr1+P+Ph4uLu7F1tv/fr1mDNnDlq3bo3+/fvj+PHj2LhxI/bu3YsTJ04gJCSkTO+XiMgqDGdKMpxaVa0WPQxhYSI9icgMBg+VTS6aYm98fa1eKj4zMxP79u3D888/DwB45513ii3z9NNP48svvzTKb9dqtfjoo4/w/vvvY+rUqfj+++91z02aNAlPP/00Fi1ahC+//NJoW5s2bUJKSgoGDx6MsLAw3eNvvPEGtm/fjjFjxmDu3Lnw8vICIHL/Bw8ejPnz52Pw4MEYNGgQAGDRokVIS0vDzJkz8eqrrxq9RkpKSrHeE1OcnJywbt06PPDAA3AxmO9aHm/w448/Yvz48ejVq1exdb/99lt89tlnePPNN3WPvfLKK5g9ezZmzJiBpUuXlvr6pVm/fj0AkfsfEBBg9NzChQuxcuVKjBgxwujx1atXY9asWejQoQN++eUXXW9LYWEhnn/+eSxYsADTpk3D559/rlvn2Wefxa1bt/Dss8/i66+/hsN/g+wWLVqEiRMnlqnNL730ErZv344+ffpgzZo1CAwM1D139epV3exRU6ZMQUhICHbu3ImhQ4eWacD0unXrsGDBAoSFhSExMVEXbCiVSgwaNAg7duzA+++/jxkzZhRbd/bs2fjpp5/w2GOPAQA0Gg1GjBiBdevWYe7cufjggw/K9H6JiMqtoEAfMGRl6Qu45eWJgCE4WFR+tvJ5n2ompi1VtsxMwN/f/n6sFPAYpoj4+fmhX79+yMjIwPLly/HKK68UWz42NrbYwFgHBwe89957CAsLw++//2703KhRo+Dj44Nly5YhPz/f6LlFixYBgNFF6a1bt7Bo0SI0bNgQCxcu1AUOgMj9nz9/PgDo/pXXkdtWVGRkJBo3blzq5+Dk5IRhw4YZBQ7ya37yyScAUOy9yXr06GEUOADA1KlTAQDbtm0r9bVLcvv2bfzwww+67T/zzDPFlhk0aFCxwAEAPv74YwDAihUrjNK0nJ2dMWfOHISEhGDRokW6lKykpCRs2rQJ/v7+mDFjhi5wAMQg7u7du1vc7uvXr2PJkiVwd3fH0qVLjQIHQARBnTp1snh75nz11VcAgA8++MCol8LHxwdz586FQqHAd999h4KCgmLrjho1Shc4AICjo6MuYK7o342IqFRqNXD7NnD2LHD0qJha9exZ4MIF0eMQEAA0awaEhooZkxg4kIXY80CVynCq1vz8fKSkpGDv3r148803ERoait69exdb586dO/jjjz9w4sQJZGRkQKPRABB3tNPT05Genq67O+7p6YnHH38c8+bNw6+//oqRI0cCEBf8f/zxB0JDQzFw4EDdtrdu3YrCwkIMGDCgWHoOALRr1w7e3t7Yv3+/7jH5IvS5557DRx99hJ49e8KpnLmfR44c0fWI5OTkQJIkXcrQ+fPnTa7Tr1+/Yo8FBgYiMDAQ169fL3Mbxo0bZ5TGJXvyyScxZcqUYo8PGTKk2GO3bt3C0aNH0aJFC5MpW25ubujcuTM2bNiA8+fPIyoqCjt37gQADBw40CiVSTZy5EiLx3Fs2bIFGo0GAwcORHh4uEXrlFVhYSH27NkDhUKBUaNGFXu+TZs2aNu2LY4ePYqjR4/innvuMXre1N+tWbNmAFCuvxsRUamKzpSUlSVSkrKyOFMSWQ2DB6pUpvL4Dx8+jN69e6N///44ffo0GjZsqHtuxYoVePrpp80OtgVEfr5has3kyZMxb948LFy4UBc8LFmyBIWFhRg/fjwcDQ6SycnJAIB58+Zh3rx5Zl8jNzdX9/vYsWOxadMmrF69WpfzLuf/jx8/HsHBwaV+DgUFBRg7dixWrFhR4vsyxdzFsZeXF+7cuVPqaxdlWOfBzc0NkZGRuP/++43GAhiKiIgo9lhKSgoAMS2topS7VWlpaYiKitINeDe1vZIeN+Xy5csAYFGvT3nduXMHBQUFCAkJgZuZgkcNGjTA0aNHiw3mB0z/3eSerqK9ZERE5VZ0piTDgEGeKSk4WPxOZAUMHiqbr68osGJvfH0rbdMdOnTApEmT8MUXX+Cbb77BzJkzAYgL0rFjx0KSJMyePRuDBg1CWFiYbjBq9+7dsXv37mKzIbVt2xZdu3bFli1bkJSUhMaNG2Px4sVQKBR46qmnjJaVezE6dOiAtm3bWtReR0dHrFq1ClOmTMHvv/+OLVu2YM+ePdi2bRs++eQT/PPPP+jatWuJ25g1axZWrFiB1q1b4/PPP0fHjh3h7+8PZ2dnnDt3DlFRUSZneQJQ6sV5WRWt81AaUxfO8udYr149k3fYDckpRfL7s+b7sfZnU97XMLVMVbSNiGqx7GwRMNy9K36XexoAcQ6PjARM9LATVRSDh8qmUAA1rI6BNci9DYbTnG7cuBEFBQV47bXX8NJLLxVb5+LFi2a3N3nyZOzZsweLFy9G//79ce7cOfTr16/YlKny3eCYmBjMmjWrTG3u0KEDOnTogGnTpkGpVGL69OmYNWsWXnrpJezdu7fEdX/99VcA0AUQlr6v6kr+HENCQszOElWUPEuW3GtRVGpqqsWvX79+fQDAhQsXLF6nrAIDA+Hi4oIbN24gNzfX5IxK8nupzAJ2REQ6OTnGAYNSKYIGrRbw9uZMSVQlOGCabEK+YDbMfb/7Xw+NfGFoaNu2bbh586bZ7Q0fPhz+/v5YsmSJLh3J1Ow9ffr0gaOjIzZs2KC7e14ePj4++Pjjj6FQKHD8+PFSly/pva1evbrc7bCV8PBwREVF4dixY7h06ZJF68j1ODZu3AiVSlXs+ZUrV1r8+jExMXB0dMTGjRtx9erVUpeXB6pbMjOWzNnZGV27dtXV6CjqxIkTOHr0KLy9vdGuXTuLt0tEVCb5+cD168DJk8Dx48Dp06IWw6VLQGGhKN7WpIn4l4EDVQEGD1TlDh8+jAULFgCA0WBmeTDpTz/9ZHRxefXq1VJrALi7u+OJJ57A9evXsWrVKtSpUwcPPvhgseXCwsIwduxYnD9/HmPGjNEVRDO0a9cubNy4Uff/ZcuW6QqOGfr7778hSZJFufryezOsHwEAa9eutcpUq7YwdepUaDQaPPzwwyY/n6SkJKNpdZs0aYK4uDjcvXsXU6ZMMSqM98MPP5Sp6F1oaCieeOIJ5ObmYuzYsUhPTzd6/tq1azh06JDR8gCKFfQrzQsvvAAAeP/99416iLKysvD8889DkiRMmjSp2CxaREQVUlgI3LolirYVnSkpJ0cUbWvaFKhXj1OsUpVj2hJVKsPc+oKCAqSkpGDPnj3QarUYPHgwxowZo3t+yJAhaNWqFQ4cOIAmTZogOjoaeXl52LJlC9q3b4/u3buXeIE5adIkzJkzR/e6zmYGh3311Ve4ePEiVqxYgQ0bNqB9+/YIDQ3FjRs3cOHCBVy9ehUvvfSSLrBZt24dnnjiCTRu3Bht2rSBu7s7kpOTsWfPHjg6OuqmLC3Jm2++ib///htTpkzBmjVr0KxZM5w/fx4HDhzA66+/ji+++MKSj7NaGT16NI4fP44ZM2agffv26NChAxo2bAilUomUlBScOXMG7dq1w/jx43XrzJs3D9HR0fjmm2+wefNmXZG4vXv3YvLkycWCq5LMmTMHZ86cwb///osGDRqgZ8+e8PLyQnJyMg4dOoT//e9/6NixIwCga9euCA4Oxtq1axETE4NGjRrBwcEB48ePL3GK2EceeQRPP/00FixYgNatWxsVibt9+za6du2K6dOnl/9DJCKSaTQiHUmeKUlOS8rOBjw8xDiG8HDOlEQ2x+CBKtWPP/6o+93BwQF+fn7o1asXxowZg7FjxxrN9e/i4oLt27fjf//7H/766y9s2LABYWFheOGFF/Dee+8Z9VKY0qJFC4SGhuLatWuYMGGC2eU8PDywadMm/Pjjj1i2bBmOHTuGvXv3Ijg4GI0bN8ZLL71kND//q6++ivDwcOzcuRPbt2+HSqVCWFgYHnvsMbz++uvo0KFDqZ9Dr169sGPHDvzvf//D4cOHce7cObRp0wbr1q1Dx44d7TJ4AIDPPvsM/fv3xzfffIPdu3fj6NGj8Pf3R3h4ON544w3d7Feypk2bYs+ePXj77bexefNm/P7772jTpg1+++03+Pj4lCl48Pb2xpYtWzBv3jz8/PPP2Lp1KyRJQnh4OMaPH49HH31Ut6ybmxv+/PNPvPPOO9i3bx+2bdsGSZLQo0ePUutLzJ8/Hz169MB3332HrVu3Qq1Wo3Hjxnj55ZfxyiuvmBwLQURkEa1WBAryTElywJCVJQY7+/iIdKRyTg9OVBkUkrkpXmq5Vq1aAQBOnjxZ4nJarVaXChEVFWV0MUxVa9euXYiOjkbv3r2RmJho6+YQkZ3gcZyqVNGpVeWAQakUQYKPj/hhOmTNduOG+Hu3aSMK9VUxS69zTWEoSzWGnD70/PPP27glRERERRSdWlUOGAARLEREAGZqyhBVJwweyK7t2rULixcvxokTJ7Bv3z506tQJw4YNs3WziIiIzE+tqtGIgCE0VIxnILIjDB7Irp07dw7ff/89vL29MXjwYHzzzTdMOSAiItvJy9MPfDbsYcjPF7UY6tblDElk1xg8kF0bO3ZsmaolExERWV1BgT5gyMoSP5mZQG4u4OUlplb19AR4c4tqAAYPRERERGWlVusDBnlKVU6tSrUAgwciIiIiS2g0+qlV5VoMmZmip8HNTQQMnFqVajju3URERETmaLX6qVXlQEGuxeDsLAY+BweL36lGys4GjhwBUlNFhpqLCxAZCbRrJ7LSahsGD0RERESGJEkEB6ZqMSgUoochMlIUcqMaq1ANbN4kAoc76UBmhshWc3ICfP2ArVuB9u2Bvv0A51p0RV2L3ioRERFRCYrWYpAHPkuS6GGoX5+1GGqJQjWwciVw/JjocXB1BQICRAdTYSGQdhu4ekXElnfSgZEja08AUUveJhEREZEJpmoxKJXiFjNrMdRamzeJwCElBWjQEPDxNn4+KAhQZgHJl8T/AwOAgQOrvp22wOCBiIiIapfSajEEB7MWQy1mOMbBVOAg8/EWz6cki+V79aodYyAYPBAREVHNV1othoAA8S9rMdR68hgHV1fzgYPMx1ssl54OHD0KREdXSRNtisFDVdBqbd2CsuGBk4iIaoLCQhEw3L2rnyFJqQRUKtGzwFoMZEJqqhgcHRBg2fIBAWLsQ0oKgweyBq0WOHzY1q0omw4drBZAqFQqLFiwAH/88QdOnTqFu3fvwtPTE82bN0ffvn0xYcIEREREWOW1yurgwYN4++23sW/fPmRmZgIALl26hOTkZPTp0wdPPvkklixZYvH2FAoFIiMjkZycXDkNrqbGjh2LH3/8ET/88AOrfZegQYMGSElJgSRJZVpPrVajVatWCAgIwO7du63aJq1Wi507d2L9+vXYunUrLl26hMzMTISHh6Nv375466230LBhQ6u+prVMmzYN06dPN9rvJElCx44dodFocOTIETjwRkjtpNHoAwbDWgzZ2YC7uxjHUK8eazHUYqVNvVpQIIa8WDr7rpOT2O0KCiq12dUGvzlV5fRpW7fAMi1aWG1Te/bswbBhw3D9+nV4eHiga9euqFu3LjIzM7F//37s2bMHM2bMwIYNG3DfffdZ7XUtkZWVhSFDhuD69euIiYlB/fr1oVAo4FUbkhWrMTkQ2bJlC2JiYmzdnGph/vz5OHfuHP766y+rb/vixYvo1asXACAsLAzdu3eHg4MD9u3bh/nz52P58uXYuHEjevToYfXXTk5ORsOGDdG7d28kJiZaZZsKhQLvvfcehg0bhiVLlmD8+PFW2S7ZAa1W3Pq9e9d4atWsLJFT4u3N4m1k8dSrjk7iscJCy7arVovOKxeXymt7dcJvUVVq2rT6pgRptcD581bb3LFjxxAbG4vc3Fy89dZbePfdd+Hp6Wnwclr89ttvePPNN3HlyhWrva6l9u/fj2vXrmHMmDFYunSp0XNdunTB6dOn4evrW+XtIjKUn5+PDz74AO3bt8eAAQOsvn2FQoH+/fvjnXfe0QUR8utOnjwZS5YsweOPP44LFy7A2U4KYA0dOhTNmzfHe++9hyeeeAJOvFisuSSpeLVneeCzk5PoYWjYsPZc0VGJyjL1qre3yGhLSxOzKpUmPR2oU0f0XtQG1fRK1nI3btzAK6+8gmbNmsHd3R0BAQHo1KkT3nzzTVs3rTgHh+r9YyWSJGH06NHIzc3FtGnT8OmnnxoFDuKjcMCwYcNw8OBBdO7c2WqvbSk5YGnUqFGx5zw8PNC8eXPUq1evqptFZGTt2rW4desWnnjiCYvXWbJkCRQKhUUpd40bN8bff/9tFDgAgKurK+bNmwdfX1+kpqZi165dZW26zSgUCjz++OO4evUq/vjjD1s3h6xNkkRwkJwsRqeeOAGcOQOcOwdcuyZu/0ZEAI0aias+Bg70H8OpVyMbAM2aiV3E11f826yZeDwlRXRgqVRi8i1lVsnbVWaJ5QICRNpTbWDXwcPu3bvRokULzJ49G87OzhgyZAi6du2KO3fuYNasWbZuXq31zz//4Pjx4wgPD8f//ve/Epf19fVF69atjR7LycnBhx9+iNatW8Pd3R2+vr7o1asXVq5caXIbDRo0gOK/6fQWLVqEtm3bwt3dHSEhIZg0aRIyMjJ0yyYnJ0OhUODJJ58EAEyfPh0KhQIKhUKXN52YmGj0f0MqlQpvvfUWIiIi4ObmhubNm2PWrFml5rHv2LEDDz30EIKDg+Hq6ooGDRrgxRdfxO3bt4stO3bsWCgUCiQmJmLbtm2IjY2Ft7c3fHx8MGjQIJw6dcrs6/z111944IEHdK8TERGBoUOH4s8//yy2bHJyMiZNmoQGDRrA1dUVderUwSOPPIJjx46V+F4sFRMTA4VCgeTkZPz222/o2rUrPD09ERAQgMcee6xYj5NCocCPP/4IAOjTp4/u7yJvw9D69evRv39/BAYGws3NDc2aNcO7776L7OzsEtuxfPlydO3aFd7e3vDz88PBgwehUCjQtWtXs+9jxowZUCgURvvyhQsXMG3aNHTr1g0hISFwcXFBeHg4nnjiCZw7d64Cn5qxRYsWQaFQ4LHHHrPaNi0lf64AcO3aNYvXu3z5Mp577jlERUXBw8MDAQEBaNWqFSZNmoSzZ88CEOMV5LEUW7duNfpbF/3ebd26FTExMfDy8kJgYCAeeughnDlzpsQ2jBo1CgCwcOFCi9tN1Vx2trhdfOwYcPy4SAU+exaQjyPh4UCTJuL2L4u4URFlnXr12jVAAhAWJuo4mAsg5DoPEREi3am2ZD7bbX/utWvXMHDgQOTn5+OXX37BQw89ZPT8vn37bNQyki9UH3300TKnDGRlZaFPnz44ePAg6tSpgwceeAAqlQoJCQnYvn079uzZg9mzZ5tc980338ScOXNwzz33YMCAAdi1axcWLFiA06dP6y5QvLy88OSTT+LChQvYuXMn2rVrh/bt2wNAqXnd+fn56NevH3bt2oWgoCAMHjwYWVlZmDJlCpKSksyu99VXX+Hll1+Gg4MDunTpgrCwMJw4cQJff/01NmzYgJ07d5rs5Vi/fj3mzJmD1q1bo3///jh+/Dg2btyIvXv34sSJEwgJCTFa/rXXXsOsWbPg6OiIbt26ITw8HNeuXcOWLVuQkZGBQYMG6ZbdsWMHBg0aBKVSiVatWmHIkCG4evUqfvnlF2zcuBF//vkn+vTpU+LnYam5c+di5syZ6Ny5MwYMGID9+/dj5cqVOHjwII4ePQp3d3cAwJNPPokdO3YgKSkJ/fv3N3p/hmNR5Pfp5uaGLl26ICgoCAcPHsRHH32Ev/76C1u3bi3W0wUAn3zyCRYtWoTo6Gg88MADuHz5Mjp16oTmzZtj7969SEpKQuPGjYutt3z5cgD6C1JAXNR/9tlnaNmyJTp37gw3NzecOnUKy5Ytw++//47t27ejbdu2FfrcsrKysH37djRv3rzY37oqaDQapKSkAIDFr3/lyhV07NgRaWlpaNu2LQYPHoy8vDykpKRg4cKF6NatG6KiotC+fXs8/PDDWLduHerWrWuUkmX4Pfz999/x8MMPQ6PRoHv37oiIiMC+fftw7733YvDgwWbb0ahRI9SvXx8JCQnIy8uDGy8m7ZNKpZ9aVaVi8TYqt/JMverlCfj5i86slGR9mpOTk9gF09NFj0NkJNCmLdC3X1W8k+rBboOHKVOmICMjA19//XWxwAEQeetkG4f/m12qY8eOZV73nXfewcGDB3Hffffh119/1V00njlzBr1798acOXPQr18/DDRRxvGnn37C3r17dcFAWloaunXrhu3bt2PLli2IjY1FUFAQlixZgiVLlmDnzp0YOnQopk2bZlHbZs2ahV27dqFLly7YtGmTbkzEoUOHzF5o79mzB6+88goiIiLwxx9/6C4oJUnCRx99hPfeew8vvvgi1qxZU2zd2bNn46efftLdddZoNBgxYgTWrVuHuXPn4oMPPjB677NmzUJ4eDj+/PNPowtXlUqFvXv36v6vVCrx6KOPIjc3F2vWrMEjjzyie+7ff//FoEGDMGbMGFy8eBEuVujynzt3LjZv3ozY2FgAomepb9++2LVrF1asWKEb1LpkyRKMHTsWSUlJmDJliskB06tXr8asWbPQoUMH/PLLL2jQoAEAoLCwEM8//zwWLFiAadOm4fPPPy+27tKlS5GQkIDevXsbPT5q1Ci89957WL58Od59912j506fPo2jR4+iffv2aNWqle7xoUOHYuLEicWCjR9++AHjx4/Hyy+/jISEhDJ/VoZ27twJjUaDe+65p0LbKa+VK1fi1q1bqFOnDrp3727ROosWLUJaWhpmzpyJV1991ei5lJQUqNVqAOLza9++PdatW4fmzZubTLHKysrChAkToNFosHz5ct33QK1WY8KECbpeKnO6dOmCdevWYe/evcX+5lSN5eaarvZcUMDibVRu5Zl6NTMTaN9BVI4+ckTslhkZYlYlR0fRyRUQIHoc+vYDnO32irrs7DJt6e7du1i9ejV8fX0xYcIEWzeHirhz5w4AoE6dOmVaT6VSYfHixXBwcMDcuXON7jY3b94cU6dOBSDu5Jvy4Ycf6gIHAAgKCsIzzzwDANi2bVuZ2mLKvHnzAABffvml0WDqjh074rnnnjO5zqeffgqtVosFCxYYXdArFApMnTpVdxGclpZWbN1Ro0YZpas4OjrinXfeMfl+Pv74YwAi4Ch6x9vT01N34Q4A33//PW7cuIHXX3/dKHAAgPvuuw/PPvssrl69ig0bNpj/MMrglVdeMXp9Dw8PvPbaaybfR2nk97lixQpd4AAAzs7OmDNnDkJCQrBo0SJoTdRWeeqpp0xeRD7++OMAgJ9//rnYc/Jj8jKyrl27muylGDduHKKjo5GYmKib/re85PSxqKgos8sYpvvIP+PGjdO1xdTzlrh8+TJefvllAMAHH3wAV1dXi9a7desWABj9vWWRkZEmPzNz1qxZg7S0NPTt29foe+Dk5IQvv/yy1JnRmjdvDgA4evSoxa9JNpKXB1y/Dpw8KdKS5JSkpCTxnJyQHhoq8kIYOFAZlXfqVY0aGDgQePFF4KGHgNhYoHdv8e9DD4nHBw6sXYEDYKc9Dzt37kR+fj7uu+8+ODs7Y+3atdixYwcKCwvRvHlzDB8+HHXr1rV1M2utss5jLzt48CByc3PRtWtXNG3atNjzY8aMwYsvvoidO3dCkqRiF0L9+hXvM5Rztq9fv16uNslSU1Nx+fJl3XSWRT322GP45JNPjB7TarWIj4+Ht7c34uLiiq2jUCgQHR2Nw4cP4+DBg+jfv7/R85a+n2vXruH06dMIDAzEww8/XOp72bx5MwBx99eUHj16YPbs2di/fz+GDRtW6vZKY62/y61bt3D06FG0aNHC5AW1m5sbOnfujA0bNuD8+fPFlhkyZIjJ7TZq1Ahdu3bFnj17cOjQIaMes5UrV8LBwQEjR44stl52djbWr1+PI0eOID09HYX/zel3/fp1SJKEpKSkcvW+Gb5fAPD39ze7jDx2x5CckhcdHY0mTZqU+XVVKhUeeughpKWlYejQoZg8ebLF63bq1AkA8Nxzz+Gjjz5Cz549yz3b0Y4dOwAAw4cPL/acv78/+vXrh19++cXs+gH/3WI0Na6IqoGCAn0PA6s9UyVzcanY1KteXqL4W20oAGcJuwweTp48CQCoW7cuevbsWaxw0ttvv40ffvgBjz76qC2aV+sFBQXh7NmzZT5py4MyDe8oG/Lz84Ovry8yMzOhVCqLTaUaHh5ebB357mR+fn6Z2mKubeYK2pl6/M6dO7oBvKVdQJnqebD0/Vy+fBkALL6rKw8+vvfee8vcpvKw1t9Fzr8/ffp0qXfQ09LSigUPJRUjfPzxx7Fnzx78/PPPugv+PXv2ICkpCX369Cn2HhISEjBy5MgS9/GsrFKm6CiF3HPh7W0+QddUuo+ckjdhwoQyF+0rLCzEww8/jIMHD6JHjx668R6WGjt2LDZt2oTVq1cjNjYWHh4e6Ny5M+6//36MHz8ewcHBFm+rPN85Qz4+PgBQ4R4gsiK52nN6ukhFktOSVCoxdoHVnqmSRESIOg5ptzn1qjXYZfBw9+5dACKH2dXVFYsXL8aQIUOQnZ2Nr7/+GrNmzcLo0aMRFRVV6qBFwzxmQ+YGT1Lp2rdvj507d+LQoUMYPXp0mde3JLXC1DKWpmSUh9ybYu41TD2u0WgAiIu/0u7gR5o4QpX1/Vi6vNyuRx99FB4lDDYsLbiwdrtKI7e7Xr16JnszDAUGBhZ7rKRBsyNGjMArr7yClStX4vPPP4eDg4PuwrloylJ2djaGDx+OO3fu4N1338Vjjz2GyMhIuLu7Q6FQYNSoUVixYkW5e+BkcnCsVCortB1LabVajB49Gv/88w/atWuH9evX6wazW8rR0RGrVq3ClClT8Pvvv2PLli3Ys2cPtm3bhk8++QT//PNPiTNbGSrtO1caOWhgvRYbU6uLV3uWAwdWe6Yq0r49sG2bqOOgzCp50HRtnHq1rOzy2ypfRKjVanz77be6AZdBQUGYOXMmUlNTsXbtWsyYMQM//fSTLZtaKw0aNAjffvst1qxZgxkzZlicthAaGgoAuHTpksnnMzMzkZmZCU9PzxLvxlYGuW3y3e+iTD0eFBQEV1dXODs7WzTnfnnVr18fgEhXsUR4eDjOnj2LqVOnVnhGoKok3/0PCQmx+udZp04d9O3bF3/99RcSExPRu3dvrF69Gq6ursVSwbZv3447d+7g4YcfNhq0Lrt48aJV2iTfpU9PT7fK9krz7LPPYvXq1WjWrBk2bdoEPz+/cm+rQ4cO6NChA6ZNmwalUonp06dj1qxZeOmll4wG75ektO9campqievLN5nKOvaKrECjESNLDXsY5MDB1VUEDKz2TFXIy0sEEBkZYmpVc9O1ylOvRkbWrqlXy8oukwnlC0cHBweTOb9yMJGYmFjqtk6ePGnyh70O5TdgwAC0atUKV65cwf/93/+VuKxSqdSloXXq1Anu7u7Yt28fzpuodi0Hgj169KjUXgZTIiMjER4ejqtXrxZLkwNgsgaFk5MTYmJikJ6ebpUB2+aEhoaiRYsWuHPnTok54LL77rsPAPDbb79VWpvKS57dSZ6Vx1B4eDiioqJw7NgxswFmRcg9DMuXL0d8fDxu3ryJQYMGFbuIli9K5aDN0IULF3Do0CGrtKfdf7e8SqtpYA3vvPMO5s+fj4iICGzevLlM6UWl8fHxwccffwyFQoHjx4/rHi/pbw3op2w1NRNZRkYGNm3aVOLrnj59GgCMJlGgSqTVimAhKUlMTXPihBj0fPYscPu2CBoaNQIaNNDPd0lUhfr2E1OqRkaKqVfPnRMVpDMyxL/nzonHa+PUq2Vll8GDnBMfEhJichYQ+Xl5wCFVLYVCgZ9++glubm6YNm0a3n77bahUKqNlJEnCH3/8gc6dO2P//v0AxKxA48ePh1arxXPPPWe0zrlz5/DRRx8BAF544YWqezMGJk2aBEDUGTBMJTly5Ai+/fZbk+u88847uiBXHgBq6Nq1a2bXLYspU6YAAF5++WVdMCaT62QYvo86derg448/xg8//FAsvUalUmHp0qXFirhVBflus1xMrKipU6dCo9Hg4YcfxokTJ4o9n5SUhO+//75crz106FB4enpi3bp1+OGHHwAUT1kC9IO9f/nlF6MxDxkZGXjqqad0A6crqnv37nB0dCxzzZqxY8dCkiSLxzvMmjULn3zyCUJCQvDvv/+WOpagJMuWLTP5d/n7778hSZLRtoOCguDs7IykpCRdb7KhRx99FAEBAboxFDKNRoPXXnvNZEFAQ/v27YOLiwun7a5MWq1IR7p4sXi151u3xNQ2DRuKn6Agy6e6IaoEzk7AyJFATAzQtq0Y05CVJQKHrCzx/7ZtxfMjR9a+GZTKwi4/mg4dOgAQdwBNzbojTxVa2lR+Vc7E9JHVhpXb1r59e/z77794+OGH8emnn+Krr75Ct27dULduXWRmZuLAgQO4efMm3NzcjO7gfvLJJ9izZw82b96MRo0aoXfv3rqL37y8PLz44otGxc6q0htvvIENGzZg9+7daNy4Mfr06YOsrCwkJCTgqaee0k3laqhXr16YM2cOXn75ZfTs2RNt27ZF06ZNdYWzTp8+DS8vL7NTvVrqiSeewP79+/HNN9+gXbt26N69u65I3OHDh9GhQwfd9Jn+/v749ddfMWTIEIwfPx7Tp09H69at4erqitTUVJw+fRoqlQqHDx82Odi5Mg0ePBgffPABXnvtNWzevBlB/41s++yzzxAYGIjRo0fj+PHjmDFjBtq3b48OHTqgYcOGUCqVSElJwZkzZ9CuXTtd72NZeHp64sEHH8Ty5cuxcuVK+Pr6mtzXOnfujL59+2Lz5s1o1qyZrh5FYmIigoKC8OCDD+L333+v0OcAiB7Wnj17IjExEVeuXDH5tyjrgGjAeJD1kSNH8PrrrwMAGjZsaLancMKECaUWUQSAdevW4YknnkDjxo3Rpk0buLu7Izk5GXv27IGjo6Nuql1A9DwMGDAA69evR7t27dCxY0e4uLggOjoa48aNg4+PDxYsWIDhw4djxIgR+Oqrr3RF4m7fvo3HH3/c5PS6gAgir1y5ggEDBrBAnLVJkkhFkie9V6lESlJWlphC1ddXjE7l507VkLOTmFq1Vy8R76akiIm/XFxEj0O7dkxVsoRdBg9t2rRBw4YNcenSJezdu7fYADw5Xaki0yRWChOpODVZdHQ0Lly4gPnz52P9+vU4duwY7t69Cy8vL0RFRWHy5MmYMGGC0UWRt7c3tm7dipkzZ2LVqlX4448/4OLigs6dO+PZZ581mu+9qrm6uuLff//F9OnTsWLFCvz+++9o0KABPvroI7z22msmgwcAeP7559GtWzd8+eWX2LZtG/744w94e3sjPDwckydPttqsYF9//TXi4uIwb9487N+/H3v37kVISAji4uIwceJEo2Wjo6Nx/PhxzJo1C3/++ScSEhLg6OiI0NBQPPDAAxg2bBhatmxplXaVRadOnfDTTz9h5syZ2LRpE3JzcwGIHgd5EPRnn32G/v3745tvvsHu3btx9OhR+Pv7Izw8HG+88YbJaVUt9fjjj+sGSj/88MNm6xv8/vvv+L//+z+sXr0af/31F4KDgzFy5EjdvmAtEydORGJiIlasWIE33nij2POlFUozxTB4yMjI0PU87d6922RKHgDExMRYFDy8+uqrCA8Px86dO7F9+3aoVCqEhYXhsccew+uvv6678SNbtGgRXn/9dWzevBnLly+HRqOBWq3W1ap4+OGHsXnzZkybNg0HDx7EyZMn0bNnT3z66adGvRFFyX/Dovs9lZMkieDAVMAAiDEM9eszYCC7walXK0YhVXRKEBuZP38+Jk+ejHvuuQcbN27U3aGUqxNnZGQUq55bFvIsTEVTQIrSarW6FIuoqCg4FJ2TWqsF/qu4bDc6dODc2kTVQH5+PiIjIxEcHKwrGkclkyQJLVq0QHZ2NpKTky2asKHU43htJEligLM8U1J2tr4WgySJas++vmLGJCIquxs3xNifNm1EAcQqZul1ril22fMAiDtK8fHxWLNmDaKiotC9e3dkZ2dj165dKCgowMSJE8sdOFiVg4O4GLcnPHESVQuurq5477338Nxzz2Hjxo0YOHCgrZtU7f322284e/YsFi9eXO4CdbWauYBBo9FPq+rhwSrPRLWY3fY8AOJu0XfffYdFixbh7NmzUCgUaNeuHSZPnowxY8ZUaNtW63kgIqoAtVqNVq1awd/fH3v27LF1c6o1SZLQsWNHqNVqHD161OLjca0/jqtUxgGDUimChoICETB4ewOengwYiKyJPQ+24eDggGeffRbPPvusrZtCRFQpnJyczM4+RcYUCgUO21uaqK3k5JgOGPLzRbBQpw4DBiIyya6DByIiIrJQbq4IFtLTRW+DUil+8vJEwBAYKAKG2tbzQkRlwuCBiIiopsrL0wcM8hgGpVIEEl5eomCblxcDBiKyGIMHIiKimiQ/Xx8wZGXp05JyckTPgp8fEB4OODrauqVEZIcYPBAREdm7ggIRLNy9KwIG+UelEgGDjw8QFsaAgYgqjMFDBRlWt9ZqtbVvlg4iIjun1Wp1vyvsaYBwQYF+0LNSqe9hUKnEdKry1KqcspaIrIhHlApSKBRwcXFBQUEBVCoVfH19bd0kIiIqA5VKBQBwcXGp/sFDYaHpgCE7WxRsY8BARJWMRxcr8Pb2xp07d3Dz5k0AgKenJ3sgiIiqOa1WC5VKpTt2e3t727hFZhQWAhkZIi2paMDg5iYqPYeEMGAgoirBI40VBAYGQqVSIS8vD9euXbN1c4iIqIzc3NwQGBho62boqdWmexiyskTA4OPDgIGIbIJHHStwdHREREQE7ty5g6ysLBQUFNi6SUREZAEXFxd4e3sjMDAQjrYeTKxWix4Gw4AhM1P86+oqajEEBwPOzrZtJxHVagwerMTR0RHBwcEIDg6GJEmQJMnWTSIiohIoFArbj3HQaEynJGVlAS4u+mrPLi62bScR0X8YPFSCanFCIiKi6kkOGO7e1fcsFA0YGjZkwEBE1RKDByIiosqm0YhAIT1dHzDItRicnMQYhgYNRHoSEVE1xuCBiIioMsgBw927oqdBpdL3MDg6ilmSIiMZMBCRXWHwQEREZC1abfGUpKwsETQ4OooehogIMWMSEZEdYvBARERUEVqtvochM1OfjpSVBTg4MGAgohqFwQMREVFZFQ0YDAc9KxQiJal+fQYMRFTjMHggIiKyRGkBg48PAwYiqvEYPBAREZnDgIGIyAiDByIiIkPmAobsbPG8tzcDBiKqtRg8EBERFQ0YVCp94ACIgCE8nAEDEdV6DB6IiKh20mpFj4JcuM2wDgMgAoawMMDd3bbtJCKqRhg8EBFR7WGqh4EBAxGRxRg8EBFRzcaAgYjIahg8EBFRzcOAgYioUjB4ICKimsGSWZIYMBARVQiDByIisl8MGIiIqhSDByIisi+WFG7jtKpERJWCwQMREVV/Go0+YJB7FljpmYjs0Z07wF9/Abt3A4sWAaGhtm5RmTB4ICKi6skwYJB7GLKyGDAQkf25dQtISBA/R46IHlRABBEdOti0aWXF4IGIiKoPjQbIyND/qFQMGIjIPl27pg8Yjh0zvczGjcA771RtuyqIwQMREdmWHDAY9jDIaUmOjgwYiMh+pKaKYCE+Hjh92vxydeoAPXoATz5ZdW2zEgYPRERU9dRq8ylJcsAQEcGAgYiqN0kCLl7U9zCcP29+2bAwIC4OiI0FAgMBZ2egTZuqa6uVMHggIqKqoVbrexjkQc9ywODkJAKGyEjA1dXWLSUiMk+SgLNn9QFDcrL5ZRs0EMFCXBzQrJlIvwSAGzeqoqWVgsEDERFVHrVaBAsZGcVnSXJ2ZsBARPZBkoCTJ0U6UkICcPWq+WWbNtUHDI0aVV0bqwiDByIisq7CwuI9DPK/Li6icFuDBgwYiKh602rFQGc5YLh50/yyLVuKgCE2VqRc1mAMHoiIqOIKCowDBpVKHzC4ugJeXmKAoIuLrVtKRGSeWg0cPqxPSbpzx/yybdvqAwY7q9VQEQweiIiofAoKRLBw965IQzLsYXBzEz0MwcEiPYmIqLoqLAQOHBA9DImJ4kaIKQqFqMkgBwzBwVXZymqDwQMREVkuP7/4GAY5cHBzE2MY6tZlwEBE1Vt+PrB3rwgYtm0TxzFTHB2Bzp1FsBATI2ZJquUYPBARUcny8vQpSfLsSFlZIjXJ3V30MISEiBmTiIiqq7w8YOdOkY60Y4c4hpni5AR07SoChl69AD+/Km1mdccjPRERFZebaxwwyD0McsDg4wPUq8eAgYiqt+xsESgkJIjAIT/f9HKurkC3bmKGpJ49xTgtMolHfSIiEnJy9AGDYZXnnBzAw4MBAxHZB6VSpCLFx4vUpIIC08u5u4sqz7GxQHS0OM5RqXgGICKqzVQqfcCgUoneBaVS9Dx4eoqAISxM5P0SEVVXd+8CW7eKgGHfPkCjMb2cp6dIRYqLE6lJrGJfZgweiIhqm+xs/aBnw4AhL0901fv5AeHhDBiIqHpLSwO2bBEpSQcPiroMpvj6Ar17ix6GLl04ZXQFMXggIqrpJKl4wCDPlCQHDAEB4l8HB1u3lojIvBs39DUYjh4VxzdTAgLE7EixsWK2JKZbWg0/SSKimkiSRI9C0R6GrCyR/+vtLaYc9PRkwEBE1duVK/oqzydPml+uTh0RLMTFAe3asfe0kjB4ICKqKbRa8wGDWi0Chjp1RMCgUNi6tURE5iUnA//+KwKGc+fML1evnr5oW5s2vBlSBRg8EBHZM60WyMwUwUJmpj4lSakUz/n4iCqoDBiIqDqTJODCBREsxMcDFy+aXzYiQvQuxMYCzZvz2FbFGDwQEdkbjUYECnfvGld5zsoSJ2BvbyA0VExDyJMqEVVXkgScPq0fw5Caan7ZRo1EwBAXBzRuzGObDTF4ICKyB2q1PmCQexjkgEGhEAFDWJgIGIiIqiutFjhxQj+G4fp188tGRenHMDRoUGVNpJIxeCAiqq4KC0U6UkZG8R4GR0cRMNSvz3nKiah602iAw4dFsLBlC3D7tvllW7XSBwzh4VXXRrIYgwciouqkoEBftE2pNO5hcHYWYxgiIhgwEFH1plYDBw6IHoatW4H0dNPLKRRiZqS4OKBPHyAkpGrbSWXG4IGIyNby8/UzJGVliR4GuafBzU3UX2jYkIWNqMbLzgaOHBGp7wUFYpePjBTXll5etm4dlaqgQFR3lgMGpdL0cg4OQKdOooehTx8gKKhq20kVwuCBiMgWcnP1PQxyOlJ2tj5g8PEB6tYVvQ1ENVyhGti8SQQOd9KBzAxx49rJCfD1E9eh7dsDffsBzrxyqV7y8oDdu0VK0rZtorfUFEdH4N57RcDQuzfg71+17SSr4VeQiKiq5OToexjkQEGpFI97eIgxDCEhrIRKtUqhGli5Ejh+TPQ4uLqK4sDOzmLYT9pt4OoV8bW5kw6MHMkAwuZycoAdO0TAsHOnuBliiouLCBji4oBevcRNEbJ7/PoREVUWSRJ34UwVbcvNFbUXfHzEtKoMGKiW2rxJBA4pKUCDhoCPt/HzQUGAMgtIviT+HxgADBxY9e2s9bKzRc9CfDywZ49ItzTF1RWIjhYBQ48e4jhHNQrPVkRE1iRJIjiQZ0mSi7ZlZ4vufS8vwM9PzCLi6GjjxhLZluEYB1OBg8zHWzyfkiyW79WLYyCKqpTxIhkZImcsIQHYu1fkkpni4QH07ClSkrp355TRNRyDByKiitJqRYBgGDDIKUmFhSIdKTBQ3IFzcLB1a4mqDXmMg6ur+cBB5uMtlktPB44eFTe3qRLGi9y5AyQmih6GgwfFNKumeHuLKC4uTqQmubpa6y1RNcfggYioPDQaERyYKtqm0YgTa3CwCBhYCZXIpNRUcbEbEGDZ8gEBIj5PSWHwAFhxvMjNm6L+QkKCqMcgSaZf0M8PiIkRAUPnzpzQoZZi8EBEZCnDKs+GRduys8XzXl5AvXqiy54BA1GpCgrE18rSa1AnJxGbFxRUbrvsRYXGi1y7JoKF+Hjg+HHzLxIYKNKRYmOBDh04PosYPBARlUiu8nz3rj5QkHsYHBzEgOfwcBZtIyoHFxdxLVpYaNnyarUYKsSSJ+UbL5K6PQX5lxPguj0eOHPG/Mbr1tVXeW7ThuOzyAiDByKiovLzTQcM2dms8kxkRRERIi8/7bZldcLS04E6dcRA4NrOovEikoSgrIvoeTUeDS8noP6FC+Y3GBamDxhatWLvKZnF4IGICChetM0wYHBzE2MY6tThLU8iK2rfXsz+efWKSK8padC0MkvE9QEBYgah2s7seBFJQt3Ms2hxNR5R1xIQlJ1ifiORkSJYiIsDmjVjwEAWYfBARLWXSqUPGAxnSMrJEeMWfHxYtI2oEnl5iQAiI0Pk5ZtLv5Hz9iMjxfKcprXIeBFJQujdk2h+LR7NrybAP+eq2fXSA5si4JH/ehgaNaq6BlONwTMiEdUekiQCBDlgyMkRvQtKpajBwKJtRFWubz+RfgOIvHx5xiAnJ3FxnJ4uehwiI4E2bcXyBLg6adAi7xi6nklA+7sJ8M29aXbZa34tccgvFrvcY9H6/giMGlWFDaUah2dHIqrZtFoRIJiaUjU/X9zCDAgQgQMHBRJVOWcnMYVoYIDI409PF/G9RiO+knXqiK9omWoV1FRqtZhKNT4eD2/aAhflHbOLXgloi9OhsTgbGotMz1CcO8fxImQdtfkrSEQ1lUYjAoWMDPFvTo7oXTCswVCnjqiKyqJtRDbn7CSmEO3VSxSAS0mxYpVke1dYCOzfL6ZU3bpVHNcAFB19pYUDLge1x5nQOJwN7YMs92DdcxwvQtbE4IGIaga1Wl/h2bCHwbAGQ0iICBg4KJCoWvLyEsXfan0BuPx8YM8eUYdh2zZxLDNB6+CIC76dsdUxFjdaxMAhKLDYMhwvQtbG4IGI7FdBgfGUqoYpSY6OoochLEwMfiYiqs5yc4GdO0XAsGOH6DE1xckJ6NoViI2FJroX9v7lh/Nyhel0jhehysfggYjsS16eCBYyMvQBg1Ip/nVxEQFDZKQYdUlEVJ1lZ4tAIT4e2LVLXOmb4uoKdOsmZkjq2VPXfeAMjhehqsfdiIiqP3lK1YwM4xoMKpWoweDjIyqiOjvbuqVERCVTKkUqUny8SE0yV17b3R3o0UMUbouOFimXJnC8CFU1Bg9EVP3IU6rKPQzylKpZWaJr39NT9DDUq8cpVYmo+rt7F0hMFAHD/v2ia8AUT08RBcTFidSkMlSx53gRqio86xJR9aDVijtycg+DYcAgT6nq5weEh3NKVSKq/m7fBrZsEWMYDh0SxzhTfH2B3r1FD0OXLqxiT9Uegwcish2NRh8syOMW5ICBU6oSkb25cUP0LiQkAMeOiV5UUwIDgZgYETB06sQeVLIr3FuJqGoVFhoHDIZjGBQKTqlKRPblyhURMMTHA6dOmV8uOFgEC7GxYiACe1DJTjF4IKLKl5enDxgMZ0jKzhaDnL29gfr1y5TfS0RkM5cu6XsYzp0zv1xoqD5gaN2aPahUIzB4IKLKkZOjH/As9y7IP25uooehTh3m9xJR9SdJwPnzIliIjxfBgzkREWLAc2ws0Lw5e1CpxmHwQETWIc+QJPcwGI5fyM0VaUje3iIlifm9RFTdSZJIQ0pIED+XL5tftlEjETDExQGNGzNgoBqNZ3AiKj/DGZIyM40DBs6QRET2RqsFjh8XvQtbtgDXr5tfNipK9C7ExQENGlRZE4lsjcEDEZWNWi0CBc6QREQ1gUYDHD4sehe2bBFTrJrTqpU+YAgPr7o2ElUjDB6IqHQFBcYDnovOkCQXbHN3Z3c9EVV/ajVw4IDoYUhMFOOzTFEoxMxIcXFAnz4i7ZKolmPwQESm5eYWnyFJDhw4QxIR2ZuCAmDfPhEwbN0qek5NcXAQtRfi4kQthqCgKm0mUXXH4IGIBEkSAYLhgGe5d0GlEkGCnJLEGZKIyB7k5QG7d4uUpG3bxLHMFCcnUd05NlYEDH5+VdlKIrtSI4KH9PR0NG/eHLdv30ZUVBTOnDlj6yYR2QetVgQHcsCQk6Mfv5CXB3h66lOSOEMSEdmDnBxgxw4RMOzYIY5lpri4AF27ioChVy/Ax6dq20lkp2rE1cCrr76KtLQ0WzeDyD5oNPoBz5mZxgFDYaGYISkgQPzLAc9EZA+yskTPQkICsGePmO3NFDc3IDpaBAw9eogbJERUJnYfPMTHx+PHH3/E008/jQULFti6OUTVU0GB8QxJhgXbJEn0LtStK2ZI4oBnu5edDRw5AqSmij+9iwsQGSnGfXp52bp1RFaSkSEGOyckiLEMarXp5Tw9gZ49RcDQvTvHaRFVkF0HD7m5uZg8eTJatmyJ119/ncEDkSHDAc+GsyMZDngOD+eJtAYpVAObN4nA4U46kJkhrqecnABfPzFGtH17oG8/wNmuj/5Ua925I6ZTTUgADh4UPammeHsDvXuLgOHeewFX16ptJ1ENZtenj+nTpyMpKQmJiYlwdna2dXOIbIsDnmu1QjWwciVw/JjocXB1Fdlnzs4iGy3tNnD1itg17qQDI0cygCA7cfOmPmA4fFgc60zx8xPTqcbGAp07i52fiKzObk8dx44dw8yZMzFu3Dj06tULycnJtm4SUdXjgGf6z+ZNInBISQEaNAR8vI2fDwoClFlA8iXx/8AAYODAqm8nkUWuXhXBQkKCqPhsTlCQCBji4kS3Go9zVsUUSDLFLr9lWq0WEydOhJ+fH2bMmGHr5hBVrZIqPKvVHPBcCxme4E0FDjIfb/F8SrJYvlcvXgBQNZKSIoKF+HigpFkT69YVwUJsLNC2LY9zlYApkFQSu/yTf/3119i3bx9++OEHBAYGVmhbrVq1Mvl4UlISGjduXKFtE1lNfr4+YDBV4dnLS1Q+5YDnWkk+wbu6mg8cZD7eYrn0dODoUTHxDJFNSBKQlKQPGJKSzC8bHi6Chbg4oGVLHucqEVMgqTR29+e+fPkypk6dit69e2Ps2LG2bg5R5cnJMT3gWaVihWcykpoq7gwGBFi2fECA2K1SUhg8UBWTJODsWREsJCSIndCcBg1EsBAXBzRtyoChijAFkkpjd8HDs88+i4KCAsybN88q2zt58qTJx831SBBVGkkSwYHcw2A4fiEnB3B3FwFDcDAHPJORggKRUmDp+FAnJzFJTUFB5baLCIA4tp08qQ8Yrl41v2zTpvqUpEaNqq6NBIApkGQZuwseNmzYAD8/PzzzzDNGj+f9V0EyNTUVMTExumW9uDdTdabRiHELcsE2wxmS8vPF0djHBwgN5UBAMsvFRewehYWWLa9WA46OjEGpEmk0wLFj+kHPN2+aX7ZlS31KUv36VddGKoYpkGQJu7waycjIwNatW00+l5ubq3tOba5gDJEtFRbq05GUSuMeBq1WP52qhwcHApJFIiLEIMa02yKloDTp6WIXi4ys9KZRbaJWA4cOiWBhyxZRk8Gctm1FwBAbK26OULXAFEiyhN0FD5KZ+Z2Tk5PRsGFDREVF4UxJszQQVSGlEtixA0g6kQtFZga8tRloEpKNdk1U8NL+N/DZ0VEEDKGhIjWJeb1URu3bA9u2iUGMyqyS7xgq/+vUCggQ0y0SVUhhoajunJAgqj1nZppezsEB6NBBBAt9+oj0S6p2mAJJlrC74IHIHhQWSFj3YzaObs1AzvVM5KWr4JyXDR9FFu54qHAiwA2N23uh9wORcPZk5VOqGC8vEUBkZIhBjOZyleVBjpGRYnlmdVK55OcDe/aIMQzbtombIKY4OopibXLAYOnt7HJiTYKKYwokWYLBA5G1aLVAZiYK0zKx4rtMnDmkQnpyFvycshHhnQe1nyey4I0TynrIueSEJA1wvZDT3JF19O0ncpUBMYhRnl7RyUmc4NPTxTVfZCTQpq1YnshiubnAzp2ih2HHDpFuaYqzM3DvvSJg6NVLVH2uZKxJYD1MgSRL8GtEVBGFhcUKtm37Ixu3dmZBfVWDhpFecPILRJ6zJ6BwgDuAxvU4zR1Zn7OTCEQDA8RFVHq62C01GnFnsE4dEUzwIooslp0tAoX4eGDXLhF9muLqCnTvLgKGnj2r9DY/axJYF1MgyRI15ivUoEEDs+MhiKwqN9e4YJvBDEk5eQ44dcIbJ26HoG6UBzQ+CmhMbILT3FFlcHYSgWivXmL2k5QUpm9QGSmV4uoxPl6kJpnLX3F3B3r0EDMkde8uJniwAdYksC6mQJIlakzwQFRpJEkEB3LAIAcL8o+bmzhyRkTg0AE3JOcBkruYYbUknOaOKouXl9ifuE+RRe7eFYOd4+OB/ftFd5UpXl4iMo2NBbp2tXmBStYkqBxMgaTSMHggMkWuv5CZqa+/kPXf7Eh5eYCnpzj7hIQY1V/gNHdEZBdu3xbTqSYkiOlVtVrTy/n6AjExImDo0sXyaXiqAGsSVA6mQFJp+Ccnksn1FzIzdeMXdAGDRiOChYAA8a+Z+guc5o6Iqq3r1/VF244dE72qpgQGioAhLg7o2LHaFqjkzZrKwxRIKkn1PCIQVZXcXH3AII9fkAMGBwdRf6FePYvrL3CaOyKqVi5fFulICQnAqVPmlwsO1hdta9dOHJiqOd6sqXxMgSRTGDxQ7SKPX5ArPOfk6AY7Q6XSj1+IjBR93GXEae6IyOYuXtT3MJw7Z3650FARLMTFAa1a2V1Fe96sIbINBg9U82k0+rEL8vgFebCzPH5B7mGoYPc8p7kjoionScD58/oehkuXzC8bESGChdhYoHlzu65oz5s1RLbB4IFqpoICfe9CkelUodWKYCEwUAQOVrzbxmnuiKhKSJJIQ0pIEEHDlSvml23USAQMcXFA48Z2HTAY4s0aIttg8EA1h0qln041O1s/fkGlEn3V3t6im97C8QvlxWnuiKhSaLVioLOcknTjhvllo6L0PQwNGlRZE6sSb9YQ2QaDB7JfWq0IDuQBz4bjF3JyxPgFb2/Rn12O8QvlxWnuiMhqNBrg8GHRu7BlC5CWZn7Z1q1FwNCnDxAeXnVttCHerCGqerxsIfuiVut7F5RKESRkZYmfggJxS8nHR/Qw2HB6QU5zR0TlplYDBw6IgCExURRxM0WhEHchYmNFwBASUpWtrBZ4s4ao6vFrRNVfXl7x6VTlHgaFQlyFBweL8QvVLJeX09wRUUnkKslXLhYgKGkvGiYnIOLSVjjnKE2v4OgIdOokAoaYGMtGCtdwvFlDVLUYPFD1I0+nKvcwGM6OlJ0t+qW9vID69UVqEhGRnSlUA/Eb8qDavAsNLyXgwbTtcNeqTC/s5CSqO8sBg59fVTbVbvBmDVHVYPBA1YNGI9KQDMcvyMXa5OlUvbyAunUtrwhERFTdqFRQb9uJa0vjEZu0Ey7aPJOLFShccC6wGzI6xeLe13vB2b+EqYSIiKoQgweynfx8fe+CqelUvbwqZTpVIqIqlZUl5hSNjwf27IFTQQFMlRoocHTDhZAeOBMai8Oe0Th32RORhYByt0jLISKqDhg8UNWRJNGjIPcuGE6nmp0tuuaraDpVIqJKlZEhBjsnJAD79olB0CbkO3nifEhPnAmLQ1JwN6idRCqmG4AGTmIGoSNHRD4/8/aJqDpg8ECVS6stno4k9y7k5oogwctLTInh4mLr1hIRlV9amj5gOHhQpGOakOPkg4NevXGteSwuBd8LjaPpY5+PtxjilZ4uBgIzl5+IqgMGD2R9BQUiUMjMNJ5ONTsbKCwUaUh+fmIeckdHW7eWiKj8btwQ9RcSEkQXgSSZXs7fH4iJQYIiDquSOsPDx8miiZICAsS9l5QUBg9EVD0weCDrKCkdycFBP9jZw4PpSERk365eFcFCfDxw4oT55YKCRP2FuDhRaMDJCalLgPxzgK+F8z44OYkOjIICazSciKjiGDxQ+VhS3dnLC4iI4HSqRGT/kpNFwJCQAJw5Y365kBAxpWpcHNCmTbHJHlxcREBQWGjZy6rVooOWWZ1EVF0weCDLFRaanh0pO1vMnFRNqjsTEVWYJAFJSaJ3ISFB/G5OeLgIFmJjgZYtS+xdjYgAfP2AtNuW1XdLTxdDwiJNTc9ERGQDvMKjkuXk6AOG7Gzj8Qtydec6dUQ6EqdTJSJ7JknA2bMiYIiPB1JTzS/bsKE+YGja1OJ0zPbtxaytV68AyiwxKNocZZa4LxMQIKokExFVBwweyJicjiQPeDZMR1Kp9OlItaC6c3a2GP+YmiryjV1cxN2/du04ZSJRjaHVAidP6scwXLtmftlmzUSwEBsLNGpUrpfz8hIBREYGkHwJaNDQdAChzBLPR0aK5XnMIaLqgsED6dORDGdHkgMGOR3J2xuoV69WpCMVqoHNm0TgcCcdyMwQecdOTiLdYOtWcTLv2w9wrvkfB1HNo9GIuU8TEsRMSTdvml+2ZUv9GIb69a3y8n37iWMLIOo4uLqK3gUnJ3GsSU8Xh97ISKBNW7E8EVF1wUuf2kpOR8rM1A9yZjoSCtXAypXA8WOix0E+qTs7ixgr7bZIN8jIECf/kSMZQBDZBbVa1F5ISBC1GO7cMb9su3b6HoZ69azeFGcncewIDBA3KdLTxTFFoxGDo+vUEccd3qQgouqIh6TagulIFtm8SQQOKSmm0wmCgvTpBIA4+Q8cWPXtJCILFBaK6s7x8aLLMDPT9HIODkCHDqJ3oU8fcfVeyZydxLGjVy/RCZKSwvRIIrIPDB5qMlPpSHLvQi1MRyqN4RgHc3nIgHi8QUORbnDkiDj58yRPVE3k5QF79ogehm3bxBfbFEdH4J57RO9CTIy41W8DXl6i+BsLwBGRveAVY01TdHYkw+lUa3E6kiXkMQ6uriXPgAKI511dRbrB0aM88RPZVG4usHOn6GHYsUP83xRnZ+Dee0UPQ69egK9v1baTiKgGKHfw8MYbb+DDDz+EWy1OcakWzBVrkwMHpiNZLDVVDI629AZkQID42FNSGDwQVbnsbBEoxMcDu3aJ3lRTXF2B7t1FwNCjB7sJiYgqqNzBw8yZM/Hbb79h/vz5iI2NtWabqDQFBcaDneXehaws8RzTkcqloECMqXR2tmx5JycxwLGgoHLbRUT/ycwUqUgJCSI1yVyZZg8PEdHHxYl/3d2rtp1ERDVYua8sH3nkEaxduxZ9+/bF2LFjMXPmTPj5+VmxaWREpdIHDHKvghw4yOlIwcFMR6oAFxcREJi7HilKrRZp0y4uldsuolotPV3MjpSQAOzfLyJ2U7y8gJ49RcDQtSt7WomIKkm5g4fVq1dj/fr1ePbZZ/HDDz/gr7/+wpw5c/Doo49as321lyTpxy5kZoocXnmwc06OPh0pIIAnSSuJiBB1HNJui1mVSpOeLoaPREZWetOIapfbt0WwkJAAHD4s0jNN8fUVg51jY4EuXSzvNiQionKrUE7L4MGD0adPH7z11luYP38+Ro4ciZ9//hlz585FaGiotdpYO129KpLplUoRNKjVgKcn4OMDhIYyHakStG8vMiKuXhHTsZY0aFr5X/28gAAxpSIRVdD162L8QkICcOyY+eUCA0XAEBcHdOzIYyERURWr8FHXy8sL3377LUaPHo0JEybgjz/+QGJiIiZNmgRPT0+z67333nsVfemaraBA3NrWaICQEJGOpFDYulU1mpeXCCAyMkQdB3PTtcp1HiIjxfIcf0lUTpcv6wOGU6fML1e3rr5oW9u2Il+QiIhswmq3bLp164bDhw+jV69e2LdvH7744guTy0mSBIVCweDBUl5eoseBqkTffmK6VkDUcZArTDs5ic6f9HTR4xAZCbRpK5YnojK4eFEEC/HxwPnz5pcLCxPBQlwc0LIlx3IREVUTVgsekpKSMHHiROzfvx+Ojo546KGHSux5IKqOnJ2AkSNF5egjR0SwkJEhOoAcHcUYh4AA0ePQt59YnohKIEkiSJB7GC5dMr9sRIQIFuLigKgo9rYSEVVDFb700Wq1+OKLLzB9+nTk5uaiffv2WLRoETp27GiN9hFVOWcnYOBAUUPq6FEx9KSgQMyqFBkpxjgwVYmoBJIk0pDkgOHKFfPLNm4sgoXYWPE7AwYiomqtQsHDkSNH8NRTT+HIkSNwdXXFxx9/jDfeeAOOzEelGsDLS0wRzwJwRBbQaoHjx/UBw40b5pdt3lw/hqFBgyprIhERVVy5g4e3334bM2fOhFqtRkxMDBYsWIAmTZpYs21ERFSdaTRiKtX4eFGL4fZt88u2aaMPGMLCqqyJRETVkrkpqO1AuYOHzz77DL6+vpgxYwYmTpxozTYREVF1pVYDBw7oA4a7d00vp1AAHTqIYCEmRswaR0RUW2k0orBvdrb4cXAAvL3tMlWz3MHDQw89hG+//RYhPCEQEdVsBQXA3r0iYNi2TdSfMcXREejUSR8wWFJtkYiopsrL0wcLeXmAu7u+wK9cuysw0NatLLNyBw/r1q2zZjuIiKg6ycsDdu0S4xe2bxd3zExxchLVnePigN69AT+/Km0mEVG1odUCOTn6gEGSRJAgBwuenoCvr/jx8rLLXgfAilO1EhGRnVOpgB07RMCwc6cIIExxcQG6dRM9DL16ia53IqLaqKBAHyzk5ABubiIwCA8XBX69vUWw4OMjikfVAAweiIhqs6wsYOtWETDs2SNOhKa4uYmpx+LixL+s40NEtZEkGfcuqNX6FKTQUBEwyL0L3t41ssAlgwciotomI0MMdk5IAPbtEyc/Uzw9Rc9CbKzoaXBzq8pWEhFVD2q1PljIzha9r15eQL16xr0Lvr614jjJ4IGIqDZISxMBQ3w8cOiQmPnDFB8fMXYhNha4915xkiQiqk0kyXiwc36+uJni5QXUrSsCBh8ffTpSLatvxuCBiKimunED2LJFBAxHj4oToin+/mJ2pLg4oHNnMQiaiKg2Uav1U6mqVCLdyMsLqFOneO+Ch4etW2tTPEMQEdUkV66IdKSEBODECfPLBQXpi7Z16FDr7pwREZmdSjUoyLh3wdeXN1UM8JMgIrJ3ycmidyEhATh71vxyISGidyE2VlR8roED+YiIzCptKlUvL32w4Olpt1OpVjYGD0RE9kaSgKQkETDExwMXL5pftn59ESzExQEtWvBkSES1S2GhPlhQqcxPperryzFeFmLwQERkDyRJ9CrIAUNqqvllGzbU9zA0bcqAgYhqD0kCcnPFNNSGU6l6e+tnR6rhU6lWNgYPRETVlVYLnDypH8Nw9ar5ZZs1049haNSo6tpIRGRrhlOpqlRifIKXl0jVLJqO5O5u69baPQYPRETViUYjZkZKSBAzJd28aX7Zli31PQz161ddG4mIbK3oYGcPDxEkBAcXH+zMCSGsisEDEZGtqdXAwYMiYEhMBO7cMb2cQgG0basPGEJCqrSZREQ2Iw92zsoSvQuSJIKFwEDRu+DpycHOVYTBAxGRLRQWAnv3ioBh61YgM9P0cg4OQMeOImDo00dMIUhEVBtwsHO1xOCBiKiq5OUBe/aIAc/bt4sToimOjsA994iAISZGFHGjGiU7GzhyRIx7LygQ1z2RkUC7duLaiKhWkgc7ywFDYSEHO1dDDB6IiCpTTg6wa5cIGHbsECdGU5ydga5dRTpSr17i5EilsreL8EI1sHmTaPOddCAzQ2StOTkBvn6iE6p9e6BvP8CZZ2iqDQwrO2dni5sn3t5A3boc7FxN8dBERGRt2dmiZyEhQQQO+fmml3N1Bbp3Fz0MPXpUz6vdasoeL8IL1cDKlcDxYyLYcXUVtamcncUN1rTbwNUrQEaGeE8jR1afthNZlbnKznXqGA929vFhZedqiH8RIiJryMwUV6wJCWIsQ2Gh6eU8PIDoaBEwREfzTlo52OtF+OZNos0pKUCDhoCPt/HzQUGAMgtIviT+HxgADBxY9e0ksrqilZ21WhEssLKzXaoGh1MiIj27SkNJTxezI8XHAwcOiGlWTfHyEqlIcXEiNcnVtUqbWdPY40W44X5tqs0yH2/xfEqyWL5Xr2q43xNZwrD2Qna2OO55eQFhYcWnUuVgZ7vC4IGIqgW7SUO5fVv0LsTHi8ZqtaaX8/UVg53j4sTgZ2fnqmxljWWvF+Hyfu3qar7NMh9vsVx6uij5ER1dJU0kqri8PH1l5/x8/WDnkBDRy+rrC/j5cbCznWPwQEQ2V+3TUK5d01d5PnbM/HKBgWI61dhYMb0qc3Wtzl4vwlNTRUAcEGDZ8gEBYn9PSWHwQNWYVms82FmhMB67YDiVqoeHrVtLVsIzGxHZXLVMQ0lN1QcMp06ZX65uXREsxMaKAm6sZFqp7PUivKBA9KRZ2gHl5CSy4AoKKrddRGUm117IyhLjGOTaC/XrG0+l6uPDHtcaisEDEdlUtUpDuXhRpCMlJADnz5tfLixMBAtxcUDLlux+r0L2ehHu4iLaYm4cfVFqtYhDmQpONmeu9oKPDxAaalzZ2dubg51rAQYPRGRTNk1DkSTg3Dn9GIbkZPPLRkbqA4aoKJ4gbcReL8IjIsTYnbTblhUJT08XmR+RkZXeNKLiNBrjdCQHB3G3xrD2gp+fCBjc3GzdWqpiDB6IyKaqPA1FkoCTJ/UpSVeumF+2SRN9wNCoEQOGasBeL8Lbtwe2bRNjd5RZJQfKyiwx1jQgQMwyRlQlCgr0g51zc/W1FwIDi6cjcTxXrca/PhHZVJWkoWi1YqBzfDywZQtw44b5ZZs3F8FCbKztrzipGHu9CPfyEm3PyBBjd8yl6MljeyIjxfKcppUqjSQZ115Qq/U9CuHh+nQkPz/WXiAjDB6IyKYqLQ1FrRY5UXLAkJZmftk2bfSDnsPCLG062YA9X4T37SdS9AAxdkeeVczJSeyu6eki2ImMBNq0FcsTWZVabZyO5OQkvhz16hWvvcB6NGQGgwcisimrpqGo1cD+/SIdacsWcYVpikIBdOgggoWYGDEHOdkNe70Id3YS0wwHBoi4Nj1d7KIajQiI69QR76Na1DOhmiM/Xz87Ul6eSEfy9tZPp2qYjsTZ4sgCPDQRkU1VOA0lPx/Yu1cEDNu2AUql6ZUdHYFOnfQBgyWRClVL9nwR7uwkphnu1UsM+k9JsYNK6mRf5HQkefyCVit2qoAA/WBnOWBgOhKVQzU6pBJRbVSeNJSOLfPgtXenCBh27BDd8KY4OQH33isCht69Re4u1Qj2fhHu5SUG/LMAHFmFnI6UlSX+dXISvQvyVKpysTY/P9tPPUZ2j8EDEdmcJWkoilwVhrjvQI/UeETs2iW6301xcQG6dRODnnv2FCdNqrF4EU61VtF0JA8P8YUIDtanI/n5iWMg05HIihg8EJHNmUtDcS1QomP2NnRVxSPq7l44ac1MseTmBvToIXoYoqPFnTYioprEcHakrCzz6Uh+fiJ4YDoSVRIGD0RULejSUNpm4MbKRHgkJyAkdS8ctBrTK3h6ipyV2FjR08BCRURU0zAdiaohBg9EZHtpaUBiIhAfD6+DB9FEqzW9nI+PGLsQFwd06cKTJRHVPHI6kmGxNm9v43Qkzo5ENsTggYhs48YNMZ1qQoLIVZIk08v5+4vZkeLigM6dWdmUiGoWSRJBgjw7kkYjehX8/UWxNs6ORNUMz8JEVHWuXBHBQkICcOKE+eWCgoA+fUTA0KED764RUc2i0ejTkVisjewMgwciqlzJyaLKc0ICcPas+eXq1dNXeW7TBnBwqLImEhFVuoIC/WBnOR3Jy4vF2sjuMHggIuuSJCApSQQM8fHAxYvml61fX/QuxMYCLVqwO56Iag5JElOoyr0LhYUi7cjPT6QjeXrqBzszHYnsCIMHIqo4SQLOnBG9C/HxQGqq+WUbNdIHDE2a8IRJRDWHVmucjuTgIHoX6tYtPjsS05HITjF4IKLy0WrFuAV5DMO1a+aXbdZMBAtxcUDDhlXXRiKiyqZW64MFlUpMG+3lBURE6HsX5B+mI1ENwOCBiCyn0QBHj4rehS1bgFu3zC/bqpU+YAgPr7o2EhFVtrw8/fiF/HwRLHh7i7FbhulIXl7sXaUah8EDEZVMrQYOHhQBQ2KiKP9sikIBtG2rT0kKCanSZhIRVRpJEr0Kcv0FSRLBgjzY2dtbBAu+vmIgNFENZpfBQ05ODjZt2oT169dj//79SE5OhkajQZMmTfDwww/j1VdfhZeXl62bSWS/CguBvXtFOtLWrUBmpunlHByAjh1FwNCnj5hilYioJlCr9cFCdrYoSuntLXpS5elU5YCB9WeoFrHLvX358uWYOHEiAKBVq1YYMGAAlEoldu3ahffffx8rVqzA1q1bERwcbOOWEtmRvDxgzx7Rw7B9uzhZmuLoKKo7x8aK4m3+/lXaTCKiSiNXd87KEsdEDw/9gGcPD32w4O3N6aSp1rLL4MHFxQXPPPMMXnnlFTRt2lT3+PXr1zFo0CAcPnwYL7/8MpYvX27DVhLZgZwcYNcuETDs2CHmHjfF2Rno2lX0MPTqJe64ERHZO3PVnQMCxL9eXiJg8PMTwQMRQSFJkmTrRljT7t270b17d7i6ukKpVMLFxaVc22nVqhUA4OTJk9ZsnuUuXgROnhS5k7yzS9aUnS16FuLjgd27xZ02U1xdge7dRcDQo4c4iRIR2TtT06l6e4tjnKenvrqzn59IVSKqgSpynWuXPQ8ladeuHQAgPz8fd+7cQb169WzcIqJqIDNTjF1ISBBjGQoLTS/n4SEChdhYIDqaA/+IqGYwN51qZKS+urOfH6s7E1mgxgUPF/+rZuvs7IyAgAAbt4bIhtLTxexICQnA/v2iO94ULy+gd28RMHTtysJFRFQzlDadqjx+gdOpEpVJjQse5syZAwAYMGAAXHkRRLXNrVui/kJ8PHDkiOieN8XXVwx2josD7rlHjGkgIrJnkiTGcck9DJIkAoOi06n6+YmeByIqlxoVPGzcuBGLFy+Gs7MzPvzwQ4vWkXO+ikpKSkLjxo2t2TyiynHtmr7K87Fj5pcLDBTTqcbFAR06cGpBIrJ/Go3xdKrOziJgCAsTvQuG9Rd4k4TIKmrM1cPp06cxevRoSJKEzz//XDf2gahGSk0VwUJ8PHD6tPnl6tbVV3lu04a5vJUgO1t08qSmAgUFYnxlZCTQrh3HmBNVisJCfe9CTo4Ym2XYwyAHCz4+nE6VqBLUiODhypUrGDBgAO7evYtXX30VL730ksXrmhtlbq5HgshmLl4UwUJCAnD+vPnlwsL0AUOrVszlrSSFamDzJhE43EkHMjPEmEwnJ8DXT4xPb98e6NsPcK4RR1oiG8rL0wcM8vgFHx8gNFQ/naqvr+ht4DGPqFLZ/SktLS0Nffv2RWpqKsaNG4cvvvjC1k0isg5JAs6d0/cwJCebXzYyUgQLcXFAs2Y8eVayQjWwciVw/JjocXB1FdPCOzuLm6Jpt4GrV4CMDBFYjBzJAIKoTCRJzIokpyNJkkhBqlOneDoSxy8QVSm7Pp1lZWXh/vvvx5kzZzBs2DAsXLgQCl40kT2TJFHfQw4Yrl41v2yTJiJYiI0FGjViwFCFNm8SgUNKCtCgIeDjbfx8UBCgzAKSL4n/BwYAAwdWfTuJ7Io8fiErSwQOzs4iSAgPF+lIPj76gIFjtohsxm6/ffn5+XjwwQdx4MAB9O/fHytWrIAj87nJHmm1YqCznJJ086bZRbPqt0BSZCwuRsYir26kyK2vC3gxbqgyhmMcTAUOMh9v8XxKsli+Vy+OgSAqRh6/kJUlKj27u4uAIThY9DDI9Re8vTl+gaiasMvgQaPR4LHHHsOWLVvQs2dP/PLLL+WuJE1kE2o1cPiwCBa2bAHS0swuqm3TFmdDY5GAPriYHyZy648yt95W5DEOrq7mAweZj7dYLj0dOHpU1N0jqvUMxy8UFOhrLoSH63/38xO/E1G1Y5eXGt988w1+/fVXAEBQUBCeffZZk8t98cUXCAoKqsqmEZlXWAgcOCB6GBITRUK8KQqFmEo1NhaFvfpgZUJd5tZXI6mpYnC0pTUoAwLE3yclhcED1VKWjl/w82ORSiI7YJeXGXfv3tX9LgcRpkybNo3BA9lWfj6wd68IGLZtE3fb/r+9Ow+Ou77vP/6SrFurY2UkS7IOy+IwNr6AYoOPGCsOaSiHCTEpIXVCgA4tjiGZtLRNaic0hUzqDIbCMEMS152WX9pxUndgoAUfMWMLcI1jOzaHHeEDYwjYQta5klb7+f3xyVertXallbT3Ph8zGtu739396OuvPtrXfo53MJMmSVdfbdcvLFtmazJIeuVF5tYnmr4+O3AU7pbxWVl2KndfX3TbBSSUgQEbGJwRBmf9glN/gfULQNJKyp/Y9evXa/369fFuBhBcT4/U3GwDw+7ddh/yYLKypAUL7KLnpUvtL9IhmFufmHJy7H9df394x3u9NhsysxIpz+v1r1/o7ra7IDkjDE79BdYvAEkvKcMDkHA6O21Q2LFD2rPHjjgEk5srXXutHWFYssT+Eg2BufWJqa7OrjU5+4kd+RlNa6t971RfH/WmAbHn8finI3k8w+svOAueqb8ApAzCAzBe7e12KtL27dLrr4f+KDo/376bb2qyfxYUhPX0zK1PTPPm2f/2D07bKWMjBbv2Dpsjy8psxWkg6RljR1edEQafz34IctFFtm8bun6B+gtASiI8AGPR1mYXO+/YYdcyDAwEP66w0M4famqSFi4c1y9R5tYnJpfLBoi2NrvWJNSUMmctSn29PZ6pZEhaPl/g+oXMTBsSqquHr18It8MCkLQID8Bozp6126nu2CHt3x86MBQXS5/5jA0M11wz4UnuzK1PXCs+Z6eUSXatibMLVlaW/X9obbUjDvX10uw59nggqXi9/ulInZ32AxCXy17UzvqFkhL7xfoFIK0QHoBgPvrIhoUdO+wiAmOCH+d2S9dfb9cwXH11RHcNYW594srOstviTi6za1NaW+1IxMCADXDl5TZMUH8DSaWvzz8dyePxT0OqrAxc8OxysX4BSGP8SgMcp0/7qzwfORL6uPJyGxiamuy7wyhVNmdufWLLzrLb4i5davPlyZP2vVdOjg1wc+cyVQlJwCnY1tFhhzldLtuRFBbav7vddnQhzLVaAFIf4QHp7cQJads2GxiOHg15WGdRld6/tEn9S5Zr+s1XyFUc/WF65tYnB5fLLlBnkTqSgjF2G1Vn/YJkL+KKCvvn0AXPzIEEEAThAenFGOl3v7NhYft26b33Qh56zlWnA2XL1ZzXpJbsGcoayFDJAWnyqdhNR2FuPYAJ8/lsULiwYFtNjR1RoGAbgDGgl0DqM0Z6+23/GoZTp0IfOn26DpU3abtp0t5zjcrNy1BZmVSRbUf0z35ipxG1tdk39V/+cnQDBHPrAYyLs+C5o8PulETBNgARwlsNpCafTzp82L+G4cMPQx976aV2wXNTk156u0G//rWdvx5smtBFF/mnCUn2Tf0XvhC170ISc+sBhOnCBc+FhTYcVFXZvzuBgYJtACaA8IDUMTBgP57fscNurfrxx6GPnTVrMDCopkaS/ZDuwP+zAxOh1hdI9vZpDXYa0YED9k19LN68M7cewDChFjy7XPbLCQz5+fFuKYAUQXhAcvN6pX37bGD49a/tvJ5gMjLsR/TLl9uvysphhxw4YKci5eaOvLORZO/PzbUvd/Agb+gBxMiFC56NsaMLLHgGECOEBySfvj5p7147JenVV6Xz54Mfl5kpXXWVDQvXXz9qsYRTp6TzbfZDu3CUldn1BydPEh4ARNHQCs8dHXZRc1GRNHWqnYJUUuJf8BylraMBwEF4QHLweKTXX/cHhq6u4MdNmmSrOzc12WrPbnfYL9HXZwcysrPDOz4ry86U6usL+yUAIDyhKjw3NLDgGUBcER6QuLq7pT177JSk3bulnp7gx+XkSAsW2MCwdKnddnAccnJsIOjvD+94r9dmFWYGAIiI/n7/6EJPz/AKz243C54BxB3hAYmls9OOLOzYIb32mi1iEExurp0r1NQkLV5sf5lOUF2dVFJqt2MdZYaTJLveobzc7ngEAOPi8fi3VO3r8++KVFPjX/DsdrPgGUDCIDxgVJ2ddjHxqVNR2iK0rU3atcsGhjfesB/pB1NQIC1ZYtcwXHddxH+Zzptnc8sHp+12rCMtmm7vsLmmrMyeBwAIm7PguaPDrmdw6i84W6s6U5Jyc+PdUgAYhvCAkPq90isv+3chOt9m39dnZdlP6HftmkBxsnPn7O5IO3bY3ZIGBoIfV1Rk1y4sX26nJkXxl6nLZb+ftjZbxyHUdq1OnYf6ens8NRYAjMgY/4Lnzk475aioSKqutoHBqfBcWkqFZwAJj14KQfV7pV/8QvrtITvikJtrP2XPnkil5Y8/9ld5PnDAfuIWTGmptGyZnZJ09dXhr2COgBWfs9+PZOs4ON93VpYNTq2tdsShvl6aPcceH0rUR2wAJK6BgcDAkJNjA0NtbeAOScXF7JAEIKkQHhDUKy/b4DDhSstnzvgDw6FDoV9w8mR/DYb58+P26Vt2lg1Ck8vsG//WVhuQBgbs7/fychsmRhpxieqIDYDE5eyQ1NFhg0NeXuCUpKE7JLHgGUCS4q0Lhhn6ifm4Ki2fPOkPDG+/HfqFpkzxV3mePTthPn3LzrJBaOlSWwDu5MnwRw6iMmIDIHE5OyS1t9vFz866haoqf2BwuyOyqQMAJALetmCYMVdazjHKfv89tf5oh1zHtku/+13oB0yd6g8MM2cm9P7kLpfd0GksBeAiNmIDIHEF2yGprGz4gmd2SAKQgggPGCasSsvGaMr5dzXjzA7dc2qHpvScCH3stGn+KUmXXZayw/UTHrFJE6wFQVLq6fGPMATbIcmpwUDhFwApjvCAYUJWWjZG1Z8e0Ywz2zXjzA65uz4I/SQXX2xHF5Yvl6ZPT9nAMNSYR2xy7ZqKgwfHNrqRrFgLgqTi7JDkjDBkZNh0yw5JANIcPR6GCai0bHyqOXdoMDCU9Pw+5OPOTZmpyV/6wwhDXV3sGpwgwhqxGaKszK59OHky9cMDa0GQFHw+/w5JHR32AmWHJAAIwK9nDFM31aurzG90yeEdWti9Q67ecyGPPV02R69mL9fxacu17M7qlH8TPJKQIzYhZGXZXZz6+qLbrkTAWhAkrIEB/+hCZ6fdIcnlkhoabGFKZ3ShuDgtRlABYDSEB1her/R//ydt365FO3+txefbgh7mU6bev2ie3qlernerl+sDb4VOnpDmTKXScsCITRi8XvvhZapPkWYtCBKO1+sfXejutgubi4ulykp/YHB2SCIwAEAAwkM66+2V3njDbqm6a5f9RSrpwl+VAxmTdLL8ar1TvVxHq5apK2+yJCotX6iuzs7dP/uJ/SR9NK2tdr1lfX3UmxZXrAVBQujr8wcGj8d2WMXFdg2Dy+Vf8FxQEO+WAkBCIzykG49H2rPHBobdu+383iBMVpY+qFmg5rwmvdy7VP0FpSorkrI8krdzbJWW08W8edKrr9q5++0dI79Rbu+w56+sLPVHbFgLgri5cEtVl8teYC5X4JaqeXnxbikAJA3CQzro7LRBYccOGxx6e4Mfl5srXXut1NSkjCVLNCXPJdfL0vQD46u0nG5cLns+2trsiEyoKTrpNmLDWhDEVKgtVS8MDKk+XxAAooS3fKmqvd1+DL59u52aFOqdWH6+tHix3SFp0aKAIftsjb/Scrpa8Tk7RUeyc/edXYWysuwb6HQcsWEtCKLKGLtuwVnwLA3fUtXttjslsaUqAEwYPWkq+fRTu3Zh+3Zp71778W0whYU2ETQ1SQsXjjpkP55Ky+kqO8tuMzq5zM71Z8SGtSCIAqcGg7OGYdIkO6pQU2M/ACkpsYGBLVUBIOLS4K1Lijt7Vtq5005JevNNO0wfTEmJPzBccw0f60ZRdhYjNkOxFgQR4fMFbqmak2MDQ329vwaD221vy8yMd2sBIGURHpLRRx/ZsLB9u3TokP0ULpiyMmnZMhsYrrqKIfsYY8TGYi0Ixm1gwB8WnBoMRUVSRUXglqouF1uqAkCM8G4yWZw+bcPCjh3SkSOhj6uokK6/3gaGuXMZskdCYC0IwnZhDYaCAhsYKivtCMPQGgwAgJgjPCSy99+3hduam6WjR0MfV11tFzwvXy5dcQVD9kg4rAXBiPr77SYPF9ZgmDrV/t0JDPn58W4pAKQ9fkUnmv5+6ZFHpF/8Qjp2LPRxtbV2dKGpSZoxgyF7JDzWgiBAb69/hCFUDQa32w5TAQASBuEh0WRnS1u2BA8O06f7A0NjI4EBSYm1IGnM4/HXYBgYsBdDebl/S1VqMABAwiM8JKIvflH6h3+wf7/4YmnFChsYpk2La7MAYMx6evxTkowJXL/g1GAoLWVDBwBIEvTWiegrX7HTlxobpYYG+8sVAJLB0KJtHR12hLSoyK7NctYyOEXb2NABAJIO4SERzZgh3XffyLsqAUCiCFW0rbbWX4OhtNT+yYYOAJDUCA8AgLHz+WxgaG+3NRiys+2oglO0zVm/UFzM+iwASCGEBwBAeAYGAqs8O0XbnEXPFG0DgJRHeAAAhOZUee7osCMNeXl2NGHKFBsYnAXP7LMLAGmB8AAACOT12pGF9vbAKs9VVf7A4Hbb2wEAaYXwAACwgcHZUrWnx7+VanW1DQ7OCANVngEgrREeACBd9ff7A4PHY6celZZKNTWBgSEvL94tBQAkCMIDAKST3l7/Goa+PhsYysr8NRicRc9UeQYABEF4AIBU5/H41zD09wfukDQ0MGRnx7ulAIAER3gAgFTk8djRhfZ2u2NSUZFUURFY5bm0VMri1wAAIHz81gCAVNHT41/DYIwNDJWV/irPbrf9k8AAABgnfoMAQDLr7vZXeZbsyEJ1deAIQ0mJNGlSfNsJAEgJhAcASCbG2MDgLHrOyLAhYerU4SMMmZnxbi0AIMUQHgAg0Rljqzs7i54nTbKBobY2MDAUFxMYAABRRXgAgETkBAZnhCEry65hqKuzgcHZIam42I4+AAAQA4QHAEgUPp9/DUNHh906tbhYqq8PDAxFRQQGAEBcEB4AIJ58PjvC4Cx6zsmx4aChITAwuFwEBgBA3BEeACDWfD7/+oXOTikvz1+HoaDAhgW324YHAgMAIIEQHgAgFgYGbFDo6PAHhuJiacoUGxKGBgYAABIU4QEAoiVUYHAKtzlVngkMAIAkQXgAgEgaGPDvkNTVJeXnDw8MbredngQAQJIhPADARF0YGAoK7BqGqiq70NkJDPn58W4pAAATQngAgPHwev2Lnru7bWAoLraBoajIv0sSgQEAkEIIDwAQrgsDQ2GhDQzV1TYwOCMMeXnxbikAAFFBeACAkYQKDFOnBk5JIjAAANIA4QEALuT12vUL7e1ST09gYBg6wpCbG++WAgAQU4QHAJCGBwaXy65bqKkhMAAA8AeEBwDpi8AAAMCYEB4ApJeRAkNxsT8w5OTEu6UAACQcwgOA1Of12rDQ3i55PP7AUFsbOMJAYAAAYESEBwCpKVhgcLttWCAwAAAwLoQHAKkjWGAoK7N/EhgAAJgwwgOA5EZgAAAgZggPAJIPgQEAgLggPABIDgQGAADijvAAIHE526qeP8+iZwAAEgDhAUBiCVaHgcAAAEBCIDwAiD8KtwEAkBQIDwDig8AAAEDSITwAiB2vV+rstIGhu1sqLCQwAACQRAgPAKIrWGAoLpamTg1cw5CbG++WAgCAURAeAETewIB/SlJ3t1RQQGAAACAFZMa7ARPh8Xi0bt06XXrppcrLy1N1dbXuvvtunT59Ot5NA9LPwIDU1ia9/7507JgNDsXF0sUXS5ddJs2cKc2dK11+uVRZSXAAACAJJe3Ig8fjUVNTk5qbm1VVVaVbbrlFJ06c0KZNm/TCCy/otddeU2NjY7ybCaS2gQH/lKSuLik/XyopkaqqAkcY8vLi3VIAABABSRse/vEf/1HNzc269tpr9fLLL8vlckmSfvKTn+jb3/627r77bu3atSvOrQRSkM9npyR1dNjgkJ9vRxiqqvxVnwkMAACkpAxjjIl3I8aqv79fFRUVamtr0/79+zV//vyA++fOnatDhw5p3759uuqqq8b1GrNmzZIkHTlyZMLtHZf33pOOHLFvzNzuEQ/t7JQOHJBOnZL6+uxGNfX1dobIHzIVMDE+n3+EYWhgKCryF3Fzu+3tAAAgoU3kfW5Sjjzs3r1bbW1tamxsHBYcJOn222/XoUOH9Pzzz487PCSDfq/0yss2OJxrlc632Y1tsrKkklJp1y5p3jxpxeek7KT8n0Zc+Xx2KtL58zYw5OXZwFBZaXdMckYYCAwAAKSNpHxLefDgQUnSlVdeGfR+53bnuFTU75V+8Qvpt4fsiENurn0vl50t9fdLZz+RPjht16+ea5W+/GUCBMJgjH+EoaPDBoaiImnKlMARhoKCeLcUAADEQVK+nTx16pQkqaamJuj9zu3OcanolZdtcDh5UprWIBUXBd5/0UVSe4d04rj99+Qy6QtfiH07kQSMsSMMTmDIybEjDBUVdoTB7bbJlMAAAEDaS8rw0NnZKUkqCPFmprCwMOC4kThzvi7U0tKSsLs1DV3jECw4OIqL7P0nT9jjly5lDQT+wAkMTi2GnBw7wtDQEBgY/vCzBAAAICVpeHDWeGdkZIx4f6py1jjk5oYODo7iIntca6t08KC0aFFMmohEZIwt2OYEhqwsO8IwbVrglCQSJgAACCEpw0NRkX3H3NXVFfT+7u5uSRrcvnUkoVaZhxqRSASnTtnF0WVl4R1fVmbXPpw8SXhIS93d/ilJmZk2MNTX+0cYnMAQIowDAAA4kjI81NXVSVLIStLO7c5xqaavz+6qlJ0d3vFZWbaWV19fdNuFBNLTY3dJ6uiwoaC4WKqtDQwMRUUEBgAAMCZJGR7mzp0rSdq/f3/Q+53b58yZE7M2xVJOjg0E/f3hHe/1SpMm2cchhXk8/sAg2XDgBIbSUhsYiosJDAAAYNySMjwsWrRIJSUlamlp0W9+85thtR62bNkiSfqTP/mTeDQv6urqbB2Hs5/YXZVG09oqlZfbmSpIMR6PnZLU3m7XNBQVSVOnBgaGkhICAwAAiIjMeDdgPHJycvTAAw9Ikh544IGAtQ8/+clPdOjQIS1evFh/9Ed/FK8mRtW8eXbr1d5eux3rSNo77HFlZbbiNFJAb6909qzU0mIXwPh8UnW1dNll0uWXS7Nm2f/shgYbIAgOAAAgQpJy5EGSvvvd72rbtm1qbm7WJZdcoiVLlujkyZN64403NHnyZG3atCneTYwal8sGiLY2W8ch1HatTp2H+np7PJvoJLG+Pv8Ig9drRxicSs/OCENpqV0QDQAAECVJGx7y8vK0c+dOPfroo3ruuee0detWud1urV69Wo888ohqa2vj3cSoWvE5u12rZOs4OBWms7Lse8vWVvsBdX29NHuOPR5Jpr/fHxj6+mxgqKiwKbCkxB8YJk2Kd0sBAECayDCpXhRhnJytWkNt5Rp1770nHTki5efbN4lB9HttpekDB2xYaGuzuypNmmTfU5aV2RGHFZ+TspM2JqYZr9cfGDweGxiKi4cHhiz+QwEAwPhM5H0u70CSWHaW9IUv2MrRBw/aOg59fXZXpfp6O+2dqUpJwOu1ZcPPn7dbrLpcNvm5XDY4lJXZwBDu3rwAAABRQnhIAS6XLf5GAbgkMjDgr/Tc3e1fu1BT4w8MbjeBAQAAJBTCAxArPp8NDB0ddqShoMAGhalT7fQkt9uGBgpyAACABEV4AKLJ55O6uuyUpM5OKS/Prl2orPRPT3K77e0AAAAJjvAARJoxNjC0t9tRhtzcwK1VncCQnx/vlgIAAIwJ4QGIBGPs2gUnMGRl2SlJDQ12hMGZklRQEO+WAgAAjBvhAZiIoYEhM9MGhvr6wBEGtrwCAAApgvAAjJXHY9cwdHTYfxcXS7W1NjA4Iwwul5SREd92AgAARBjhAQhHb6+/eJvPZ9cwTJ3qDwxutw0RBAYAAJDCCA9AKH19/sDg9foXPRcV2R2TyspsYMjMjHdLAQAAYoLwAAzl9foDQ2+vDQoVFXYakhMYSkqkSZPi3VIAAICYIzwAXq+/2nNPjw0KkyfbKUlOYCgttTsoAQAApDHeDSE9DQz4qz13ddmgUFpqFz4XF/vXMWRnx7ulAAAACYPwgPTh89kqz+3t9s/8fDuyUF1tpyc5W6vm5MS7pQAAAAmJ8IDU5lR7drZWzcuzIwuVlXZ6khMY8vLi3VIAAICER3hA6glV7bmiIrB4G9WeAQAAxoTwgNTR0+MfYcjIsFOS6uvtCINTvK2wMN6tBAAASFqEByQ3j8e/tapk1y5Q7RkAACAqCA9IPhcWbysutoueXS67Y5JTvI3AAAAAEFGEBySHkYq3lZbaUYbSUqo9AwAARBHhAYnL67Vbqp4/H7p4m9tNtWcAAIAYITwgsfh8/mrPTvG2khKppsZORXICA8XbAAAAYo7wgPgzxl+8zanFUFIiVVX5i7eVlVG8DQAAIM4ID4iPobUY2tttMHBqMTjF28rKKN4GAACQQAgPiK2htRgyM+0Iw7Rp1GIAAABIAoQHRF9vrx1dOH/e/ptaDAAAAEmJ8IDo6O+3YYFaDAAAACmD8IDI8XrtdKTz523l56G1GJytVanFAAAAkLQID5iYgQH/1qrd3f7FzhcGhiwuNQAAgGTHOzqMnbO16vnz9s/8fBsUpk4N3FqVWgwAAAAphfCA8Fy4tWpurg0KlZX+0Qa3m61VAQAAUhjhASMLtrVqQ4PdKckZYSgoiHcrAQAAEAOEBwwXztaqRUXxbSMAAABijvAAq7/fPyWpv9+Gg6oq+ydbqwIAAECEh/Tm9foXPvf02KBQXs7WqgAAAAiK8JBufD4bGNrb7Z+FhTYg1NbakQVn4TNbqwIAAOACvENMB8ZIXV3+aUl5eXZkobIycGvVnJx4txQAAAAJjPCQyoZurZqVZUcWGhsDd0rKz493KwEAAJAkCA+pxuPxBwbJBoa6OruOwe2WJk+24QEAAAAYI8JDKujvt4ue29vtIujiYqm62h8YnK1V2SkJAAAAE0B4SFbOTkltbXa0oahIqqiwgcHZWrWkhJ2SAAAAEDGEh2QSbKckt9sGh6Fbq7JTEgAAAKKAd5mJzhh/YLhwp6ShW6uyUxIAAACijPCQ6D7+2AYDZ6ckl8sfGNgpCQAAADFEeEhUGRn+kYWhC59drni3DAAAAGmK8JCoKiv9tRmKi9kpCQAAAHFHeEhU+flSbW28WwEAAAAMYh9PAAAAAGEhPAAAAAAIC+EBAAAAQFgIDwAAAADCQngAAAAAEBbCAwAAAICwEB4AAAAAhIXwAAAAACAshAcAAAAAYSE8AAAAAAgL4QEAAABAWAgPAAAAAMJCeAAAAAAQFsIDAAAAgLAQHgAAAACEhfAAAAAAICyEBwAAAABhyTDGmHg3IhEVFRWpv79fjY2N8W4KAAAAEDEtLS3Kzs5WR0fHmB/LyEMIhYWFys7Ojtvrt7S0qKWlJW6vn8w4d+PHuZsYzt/4ce7Gj3M3fpy7ieH8jV+8z112drYKCwvH9VhGHhLUrFmzJElHjhyJc0uSD+du/Dh3E8P5Gz/O3fhx7saPczcxnL/xS+Zzx8gDAAAAgLAQHgAAAACEhfAAAAAAICyEBwAAAABhITwAAAAACAu7LQEAAAAICyMPAAAAAMJCeAAAAAAQFsIDAAAAgLAQHgAAAACEhfAAAAAAICyEBwAAAABhITwAAAAACAvhIYI8Ho/WrVunSy+9VHl5eaqurtbdd9+t06dPj/m52tra9OCDD6q+vl65ubmqr6/X2rVr1dbWFvIxPp9Pjz/+uGbPnq38/HyVl5frS1/6kt56660JfFexEYlz19bWpueee0533nmnZs6cqcLCQhUVFWnBggXauHGj+vv7gz7ua1/7mjIyMkJ+PfPMM5H6NqMmUtfetGnTRjwX77zzTtDHpfu19y//8i8jnjfn61//9V8DHpfM196bb76pxx57TLfddpumTp2qjIwM5eXljfv50qnPi9S5S9c+L5LXXrr1eZE6d+nW53V3d2vr1q36xje+oTlz5qi4uFiFhYWaO3eufvCDH6izs3PMz5nMfR5F4iLE4/GoqalJzc3Nqqqq0pIlS3TixAnt3btX5eXleu2119TY2BjWc507d07XXnutjh07punTp+vqq6/WkSNHdOTIEV188cV6/fXXNXny5IDHGGO0atUqbdmyRaWlpWpqatLZs2f16quvKi8vTzt37tSCBQui8a1PWKTO3Xe/+1398Ic/VGZmpubPn6+LL75Yn3zyifbs2aPe3l4tXrxY//u//6uCgoKAx33ta1/T5s2bdcMNN6iysnLY865evVrXX399xL7fSIvktTdt2jSdPHlSq1evDnr/o48+qqqqqoDbuPak3bt366c//WnQ+86fP6+tW7dKklpaWjR9+vTB+5L52rv11lv13//93wG35ebmyuPxjPm50q3Pi9S5S9c+L5LXXrr1eZE6d+nW5/30pz/VvffeK0maNWuWZs6cqfb2djU3N6ujo0MzZszQrl27VFFREdbzJX2fZxAR3/ve94wkc+2115qOjo7B2zds2GAkmaVLl4b9XF/96leNJHPbbbeZ/v7+wdvXrFljJJk/+7M/G/aYn/3sZ0aSueSSS8xHH300ePuWLVuMJNPY2BjwXIkkUufu0UcfNX/7t39rTp8+HXD70aNHTV1dnZFk/uZv/mbY41avXm0kmZ07d07o+4iXSF579fX1ZqzdAtfeyJ5++mkjySxatGjYfcl87T322GPm7//+783zzz9vPvroIyPJ5Obmjuu50q3Pi9S5S9c+L5LXXrr1eZE8d6GkYp+3efNmc//995ujR48G3H7mzBkzf/58I8n86Z/+adjPl+x9HuEhAvr6+kxpaamRZPbv3z/s/jlz5hhJZt++faM+14cffmgyMzNNdnZ2wMVhjDEej8eUl5ebSZMmDbtv5syZRpL5r//6r2HPefPNNxtJZsuWLWP7xmIgkuduJM8995yRZKZNmzbsvmTtzIyJ/Pkbzy9Srr2RXXfddUaSeeaZZ4bdl8zX3oXG+yYk3fq8YKLxBi5V+7xgYh0euPZGli59nqO5uXnwPPb29o56fCr0eax5iIDdu3erra1NjY2Nmj9//rD7b7/9dknS888/P+pzvfTSS/L5fFq6dKmmTJkScF9ubq5uuukmDQwM6KWXXhq8/fjx43rrrbeUn5+vG2+8cUKvH2uRPHcjmTt3riTpzJkzE3qeRBOr8xcK197Ijh8/rubmZuXk5GjVqlXjfp5Ulm59Xqykap8Xb1x7I0vHPs/5Wevt7dW5c+dGPT4V+rysmLxKijt48KAk6corrwx6v3O7c9xEn+vnP/95wHM5f7/iiiuUnZ09odePtUieu5G89957khR0jqXjV7/6lX75y19qYGBADQ0NuummmzRjxowJvW60Rev8/fjHP1ZLS4tyc3M1a9YsrVy5UuXl5SFfn2svuH/7t3+TJN14441yu90hj0vGay9S0q3Pi5VU7fOiJR36vFhIxz7P+VnLzs5WWVnZqMenQp9HeIiAU6dOSZJqamqC3u/c7hwX6eeK5OvHWqzavnHjRknSLbfcEvKYJ598MuDff/3Xf637779fGzduVFZWYv6oROv8/dVf/VXAvx966CE98cQT+sY3vhGT14+FWLT93//93yVJX/3qV0c8LhmvvUhJtz4vVlK1z4uWdOjzYiEd+zznZ+3zn/+8cnNzRz0+Ffo8pi1FgLNF14U7WjgKCwsDjov0c0Xy9WMtFm1/5plntG3bNpWWlurhhx8edv/8+fP1zDPP6OjRo+ru7tZ7772np556SqWlpXr66af1ne98Z9yvHW2RPn8333yzfvWrX+nkyZPq7u7W4cOH9a1vfUu9vb265557BnfQiNbrx1K027537169++67crvdQYeZpeS+9iIl3fq8WEjlPi/S0qnPi7Z07PNefPFF/exnP1N2drYeeeSRsB6TCn0e4SECzB92u83IyBjx/mg912iPSWSRPHfB7Nq1S2vXrlVGRoZ+/vOfq7q6etgxa9eu1Z//+Z/rkksuUX5+vhoaGvQXf/EXevXVV5WTk6Mnn3xS77///oTaES2RPn9PPPGEVq5cqbq6OuXn52vWrFnasGGDnn76aUn206GxvH4ii/a15wzf33HHHcrJyQl6TDJfe5GSbn1etKV6nxdp6dTnRVu69Xlvv/227rrrLhlj9OMf/3hw7cNoUqHPIzxEQFFRkSSpq6sr6P3d3d2SJJfLFZXnGu0xzu3hvH6sRfLcXejQoUO69dZb1dfXp40bN2rlypVjevwVV1yhm2++WQMDA9q2bduYXz8Wonn+hrrnnntUUVGho0eP6vjx42G/frpee16vV//xH/8hafTh+2CS4dqLlHTr86IpHfq8WEnFPi+a0q3PO336tD7/+c/r008/1be+9S2tXbs27MemQp9HeIiAuro6SQpZkda53Tku0s8VydePtWi1vaWlRTfccIPa2tq0fv16rVmzZlztu+SSSyRJH3744bgeH22x+r/PzMwcLJY29Fxw7QX38ssv6+OPP9b06dN13XXXjat9iX7tRUq69XnRki59XqykYp8XTenU5509e1YrVqzQqVOn9PWvf13/9E//NKbHp0KfR3iIAGeoav/+/UHvd26fM2dOVJ7Leczhw4fV398/odePtUieO8eZM2e0YsUKffTRR1q7dq3WrVs37vZ9+umnkhL3U6RonL9Qgp0Lrr3gnOH7u+66a5ytS/xrL1LSrc+LhnTq82Ip1fq8aEqXPq+jo0N//Md/rHfeeUe33Xabnn322TFPJUqJPi8m1SRSXG9vrykpKRm12NTevXtHfa4zZ86YzMxMk5OTY37/+98H3OcUD8nMzDQffvhhwH2XX375qMVD/vM//3Ns31gMRPLcGWNMa2urueKKK4wk8/Wvf934fL5xt83j8Zja2lojyezevXvczxNNkT5/oRw+fNhkZGSYgoKCYUVwuPYCdXR0mIKCAiNpWDXScCXDtXchjbPYVLr1ecGM99wZk359XjATOX+hpGKfF0wkzl269Hkej8dcf/31RpK54YYbwioIF0wq9HmEhwj5u7/7OyPJXHfddaazs3Pw9g0bNhhJZvHixQHHP/nkk+ayyy4zDz/88LDn+spXvmIkmS9+8YsBpca/+c1vGknmrrvuGvaYZ599drBs+dCL8Ze//KWRZBoaGkxfX18kvtWIi9S56+rqMgsXLjSSzKpVq4zX6x31td955x2zdevWYcd+/PHH5tZbbzWSzNy5cyf0CznaInX+/ud//idoNeWDBw8Odlrf/OY3h93PtRdo8+bNRpJZuHDhiK+dCtfeUKO9CaHPC2285y5d+7wLjff8pWOfd6GJ/Nw60qHP83q9ZuXKlUaSWbJkienq6hr1Manc5xEeIqSnp8csWLDASDJVVVVm1apVg/+ePHmyOXbsWMDx69atM5LM6tWrhz3XJ598YhobG40k09jYaO64447BT5YaGxvNJ598MuwxAwMDgxe22+02t99+u1m2bJnJyMgweXl5Zs+ePdH61icsUufuwQcfNJLMpEmTzJ133mlWr14d9GuonTt3Dr7OokWLzKpVq8yyZctMUVGRkWRqamrMu+++G+UzMDGROn/O7fX19Wb58uXmjjvuMNdcc43JysoyksxnPvOZoB0m116gFStWGEnmqaeeGvG1k/3ae+GFF8yCBQsGvySZjIyMgNteeOGFwePp8/wide7Stc+L1PlLxz4vkj+3jnTo8x5//HEjyUgyK1euDPmzNrSvSuU+j/AQQd3d3eZ73/ueaWxsNDk5OWbKlClm9erV5tSpU8OOHe0HsrW11axZs8bU1taanJwcU1tbax544AFz7ty5kK/v9XrNhg0bzKxZs0xeXp6ZPHmyue2228zhw4cj9S1GTSTO3erVqwd/uEf6GuqDDz4wDz74oFm4cKGprKw02dnZxuVymSuvvNKsW7fOtLa2RvPbjphInL/m5mZz9913m9mzZ5vJkyebrKwsU1ZWZpYtW2aeffbZET/VTPdrz3HmzBkzadIkk52dbc6ePTvi6yb7tbdp06ZRf9Y2bdo0eDx9nl+kzl269nmROn/p2OdF+uc2Xfo85zyM9nX8+PFhj0nFPi/DmAluZg4AAAAgLbDbEgAAAICwEB4AAAAAhIXwAAAAACAshAcAAAAAYSE8AAAAAAgL4QEAAABAWAgPAAAAAMJCeAAAAAAQFsIDAAAAgLAQHgAAAACEhfAAAAAAICyEBwBATG3evFkZGRmaN2+evF5v0GP27NmjzMxMVVZW6tNPP41xCwEAoRAeAAAxtXr1an32s5/VwYMHtWHDhmH39/X16d5775UxRhs3bpTb7Y5DKwEAwWQYY0y8GwEASC8tLS2aPXu2JOm3v/2tGhsbB+9bv369vv/97+vGG2/UCy+8EK8mAgCCIDwAAOLiRz/6kR5++GF99rOf1SuvvCJJeuuttzR//nzl5OToyJEjqquri3MrAQBDMW0JABAX3/72tzV37lxt27ZNmzdvljFG9957r/r6+vTDH/6Q4AAACYiRBwBA3Ozbt08LFiyQ2+3WmjVrtH79el1zzTV67bXXlJnJ51sAkGgIDwCAuHrooYf0+OOPS5KysrL05ptvas6cOfFtFAAgKD7WAQDE1UMPPTT49/vuu4/gAAAJjPAAAIir9evXD/79xRdfVHd3d/waAwAYEeEBABA3O3fu1KZNm1RTU6ObbrpJJ06cCAgTAIDEwpoHAEBceDwezZkzR8eOHdPWrVt1zTXX6PLLL1dXV5f27dunuXPnxruJAIALMPIAAIiLH/zgBzp27JhWrlypW265RVVVVXrsscfk9Xp13333yefzxbuJAIALMPIAAIi5w4cP68orr1R+fr7eeustTZ06VZJkjNHixYvV3NysJ554QmvWrIlzSwEAQxEeAAAx5fP5tGjRIr3++uv653/+Z/3lX/5lwP1HjhzR/PnzlZeXp7fffnswWAAA4o9pSwCAmHrqqaf0+uuva+HChbr//vuH3T9r1ix95zvfUUdHByMPAJBgGHkAAMTM6dOnNXPmTPX09Gj//v2aPXt20OM8Ho9mz56t3/3ud9q6datuueWWGLcUABAM4QEAAABAWJi2BAAAACAshAcAAAAAYSE8AAAAAAgL4QEAAABAWAgPAAAAAMJCeAAAAAAQFsIDAAAAgLAQHgAAAACEhfAAAAAAICyEBwAAAABhITwAAAAACAvhAQAAAEBYCA8AAAAAwkJ4AAAAABAWwgMAAACAsBAeAAAAAISF8AAAAAAgLIQHAAAAAGH5/2JhRaE9KYzOAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 900x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Bayesian Linear Regression Visualization without Explicit mu_theta and Sigma_theta Calculation\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "n_samples = 20  # Number of training samples n\n",
    "m_predicts = 100 # Number of prediction input m\n",
    "X = (2 * np.random.rand(n_samples, 1)).T  # Training features: 1*n\n",
    "X = np.vstack((np.ones(n_samples),X)) # Training features added intercept handling: 2*n\n",
    "true_theta = np.array([[1.0,2.5]]).T  # True parameter theta [intercept, slope]: 2*1\n",
    "sigma_model = 1.0  # Noise in the original data generation process (σ)\n",
    "#sigma_pred = 0.5  # Prediction model noise (σ*)\n",
    "\n",
    "# Generate noisy observations\n",
    "Y = true_theta.T @ X + np.random.normal(0, sigma_model, (1, n_samples)) # Training Label: 1*n\n",
    "\n",
    "# Regularization strength λ\n",
    "lambda_reg = 1.0  \n",
    "\n",
    "# Posterior Mean & Covariance Matrix\n",
    "inv_term = np.linalg.inv(X @ X.T + lambda_reg * np.eye(X.shape[0])) # Define (X X^T + λI)⁻¹ for convinience: d*d\n",
    "cov_theta = sigma_model**2 * inv_term # Posterior Covariance: d*d\n",
    "mu_theta = cov_theta @ X @ Y.T # Posterior Mean: d*1\n",
    "\n",
    "# Define predictive input\n",
    "X_hat = np.linspace(0, 2, m_predicts).T  # 1*m\n",
    "X_hat = np.vstack([np.ones(m_predicts), X_hat])  # Add intercept handling: 2*m\n",
    "\n",
    "# Calculate Predictieve Mean & Cov\n",
    "mu_hat = mu_theta.T @ X_hat # 1*m\n",
    "cov_hat = sigma_model ** 2 * (np.eye(m_predicts) + X_hat.T @ cov_theta @ X_hat) # m*m\n",
    "\n",
    "# Compute standard deviation for confidence intervals\n",
    "std_Y_star = np.sqrt(np.diag(cov_hat))\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(6, 4),dpi=150)\n",
    "plt.scatter(X[1], Y, color=\"blue\", label=\"Training Data\", alpha=0.6)\n",
    "plt.plot(X_hat[1], mu_hat.flatten(), \"r-\", label=\"Bayesian Prediction\")\n",
    "plt.fill_between(\n",
    "    X_hat[1],\n",
    "    mu_hat.flatten() - 2 * std_Y_star,\n",
    "    mu_hat.flatten() + 2 * std_Y_star,\n",
    "    color=\"red\",\n",
    "    alpha=0.2,\n",
    "    label=\"Confidence Interval (±2 std)\",\n",
    ")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.title(\"Bayesian Linear Regression: Predictive Distribution\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.Kernel Methods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
