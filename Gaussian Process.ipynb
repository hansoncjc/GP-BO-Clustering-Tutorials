{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Process Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to introduce **Gaussian Process Regression**. The notebook will introduce in the order of statistical explanation, implementations, and hyperparameter tuning. Many contents of this notebook will based on the prerequisite concepts explained in the *fundamental.ipynb* file.\n",
    "- **Mathematic Derivation & Definition**\n",
    "    1. Definition of Gaussian Process (GP)\n",
    "    2. Definition of Gaussian Process Regression (GPR)\n",
    "    3. Pros/Cons of GPR\n",
    "    4. Origination of GPR\n",
    "    5. Derivation of GPR\n",
    "    6. Kernel Function in GPR\n",
    "    7. Sizes of terms\n",
    "    8. Simple Illustration of GPR\n",
    "    9. Testing Size and Smoothness of Output\n",
    "    10. Common Confusions about GPR\n",
    "- **Gaussian Process Regression Implementation Codes**\n",
    "    1. GP From Scratch Code\n",
    "    2. GP interactive visualization\n",
    "    3. The role of hyperparameters\n",
    "    4. GP Using Libraries\n",
    "- **Gaussian Process Hyperparameter Tuning**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Mathematic Derivation & Definition**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.Definition of Gaussian Process(GP)\n",
    "A **Gaussian Process (GP)** is a stochastic process that defines a distribution over **functions** rather than just over points. Formally, a GP is a collection of **random variables** where any finite subset follows a **multivariate Gaussian distribution**. A gaussian process is represented as:\n",
    "$$f\\sim \\mathcal{GP}(\\mu(x),k(x,x'))$$\n",
    "- $f$ is the function that follows the GP (which is also our target when using GP to make predictions)\n",
    "- A gaussian process, like gaussian distribution, are defined by only two terms: **mean function** $\\mu(x)$ and **kernel (covariance) function** $k(x,x')$.\n",
    "- The mean function $\\mu(x)$ represents the **Expected Value** of the function $f$ given any input $x$.\n",
    "- The **kernel (covariance) function** $k(x,x')$ captures how function values at different inputs $x$ and $x'$ are related.- GP assumes that function values near each other are correlated in a way dictated by the covariance function (key for generating smooth prediction).\n",
    "\n",
    "The terminologies seem intimidating, but a gaussian process can be easily understood by the plot below:\n",
    "<div style=\"text-align: center\"> <img src=\"https://www.lancaster.ac.uk/stor-i-student-sites/thomas-newman/wp-content/uploads/sites/37/2022/05/Gaussian-process-with-noise.svg\" alt=\"Drawing\" width=\"500\"/> </div>\n",
    "\n",
    "The plot shown here involves training datas ($\\times$), some functions, and a shade area. In a gaussian distribution of a random variable (explained in the *fundamental* notebook), we model the distribution of datapoints. Here in GP, we model **the distribution of functions that go through (or nearby, controlled by a hyperparameter explained later) training points**. The shaded area represents the distribution of the **function**, and the functions shown above are some subsets of the distribution. The **bold** function shown in the plot represents the value of **mean function $\\mu(x)$**, the most possible function value. The shaded area is dictated by the **kernel(covariance) function**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.Definition of Gaussian Process Regression(GPR)\n",
    "Gaussian Process Regression (GPR) is a Bayesian, nonparametric regression method in which we model an unknown function $f(x)$ as being **drawn from a Gaussian process**. \n",
    "Note that gaussian process is **NOT** a machine learning model for prediction. It only describes **Distribution of a function**. Gaussian process can be used to peform multiple tasks, including regression and classification. Here, we focus on **Gaussian Process Regression**, a supervised machine learning model that is cabable of predicting function regardless of its form using gaussian process while also providing uncertainty of the prediction. \n",
    "\n",
    "Recall that a regression is a model the relationship between input variables (features $X$) and a **continuous** output variable (Label $Y$):\n",
    "$$Y=f(X)+\\epsilon,\\,\\epsilon\\sim\\mathcal{N}(0,\\sigma^2)$$\n",
    "\n",
    "Recall that in the case where the function has a fixed form (linear regression), we use true bayesian prediction to obtain a predictive distribution of label $\\hat{Y}$:\n",
    "$$P(\\hat{Y}|X,Y,\\hat{X})=\\int_{\\theta} P(\\hat{Y}|\\theta,X,Y)P(\\theta|X,Y)d\\theta$$\n",
    "The predictive distribution involves a prior $P(\\theta)$ over parameter $\\theta$, which we assumed to be gaussian in the previous example:\n",
    "$$P(\\theta)\\sim \\mathcal{N}(0,\\tau^2I)$$\n",
    "\n",
    "**In GPR**, the goal is to predict the function $f$ directly, regardless of its form. Thus, assuming a prior over parameter $\\theta$ is **unrealistic**, since the form of $\\theta$ is unknown. Instead, we put a **GP prior** over the function which we try to predict. (you can think of that before we have an assumption of $\\theta$, now we have an assumption of $f$. Since the distribution of a function is a gaussian process, we use notation $\\mathcal{GP}$ instead of $P$.)\n",
    "$$ \\begin{align}\n",
    "P(\\hat{Y}|X,Y,\\hat{X})&=\\int_{f} P(\\hat{Y}|f)P(f|X,Y)df \\\\\n",
    "f&\\sim \\mathcal{GP}(\\mu,k)\n",
    "\\end{align}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Pros/Cons of GPR\n",
    "##### Advantages of GPR\n",
    "1. GPR is very powerful in predicting functions in **any form**. Unlike other models that stick with one particular function form (like linear regression for linear model), GP learns the function from the data.\n",
    "2. GPR uses **Bayesian Approach**, incorporating prior knowledge and allow for principled Bayesian inference, making them adaptable to different domains (for reference MAP/True bayesian prediction in fundamental notebook).\n",
    "3. GPR is a probabilistic model, whose output not only provides the most probable value (mean function), but also **uncertainty** of the value.\n",
    "4. GPR does **not require a large dataset**, which is very friendly to the situation where data collection is time-consuming and expensive.\n",
    "5. The choice of **kernel function** allows GPs to capture various data patterns, such as periodicity, smoothness, or sudden changes.\n",
    "##### Disadvantages of GPR\n",
    "1. GPR is computationally **expensive and complex**. This will be explained in later parts.\n",
    "2. GPR does not work ideally with **large datasets ($n>10^6$)**, primarily because its complexity.\n",
    "3. GPR does not work ideally with **high dimensional settings**.\n",
    "4. Choosing kernel functions could be tricky, and it dictates the overall performance of GP.\n",
    "5. Hyperparameter tuning is challenging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Origination of GPR\n",
    "The origination of GPR as a powerful model to predict any arbitraty shape function $f$  with uncertainty comes from several aspects.\n",
    "##### Bayesian Approach\n",
    "As introduced and explained in the *fundamental* notebook, **bayesian approach** is very powerful at making predictions of model parameters (MLE and MAP). In addition, a **true bayesian prediction** provides uncertainty on top of the prediction itself, making it very useful in many usecase where uncertainty is necessary to obtain. Therefore, gaussian process natually emerges as a model using the bayesian approach.\n",
    "##### Universality of Gaussian Distribution\n",
    "Bayesian Approach relies on previous assumptions/knowledge of the distribution (**Prior**) to infer the posterior. This means choosing a **reliable** prior is critical. As shown in the *fundamental* notebook, a **gaussian prior** is the popular choice, due to the universality of gaussian process, and most importantly the **central limit theorem(CLT)**. This property also extends to **gaussian process prior**.\n",
    "\n",
    "In addition, **operations** among gaussian distributions (addition, multiplication, integration, etc) result in **another gaussian** in most cases. \n",
    "\n",
    "These two reasons make choosing a gaussian process prior reasonable. Therefore, a GPR places a gaussian process prior in the model (why it named \"gaussian\" process).\n",
    "##### Closed Form of Predictive Distribution\n",
    "In true bayesian prediction, the **Predictive Distribution** is calculated by integrating over all parameter $\\theta$ in the case of fixed function form regression; and integrating over all function $f$ inthe case of gaussian process regression:\n",
    "$$\\begin{align}\n",
    "P(\\hat{Y}|X,Y,\\hat{X})&=\\int_{\\theta} P(\\hat{Y}|\\theta,X,Y)P(\\theta|X,Y)d\\theta \\\\\n",
    "P(\\hat{Y}|X,Y,\\hat{X})&=\\int_{f} P(\\hat{Y}|f)P(f|X,Y)df\n",
    "\\end{align}$$\n",
    "The indefinite integral is **computationally complex**, and rarely has a **closed form**. In the case of gaussian distribution, a **closed form is available**. This is the key why a GPR can leverage true bayesian prediction as its approach to peform modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.Derivation of GPR\n",
    "Recall that for a regression problem, our goal is that given training feature $X$ and training label$Y$, we want to find the unknown function that maps $X$ to $Y$:\n",
    "$$Y=f(X)+\\epsilon, \\,\\epsilon\\sim\\mathcal{N}(0,\\sigma^2)$$\n",
    "As Stated above, GPR uses bayesian approach to infer the predictive distribution of function $f(x)$:\n",
    "$$P(\\hat{Y}|X,Y,\\hat{X})=\\int_{f} P(\\hat{Y}|f)P(f|X,Y)df$$\n",
    "\n",
    "##### Gaussian Conditioning Formula\n",
    "In the derivation of the predictive distribution, the **Gaussian Conditioning Formula** is frequently used to bypass the cpmlex explicit calculation (specifically the integral).\n",
    "\n",
    "For two multivariate gaussian random Variable $A$ and $B$:\n",
    "$$\\begin{align}\n",
    "A&\\sim\\mathcal{N}(\\mu_A,\\Sigma_{AA})\n",
    "B&\\sim\\mathcal{N}(\\mu_B,\\Sigma_AA)\n",
    "\\end{align}$$\n",
    "The joint distribution between $A$ and $B$ is:\n",
    "$$\\begin{bmatrix} A \\\\ B  \\end{bmatrix}\\sim\\mathcal{N}\n",
    "\\begin{pmatrix}\n",
    "\\begin{bmatrix} \\mu_A \\\\ \\mu_B \\\\ \\end{bmatrix} &&\n",
    "\\begin{bmatrix} \\Sigma_{AA} && \\Sigma_{AB} \\\\ \\Sigma_{BA} && \\Sigma_{BB} \\end{bmatrix}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "And the conditional distribution $P(B|A)$ is:\n",
    "$$P(B|A)\\sim\\mathcal{N}(mu_B+\\Sigma_{BA}\\Sigma_{AA}^{-1}(A-\\mu_A),\n",
    "        \\Sigma_{BB}-\\Sigma_{BA}\\Sigma_{AA}^{-1}\\Sigma_{AB})$$\n",
    "##### What is the posterior $P(f|X,Y)$\n",
    "This step is very similar to the posterior function in bayesian linear regression, we use bayes's theorem to infer:\n",
    "$$P(f|X,Y)\\propto P(Y|f)P(f|X)$$\n",
    "The only difference is that here the prior is a **GP Prior** over $f$:\n",
    "$$P(f|X)\\sim\\mathcal{GP}(m(X),K(X,X))$$\n",
    "where\n",
    "- $m(X)$ is the mean function evaluated at $X$, and <span style=\"color: red;\">It is assumed that $m(X)=0$ in the gaussia prior.</span>\n",
    "- $K(X,X) is the **Covariance Matrix**, where entry $K_{ij}=k(X_i,X_j)$, $k(x,x')$ being the chosen kernel function.\n",
    "\n",
    "$P(Y|f)$ is just the function value with noise:\n",
    "$$P(Y|f)\\sim\\mathcal{GP}(m(X),K(X,X)+\\sigma^2I)\n",
    "$$\n",
    "\n",
    "The **joint distritbuion** of $f(X)$ and $Y$ is:\n",
    "$$\\begin{bmatrix} f(X) \\\\ Y  \\end{bmatrix}\\sim\\mathcal{N}\n",
    "\\begin{pmatrix}\n",
    "\\begin{bmatrix} m(X) \\\\ m(X) \\\\ \\end{bmatrix} &&\n",
    "\\begin{bmatrix} K(X,X) && K(X,X) \\\\ K(X,X) && K(X,X)+\\sigma^2I \\end{bmatrix}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Using the **Gaussian Conditioning Formula**,\n",
    "$$\\begin{align}\n",
    "P(f|X,Y)&=P(f(X)|Y)\\sim\\mathcal{N}(\\mu_f,\\Sigma_f) \\notag \\\\\n",
    "\\mu_f &= K(X,X)[K(X,X)+\\sigma^2I]^{-1}(Y-m(X)) \\notag \\\\\n",
    "&=K(X,X)[K(X,X)+\\sigma^2I]^{-1}Y \\\\\n",
    "\\Sigma_f &= K(X,X)-K(X,X)[K(X,X)+\\sigma^2I]^{-1}K(X,X)\n",
    "\\end{align}$$\n",
    "\n",
    "##### What is $P(\\hat Y|f)$\n",
    "The model prediction is just the function output:\n",
    "$$P(\\hat Y|f)\\sim\\mathcal{N}(m(\\hat X),K(\\hat X,\\hat X))$$\n",
    "\n",
    "##### Predictive Distribution\n",
    "To derive the predictive distribution, we don't have to explicitly calculate the integral, because the result of the integral is analytically equal to another gaussian conditioning (both terms under integration are gaussian). The joint distribution of $Y$ and $f(\\hat X)$ is:\n",
    "$$\\begin{bmatrix} Y \\\\ f(\\hat X) \\end{bmatrix}\\sim\\mathcal{N}\n",
    "\\begin{pmatrix}\n",
    "\\begin{bmatrix} m(X) \\\\ m(\\hat X) \\\\ \\end{bmatrix} &&\n",
    "\\begin{bmatrix} K(X,X)+\\sigma^2I && K(\\hat X ,X) \\\\ K (\\hat X,X) && K(\\hat X,\\hat X) \\end{bmatrix}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "Using the **Gaussian Conditioning Formula**,\n",
    "$$\\begin{align}\n",
    "P(\\hat Y|X,Y,\\hat X)&=P(f(\\hat X)|Y)\\sim\\mathcal{N}(\\mu_*,\\Sigma_*) \\notag \\\\\n",
    "\\mu_* &= K_*^T[K+\\sigma^2I]^{-1}Y \\notag \\\\\n",
    "\\Sigma_* &= K_{**}-K_*^T[K+\\sigma^2I]^{-1}K_*\n",
    "\\end{align}$$\n",
    "Couple Notes:\n",
    "- $K=K(X,X)$ is the **Training Kernel Matrix**\n",
    "- $K_*=K(X,\\hat X)$ is the **Training& Testing Covariance Matrix**\n",
    "- $K_{**}=K(\\hat X,\\hat X)$ is the **Testing Kernel Matrix**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.Kernel Function in GPR\n",
    "A kernel function (also called a covariance function) defines the similarity between data points in Gaussian Process Regression. It specifies how much influence one point has on another, determining the structure of the function space that GPR models. It is extremely important in **GPR** because it controls **smoothness, periodicity, and variability** of the function learned.With that said, choosing an appropriate kernel function is critical.\n",
    "\n",
    "Before we dive in to determine the form of kernel function, we need to identify the requirements on the kernel function. We recognize that for kernel matrix $K$, where $K_ij=k(X,X')$:\n",
    "1. The Diagonal of the matrix must represent the variance of the distribution. This is because we need to satisfy gaussian prior:\n",
    "$$K_{ii}=k(X_i,X_i)=Var(X_i)$$\n",
    "2. $K$ is always **Positive semi-definite**, because variance/covariance **cannot be negative**.\n",
    "3. If $X_i$ and $X_j$ are very **independent**, i.e.$X_i$ and $X_j$ are very different from each other, $K(X_i,X_j)=0$.\n",
    "4. If $X_i$ and $X_j$ are very **similar**, $K(X_i,X_j)>0$.\n",
    "\n",
    "##### Radial Basis Function (RBF) Kernel\n",
    "A very popular kernel function that satisfies all the requirements mentioned above is the **RBF** kernel, which defines similarity with eucleadian distance:\n",
    "$$K(X_i,X_j)=\\tau \\exp \\frac{-||X_i-X_j||^2}{2l^2}$$\n",
    "where\n",
    "- $tau$ is the prefactor (coefficient)of the model. Sometimes it is also called $\\sigma^2$, but may cause confusion because there is another $\\sigma^2$ in the model which is the observation noise variance.\n",
    "- $l$ is the lengthscale. Controls how far the influence of each data point extends. A larger $l$ means smoother functions, while a smaller $l$ allows rapid changes.\n",
    "- $||X_i-X_j||^2$ is the eucleadian distance between the two feature vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.Sizes of terms\n",
    "There are a lot of matrices involved in a Gaussian Process Regression. It is beneficial to sort out all of their sizes to ensure that the computation and results are truly what we want. Here is a summarized table for all the matrices and their sizes in GPR:\n",
    "<div align=\"center\">\n",
    "\n",
    "| Matrix         | Description                           | Size   |\n",
    "|---------------|-------------------------------------|--------|\n",
    "| $X$             | Training input matrix               | $d × n$  |\n",
    "| $Y$             | Training output vector             | $1 × n$  |\n",
    "| $\\hat{X}$    | Prediction input matrix                  | $d × m$  |\n",
    "| $\\hat{Y}$    | Prediction output vector                 | $1 × m$  |\n",
    "| $K$            | Training Kernel Matrix                     | $n × n$  |\n",
    "| $K_*$             | Training&Testing Covariance Matrix    | $n × m$ |\n",
    "| $K_*^T$             | Transpose of Training&Testing Covariance Matrix    | $m × n$ |\n",
    "| $K_{**}$             | Testing Kernel Matrix                   |$m × m$|\n",
    "| $\\mu_*$          | Predictive Mean               | $ 1× m $  |\n",
    "| $\\Sigma_*$          | Predictive covariance              | $m × m$  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.Simple Illustration of GPR\n",
    "Understanding all those abstract math representations of a bunch of matrices could be challenging. Here is a very simplified example of only 2 training points, and we attempt to make only one prediction.\n",
    "##### Set up\n",
    "In our simplified set-up, We have only **two training points** on a plane:\n",
    "$$\n",
    "X=\\begin{bmatrix} 1.5, 4  \\end{bmatrix},\\,\n",
    "Y=\\begin{bmatrix} 2, 5  \\end{bmatrix}\n",
    "$$\n",
    "And we want to make **1 prediction(testing point) at $\\hat X=2$**.\n",
    "<div style=\"text-align: center\"> <img src=\"images/Simp_1.png\" alt=\"Drawing\" width=\"250\"/> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To comprehend how GPR is performed without any abstract math, we summarize it as a question: Given the **known** datapoints; the **trend** among known datapoints;,and the **similarity** between testing point and datapoints, what is the most likely testing output, and how certain we are about it?\n",
    "\n",
    "Suppose we have two prediction candidates: $[2,2.2]$ and $[2,4.5]$. It is obvious that $[2,2.2]$ is more probable, because \n",
    "1. The slope between two training points is positive.\n",
    "2. Testing input $\\hat X=2$ is close to the training point$X=1.5$, so its corresponding label should be closer to that of $X=1.5$.\n",
    "\n",
    " The image below illustrates this approach:\n",
    "<div style=\"text-align: center\"> <img src=\"images/Simp_2.png\" alt=\"Drawing\" width=\"250\"/> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we understand the priciples behind a gaussian process, let's go ahead and define all the covariance matrices in addition to $X$, $Y$, and $\\hat X$:\n",
    "$$\\begin{align}\n",
    "K&=\\begin{bmatrix}\n",
    "\\tau e^{\\frac{-(X_1-X_1) ^2}{2l^2}} & \\tau e^{\\frac{-(X_1-X_2) ^2}{2l^2}} \\\\\n",
    "\\tau e^{\\frac{-(X_2-X_1)^2}{2l^2}} & \\tau e^{\\frac{-(X_2-X_2)^2}{2l^2}} \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "K_{*}&=\\begin{bmatrix}\n",
    "\\tau e^{\\frac{-(X_1-\\hat X_1)^2}{2l^2}}  \\\\\n",
    "\\tau e^{\\frac{-(X_2-\\hat X_1)^2}{2l^2}}  \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "K_{**}&=\\begin{bmatrix}\n",
    "\\tau e^{\\frac{-(\\hat X_{1}-\\hat X_{1}) ^2}{2l^2}}\n",
    "\\end{bmatrix}\n",
    "\\end{align}$$\n",
    "\n",
    "Then we proceed to calculate predictive distribution mean and covariance matrix, applying:\n",
    "$$\\begin{align}\n",
    "\\mu_* &= K_*^T[K+\\sigma^2I]^{-1}Y \\notag \\\\\n",
    "\\Sigma_* &= K_{**}-K_*^T[K+\\sigma^2I]^{-1}K_* \\\\\n",
    "&\\text{assuming:} \\sigma^2=1\n",
    "\\end{align}$$\n",
    "\n",
    "The result is:\n",
    "$$\\begin{align}\n",
    "\\mu_{*}=[2.20],\\,\\Sigma_{*}=[0.0227] \\notag \\\\\n",
    "STD=Diag(\\Sigma_*)=[0.0227]\n",
    "\\end{align}$$\n",
    "<div style=\"text-align: center\"> <img src=\"images/Simp_3.png\" alt=\"Drawing\" width=\"600\"/> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At last, it is always safe to make sure that the matrix sizes make sense.\n",
    "<div align=\"center\">\n",
    "\n",
    "| Terms        | Description                           | Size   |\n",
    "|---------------|-------------------------------------|--------|\n",
    "| $d$             | Feature Dimension               | $1$  |\n",
    "| $n$             | Number of Training Points               | $2$  |\n",
    "| $m$             | Number of Testing Points               | $1$  |\n",
    "| $X$             | Training input matrix               | $d × n = 1 × 2$  |\n",
    "| $Y$             | Training output vector             | $1 × n = 1 × 1$  |\n",
    "| $\\hat{X}$    | Prediction input matrix                  | $d × m = 1 × 1$  |\n",
    "| $\\hat{Y}$    | Prediction output vector                 | $1 × m = 1 × 1$  |\n",
    "| $K$            | Training Kernel Matrix                     | $n × n = 2 × 2$  |\n",
    "| $K_*$             | Training&Testing Covariance Matrix    | $n × m = 2 × 1$ |\n",
    "| $K_*^T$             | Transpose of Training&Testing Covariance Matrix    | $m × n = 1 × 2$ |\n",
    "| $K_{**}$             | Testing Kernel Matrix                   |$m × m = 1 × 1$|\n",
    "| $\\mu_*$          | Predictive Mean               | $ 1× m  = 1 × 1$  |\n",
    "| $\\Sigma_*$          | Predictive covariance              | $m × m = 1 × 1$  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Testing Size and Smoothness of Output\n",
    "Testing Size is closely related to the smoothness of the output function. Let's say we have 100 testing points, evenly distributed between 0 and 5, in the previous simple example:\n",
    "$$\\begin{align}\n",
    "X&=\\begin{bmatrix} 1.5, 4  \\end{bmatrix},\\,\n",
    "Y=\\begin{bmatrix} 2, 5  \\end{bmatrix} \\notag \\\\\n",
    "\\hat X &= \\begin{bmatrix} 0, 0.05, 0.10, 0.15, \\cdots, 100 \\end{bmatrix}\n",
    "\\end{align}$$\n",
    "\n",
    "Our covariance matrices will become:\n",
    "$$\\begin{align}\n",
    "K&=\\begin{bmatrix}\n",
    "\\tau e^{\\frac{-(X_1-X_1) ^2}{2l^2}} & \\tau e^{\\frac{-(X_1-X_2) ^2}{2l^2}} \\\\\n",
    "\\tau e^{\\frac{-(X_2-X_1)^2}{2l^2}} & \\tau e^{\\frac{-(X_2-X_2)^2}{2l^2}} \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "K_{*}&=\\begin{bmatrix}\n",
    "\\tau e^{\\frac{-(X_1-\\hat X_{1}) ^2}{2l^2}}  & \n",
    "\\tau e^{\\frac{-(X_1-\\hat X_{2}) ^2}{2l^2}} & \\cdots & \\tau e^{\\frac{-(X_1-\\hat X_{100}) ^2}{2l^2}} \\\\\n",
    "\\tau e^{\\frac{-(X_2-\\hat X_{1}) ^2}{2l^2}}  & \n",
    "\\tau e^{\\frac{-(X_2-\\hat X_{2}) ^2}{2l^2}} & \\cdots & \\tau e^{\\frac{-(X_2-\\hat X_{100}) ^2}{2l^2}} \n",
    "\\end{bmatrix} \\\\\n",
    "K_{**}&=\\begin{bmatrix}\n",
    "\\tau e^{\\frac{-(\\hat X_1-\\hat X_1) ^2}{2l^2}}  & \n",
    "\\tau e^{\\frac{-(\\hat X_1-\\hat X_2) ^2}{2l^2}} & \\cdots & \\tau e^{\\frac{-(\\hat X_1-\\hat X_{100}) ^2}{2l^2}} \\\\\n",
    "\\tau e^{\\frac{-(\\hat X_2-\\hat X_1) ^2}{2l^2}}  & \n",
    "\\tau e^{\\frac{-(\\hat X_2-\\hat X_2) ^2}{2l^2}} & \\cdots & \\tau e^{\\frac{-(\\hat X_2-\\hat X_{100}) ^2}{2l^2}} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\tau e^{\\frac{-(\\hat X_{100}-\\hat X_1) ^2}{2l^2}}  & \n",
    "\\tau e^{\\frac{-(\\hat X_{100}-\\hat X_2) ^2}{2l^2}} & \\cdots & \\tau e^{\\frac{-(\\hat X_{100}-\\hat X_{100}) ^2}{2l^2}}\\end{bmatrix}\n",
    "\\end{align}$$\n",
    "\n",
    "The resulting plot will become:\n",
    "<div style=\"text-align: center\"> <img src=\"images/Simp_4.png\" alt=\"Drawing\" width=\"600\"/> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we further expand the testing size to 1000, the output funciton will be smoother:\n",
    "<div style=\"text-align: center\"> <img src=\"images/Simp_5.png\" alt=\"Drawing\" width=\"600\"/> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.Common Confusions about GPR\n",
    "##### Is \"Tesing Data\" the same as \"Testing\" in \"Train/Test Split\"?\n",
    "**NO**, the testing data in a GPR is not the same as the term in train/test split. In train/test split, the testing data is a part of the avaible data that is preserved and **not** used in the process of model training. The purporse of testing data in train/test split is to have extra observed data to validate the model.\n",
    "\n",
    "In **GPR**, testing data is a chosen set of interested inputs to obtain the predicted label from the GPR model. The confusion could be avoided if we use \"predictive input\" in stead of \"testing data\" here, but note that \"testing data\" is still occasionally used.\n",
    "\n",
    "##### Does smoothness of the output depend on testing data size?\n",
    "This is a common confusion between \"the model\" and \"inquiry of the model\". In GPR, the testing data (predictive input) is just a way for us to inquire a subset of the model. Just like in a sine model $Y=sin(X)$, if we inquire the model with a large interval (for example we input $X=[0.5,1,1.5,2]$), the output function will be squiggling(if we connect all the points with straight line). However, that does not mean that the model is not smooth. \n",
    "\n",
    "It is similar in GPR: when we inquire the predictive label value from a GPR model, the denser the points we include in testing data $\\hat X$ within a certain interval, the smoother of a function we get (see exmaple above). **But** it does not mean the model is not smooth.\n",
    "\n",
    "In summary, the density of testing data only increase the degree we \"probe\" the model. It is not related to the peformance of the model itself.\n",
    "\n",
    "##### How to understand that GPR is a \"Non parametric\" model yet it has \"infinite parameters\"?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
