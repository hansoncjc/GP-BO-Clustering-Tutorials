{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Process Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to introduce **Gaussian Process Regression**. The notebook will introduce in the order of statistical explanation, implementations, and hyperparameter tuning. Many contents of this notebook will based on the prerequisite concepts explained in the *fundamental.ipynb* file.\n",
    "- **Mathematic Derivation & Definition**\n",
    "    1. Definition of Gaussian Process (GP)\n",
    "    2. Definition of Gaussian Process Regression (GPR)\n",
    "    3. Pros/Cons of GPR\n",
    "    4. Origination of GPR\n",
    "    5. Derivation of GPR\n",
    "    6. Kernel Function in GPR\n",
    "    7. Sizes of terms\n",
    "    8. Simple Illustration of GPR\n",
    "- **Gaussian Process Regression Visualization**\n",
    "    1. GP From Scratch Code\n",
    "    2. GP interactive visualization\n",
    "    3. The role of hyperparameters\n",
    "    4. GP Using Libraries\n",
    "- **Gaussian Process Hyperparameter Tuning**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Mathematic Derivation & Definition**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.Definition of Gaussian Process(GP)\n",
    "A **Gaussian Process (GP)** is a stochastic process that defines a distribution over **functions** rather than just over points. Formally, a GP is a collection of **random variables** where any finite subset follows a **multivariate Gaussian distribution**. A gaussian process is represented as:\n",
    "$$f\\sim \\mathcal{GP}(\\mu(x),k(x,x'))$$\n",
    "- $f$ is the function that follows the GP (which is also our target when using GP to make predictions)\n",
    "- A gaussian process, like gaussian distribution, are defined by only two terms: **mean function** $\\mu(x)$ and **kernel (covariance) function** $k(x,x')$.\n",
    "- The mean function $\\mu(x)$ represents the **Expected Value** of the function $f$ given any input $x$.\n",
    "- The **kernel (covariance) function** $k(x,x')$ captures how function values at different inputs $x$ and $x'$ are related.- GP assumes that function values near each other are correlated in a way dictated by the covariance function (key for generating smooth prediction).\n",
    "\n",
    "The terminologies seem intimidating, but a gaussian process can be easily understood by the plot below:\n",
    "<div style=\"text-align: center\"> <img src=\"https://www.lancaster.ac.uk/stor-i-student-sites/thomas-newman/wp-content/uploads/sites/37/2022/05/Gaussian-process-with-noise.svg\" alt=\"Drawing\" width=\"500\"/> </div>\n",
    "\n",
    "The plot shown here involves training datas ($\\times$), some functions, and a shade area. In a gaussian distribution of a random variable (explained in the *fundamental* notebook), we model the distribution of datapoints. Here in GP, we model **the distribution of functions that go through (or nearby, controlled by a hyperparameter explained later) training points**. The shaded area represents the distribution of the **function**, and the functions shown above are some subsets of the distribution. The **bold** function shown in the plot represents the value of **mean function $\\mu(x)$**, the most possible function value. The shaded area is dictated by the **kernel(covariance) function**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.Definition of Gaussian Process Regression(GPR)\n",
    "Gaussian Process Regression (GPR) is a Bayesian, nonparametric regression method in which we model an unknown function $f(x)$ as being **drawn from a Gaussian process**. \n",
    "Note that gaussian process is **NOT** a machine learning model for prediction. It only describes **Distribution of a function**. Gaussian process can be used to peform multiple tasks, including regression and classification. Here, we focus on **Gaussian Process Regression**, a supervised machine learning model that is cabable of predicting function regardless of its form using gaussian process while also providing uncertainty of the prediction. \n",
    "\n",
    "Recall that a regression is a model the relationship between input variables (features $X$) and a **continuous** output variable (Label $Y$):\n",
    "$$Y=f(X)+\\epsilon,\\,\\epsilon\\sim\\mathcal{N}(0,\\sigma^2)$$\n",
    "\n",
    "Recall that in the case where the function has a fixed form (linear regression), we use true bayesian prediction to obtain a predictive distribution of label $\\hat{Y}$:\n",
    "$$P(\\hat{Y}|X,Y,\\hat{X})=\\int_{\\theta} P(\\hat{Y}|\\theta,X,Y)P(\\theta|X,Y)d\\theta$$\n",
    "The predictive distribution involves a prior $P(\\theta)$ over parameter $\\theta$, which we assumed to be gaussian in the previous example:\n",
    "$$P(\\theta)\\sim \\mathcal{N}(0,\\tau^2I)$$\n",
    "\n",
    "**In GPR**, the goal is to predict the function $f$ directly, regardless of its form. Thus, assuming a prior over parameter $\\theta$ is **unrealistic**, since the form of $\\theta$ is unknown. Instead, we put a **GP prior** over the function which we try to predict. (you can think of that before we have an assumption of $\\theta$, now we have an assumption of $f$. Since the distribution of a function is a gaussian process, we use notation $\\mathcal{GP}$ instead of $P$.)\n",
    "$$ \\begin{align}\n",
    "P(\\hat{Y}|X,Y,\\hat{X})&=\\int_{f} P(\\hat{Y}|f)P(f|X,Y)df \\\\\n",
    "f&\\sim \\mathcal{GP}(\\mu,k)\n",
    "\\end{align}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Pros/Cons of GPR\n",
    "##### Advantages of GPR\n",
    "1. GPR is very powerful in predicting functions in **any form**. Unlike other models that stick with one particular function form (like linear regression for linear model), GP learns the function from the data.\n",
    "2. GPR uses **Bayesian Approach**, incorporating prior knowledge and allow for principled Bayesian inference, making them adaptable to different domains (for reference MAP/True bayesian prediction in fundamental notebook).\n",
    "3. GPR is a probabilistic model, whose output not only provides the most probable value (mean function), but also **uncertainty** of the value.\n",
    "4. GPR does **not require a large dataset**, which is very friendly to the situation where data collection is time-consuming and expensive.\n",
    "5. The choice of **kernel function** allows GPs to capture various data patterns, such as periodicity, smoothness, or sudden changes.\n",
    "##### Disadvantages of GPR\n",
    "1. GPR is computationally **expensive and complex**. This will be explained in later parts.\n",
    "2. GPR does not work ideally with **large datasets ($n>10^6$)**, primarily because its complexity.\n",
    "3. GPR does not work ideally with **high dimensional settings**.\n",
    "4. Choosing kernel functions could be tricky, and it dictates the overall performance of GP.\n",
    "5. Hyperparameter tuning is challenging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Origination of GPR\n",
    "The origination of GPR as a powerful model to predict any arbitraty shape function $f$  with uncertainty comes from several aspects.\n",
    "##### Bayesian Approach\n",
    "As introduced and explained in the *fundamental* notebook, **bayesian approach** is very powerful at making predictions of model parameters (MLE and MAP). In addition, a **true bayesian prediction** provides uncertainty on top of the prediction itself, making it very useful in many usecase where uncertainty is necessary to obtain. Therefore, gaussian process natually emerges as a model using the bayesian approach.\n",
    "##### Universality of Gaussian Distribution\n",
    "Bayesian Approach relies on previous assumptions/knowledge of the distribution (**Prior**) to infer the posterior. This means choosing a **reliable** prior is critical. As shown in the *fundamental* notebook, a **gaussian prior** is the popular choice, due to the universality of gaussian process, and most importantly the **central limit theorem(CLT)**. This property also extends to **gaussian process prior**.\n",
    "\n",
    "In addition, **operations** among gaussian distributions (addition, multiplication, integration, etc) result in **another gaussian** in most cases. \n",
    "\n",
    "These two reasons make choosing a gaussian process prior reasonable. Therefore, a GPR places a gaussian process prior in the model (why it named \"gaussian\" process).\n",
    "##### Closed Form of Predictive Distribution\n",
    "In true bayesian prediction, the **Predictive Distribution** is calculated by integrating over all parameter $\\theta$ in the case of fixed function form regression; and integrating over all function $f$ inthe case of gaussian process regression:\n",
    "$$\\begin{align}\n",
    "P(\\hat{Y}|X,Y,\\hat{X})&=\\int_{\\theta} P(\\hat{Y}|\\theta,X,Y)P(\\theta|X,Y)d\\theta \\\\\n",
    "P(\\hat{Y}|X,Y,\\hat{X})&=\\int_{f} P(\\hat{Y}|f)P(f|X,Y)df\n",
    "\\end{align}$$\n",
    "The indefinite integral is **computationally complex**, and rarely has a **closed form**. In the case of gaussian distribution, a **closed form is available**. This is the key why a GPR can leverage true bayesian prediction as its approach to peform modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.Derivation of GPR\n",
    "Recall that for a regression problem, our goal is that given training feature $X$ and training label$Y$, we want to find the unknown function that maps $X$ to $Y$:\n",
    "$$Y=f(X)+\\epsilon, \\,\\epsilon\\sim\\mathcal{N}(0,\\sigma^2)$$\n",
    "As Stated above, GPR uses bayesian approach to infer the predictive distribution of function $f(x)$:\n",
    "$$P(\\hat{Y}|X,Y,\\hat{X})=\\int_{f} P(\\hat{Y}|f)P(f|X,Y)df$$\n",
    "\n",
    "##### Gaussian Conditioning Formula\n",
    "In the derivation of the predictive distribution, the **Gaussian Conditioning Formula** is frequently used to bypass the cpmlex explicit calculation (specifically the integral).\n",
    "\n",
    "For two multivariate gaussian random Variable $A$ and $B$:\n",
    "$$\\begin{align}\n",
    "A&\\sim\\mathcal{N}(\\mu_A,\\Sigma_{AA})\n",
    "B&\\sim\\mathcal{N}(\\mu_B,\\Sigma_AA)\n",
    "\\end{align}$$\n",
    "The joint distribution between $A$ and $B$ is:\n",
    "$$\\begin{bmatrix} A \\\\ B  \\end{bmatrix}\\sim\\mathcal{N}\n",
    "\\begin{pmatrix}\n",
    "\\begin{bmatrix} \\mu_A \\\\ \\mu_B \\\\ \\end{bmatrix} &&\n",
    "\\begin{bmatrix} \\Sigma_{AA} && \\Sigma_{AB} \\\\ \\Sigma_{BA} && \\Sigma_{BB} \\end{bmatrix}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "And the conditional distribution $P(B|A)$ is:\n",
    "$$P(B|A)\\sim\\mathcal{N}(mu_B+\\Sigma_{BA}\\Sigma_{AA}^{-1}(A-\\mu_A),\n",
    "        \\Sigma_{BB}-\\Sigma_{BA}\\Sigma_{AA}^{-1}\\Sigma_{AB})$$\n",
    "##### What is the posterior $P(f|X,Y)$\n",
    "This step is very similar to the posterior function in bayesian linear regression, we use bayes's theorem to infer:\n",
    "$$P(f|X,Y)\\propto P(Y|f)P(f|X)$$\n",
    "The only difference is that here the prior is a **GP Prior** over $f$:\n",
    "$$P(f|X)\\sim\\mathcal{GP}(m(X),K(X,X))$$\n",
    "where\n",
    "- $m(X)$ is the mean function evaluated at $X$, and <span style=\"color: red;\">It is assumed that $m(X)=0$ in the gaussia prior.</span>\n",
    "- $K(X,X) is the **Covariance Matrix**, where entry $K_{ij}=k(X_i,X_j)$, $k(x,x')$ being the chosen kernel function.\n",
    "\n",
    "$P(Y|f)$ is just the function value with noise:\n",
    "$$P(Y|f)\\sim\\mathcal{GP}(m(X),K(X,X)+\\sigma^2I)\n",
    "$$\n",
    "\n",
    "The **joint distritbuion** of $f(X)$ and $Y$ is:\n",
    "$$\\begin{bmatrix} f(X) \\\\ Y  \\end{bmatrix}\\sim\\mathcal{N}\n",
    "\\begin{pmatrix}\n",
    "\\begin{bmatrix} m(X) \\\\ m(X) \\\\ \\end{bmatrix} &&\n",
    "\\begin{bmatrix} K(X,X) && K(X,X) \\\\ K(X,X) && K(X,X)+\\sigma^2I \\end{bmatrix}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Using the **Gaussian Conditioning Formula**,\n",
    "$$\\begin{align}\n",
    "P(f|X,Y)&=P(f(X)|Y)\\sim\\mathcal{N}(\\mu_f,\\Sigma_f) \\notag \\\\\n",
    "\\mu_f &= K(X,X)[K(X,X)+\\sigma^2I]^{-1}(Y-m(X)) \\notag \\\\\n",
    "&=K(X,X)[K(X,X)+\\sigma^2I]^{-1}Y \\\\\n",
    "\\Sigma_f &= K(X,X)-K(X,X)[K(X,X)+\\sigma^2I]^{-1}K(X,X)\n",
    "\\end{align}$$\n",
    "\n",
    "##### What is $P(\\hat Y|f)$\n",
    "The model prediction is just the function output:\n",
    "$$P(\\hat Y|f)\\sim\\mathcal{N}(m(\\hat X),K(\\hat X,\\hat X))$$\n",
    "\n",
    "##### Predictive Distribution\n",
    "To derive the predictive distribution, we don't have to explicitly calculate the integral, because the result of the integral is analytically equal to another gaussian conditioning (both terms under integration are gaussian). The joint distribution of $Y$ and $f(\\hat X)$ is:\n",
    "$$\\begin{bmatrix} Y \\\\ f(\\hat X) \\end{bmatrix}\\sim\\mathcal{N}\n",
    "\\begin{pmatrix}\n",
    "\\begin{bmatrix} m(X) \\\\ m(\\hat X) \\\\ \\end{bmatrix} &&\n",
    "\\begin{bmatrix} K(X,X)+\\sigma^2I && K(\\hat X ,X) \\\\ K (\\hat X,X) && K(\\hat X,\\hat X) \\end{bmatrix}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "Using the **Gaussian Conditioning Formula**,\n",
    "$$\\begin{align}\n",
    "P(\\hat Y|X,Y,\\hat X)&=P(f(\\hat X)|Y)\\sim\\mathcal{N}(\\mu_*,\\Sigma_*) \\notag \\\\\n",
    "\\mu_* &= K_*^T[K+\\sigma^2I]^{-1}Y \\notag \\\\\n",
    "\\Sigma_* &= K_{**}-K_*^T[K+\\sigma^2I]^{-1}K_*\n",
    "\\end{align}$$\n",
    "Couple Notes:\n",
    "- $K=K(X,X)$ is the **Training Kernel Matrix**\n",
    "- $K_*=K(X,\\hat X)$ is the **Training& Testing Covariance Matrix**\n",
    "- $K_{**}=K(\\hat X,\\hat X)$ is the **Testing Kernel Matrix**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.Kernel Function in GPR\n",
    "A kernel function (also called a covariance function) defines the similarity between data points in Gaussian Process Regression. It specifies how much influence one point has on another, determining the structure of the function space that GPR models. It is extremely important in **GPR** because it controls **smoothness, periodicity, and variability** of the function learned.With that said, choosing an appropriate kernel function is critical.\n",
    "\n",
    "Before we dive in to determine the form of kernel function, we need to identify the requirements on the kernel function. We recognize that for kernel matrix $K$, where $K_ij=k(X,X')$:\n",
    "1. The Diagonal of the matrix must represent the variance of the distribution. This is because we need to satisfy gaussian prior:\n",
    "$$K_{ii}=k(X_i,X_i)=Var(X_i)$$\n",
    "2. $K$ is always **Positive semi-definite**, because variance/covariance **cannot be negative**.\n",
    "3. If $X_i$ and $X_j$ are very **independent**, i.e.$X_i$ and $X_j$ are very different from each other, $K(X_i,X_j)=0$.\n",
    "4. If $X_i$ and $X_j$ are very **similar**, $K(X_i,X_j)>0$.\n",
    "\n",
    "##### Radial Basis Function (RBF) Kernel\n",
    "A very popular kernel function that satisfies all the requirements mentioned above is the **RBF** kernel, which defines similarity with eucleadian distance:\n",
    "$$K(X_i,X_j)=\\tau \\exp \\frac{-||X_i-X_j||^2}{2l^2}$$\n",
    "where\n",
    "- $tau$ is the prefactor (coefficient)of the model. Sometimes it is also called $\\sigma^2$, but may cause confusion because there is another $\\sigma^2$ in the model which is the observation noise variance.\n",
    "- $l$ is the lengthscale. Controls how far the influence of each data point extends. A larger $l$ means smoother functions, while a smaller $l$ allows rapid changes.\n",
    "- $||X_i-X_j||^2$ is the eucleadian distance between the two feature vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
